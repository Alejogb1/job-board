---
title: "How do I pass objects to callback methods in Airflow?"
date: "2024-12-23"
id: "how-do-i-pass-objects-to-callback-methods-in-airflow"
---

Alright, let's talk about passing objects to callback methods in Apache Airflow. This is a recurring challenge I've seen surface a good number of times over the years, particularly as workflows become more complex. I recall one project, a data pipeline for a financial institution, where we initially struggled with this, attempting various convoluted methods before settling on approaches that were not only more robust but also much easier to maintain. So, let's break it down from my perspective, focusing on practical applications and best practices, not just theoretical options.

The core issue stems from the nature of callbacks in Airflow. They're essentially functions that get executed after a task completes (or fails). By default, Airflow provides the task context via the `context` dictionary. While this context is incredibly useful for many common use cases, like accessing task ids, start times, etc., it doesn't natively support passing arbitrary objects. This often leads folks to question how to pass custom data generated by one task to the function that handles the success or failure of that task, specifically outside of the typical templated variables.

There are several viable methods to accomplish this. Let's consider three, which I've found to be the most reliable and adaptable in my experience.

**1. Using XCom (Cross-Communication):**

XCom is perhaps the most common and generally the recommended way to pass data between Airflow tasks. It's a built-in mechanism for exchanging messages. In my previous work, I’ve often used this for passing processed data, validation results, or simple status flags between stages of an ETL pipeline. We can leverage it to pass our object from the main task to a callback function. Here’s how it would look:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.models import XCom

def my_main_task(**context):
    # Suppose this is where we generate an object
    my_object = {"data": "some complex data", "status": "initial"}

    # Push the object to XCom
    context['ti'].xcom_push(key='my_custom_object', value=my_object)

    return 'done'

def my_callback(context):
  # Pull the object from XCom
  pulled_object = context['ti'].xcom_pull(key='my_custom_object', task_ids='main_task_id')

  if pulled_object['status'] == 'initial':
    print("Object was in an initial state, proceeding further.")
    # process further the object..
    pulled_object['status'] = "processed"
  print(f"Callback received the object: {pulled_object}")

with DAG(
    dag_id='xcom_callback_example',
    start_date=days_ago(2),
    schedule_interval=None,
    catchup=False
) as dag:
    main_task = PythonOperator(
        task_id='main_task_id',
        python_callable=my_main_task,
        provide_context=True
    )
    callback_task = PythonOperator(
        task_id='callback_task_id',
        python_callable=my_callback,
        provide_context=True,
        on_success_callback=my_callback
        )

    main_task >> callback_task
```

In this example, `my_main_task` generates a dictionary (your object) and pushes it to XCom using the `xcom_push` method, giving it a key, `my_custom_object`. The `my_callback` function uses `xcom_pull` to retrieve the object. Notice that in order to access the task instance `ti`, we must provide `provide_context=True`. This approach is straightforward and works well for smaller, serializable objects. I’d recommend consulting the official Airflow documentation on XCom for further details about data serialization strategies as large objects can affect performance in complex workflows.

**2. Utilizing a Database or Data Storage:**

When dealing with large objects or complex datasets, XCom, especially when used excessively, might not be the most efficient method. In such cases, storing the object in an external database or dedicated data storage system is far more appropriate. In a particular project that processed massive genomics data, we used a cloud-based object storage to handle interim results between task stages. Consider this example using sqlite as a storage:

```python
import sqlite3
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

def main_task_with_db(**context):
  conn = sqlite3.connect('my_database.db')
  cursor = conn.cursor()
  cursor.execute('''CREATE TABLE IF NOT EXISTS objects (id INTEGER PRIMARY KEY, data TEXT)''')
  my_object = {'data': "large data block", 'id': 12345}
  cursor.execute("INSERT INTO objects (id, data) VALUES (?,?)", (my_object['id'], str(my_object['data'])))
  conn.commit()
  conn.close()
  return 'done'


def callback_task_with_db(context):
    conn = sqlite3.connect('my_database.db')
    cursor = conn.cursor()
    cursor.execute("SELECT data FROM objects WHERE id=?",(12345,))
    rows = cursor.fetchall()
    conn.close()
    pulled_object = rows[0][0]
    print(f"Callback received object: {pulled_object}")

with DAG(
    dag_id='db_callback_example',
    start_date=days_ago(2),
    schedule_interval=None,
    catchup=False
) as dag:
    main_task = PythonOperator(
        task_id='main_task_db',
        python_callable=main_task_with_db,
        provide_context=True
    )
    callback_task = PythonOperator(
        task_id='callback_task_db',
        python_callable=callback_task_with_db,
        provide_context=True,
        on_success_callback=callback_task_with_db
        )

    main_task >> callback_task
```

In this modified example, the main task stores the object data into the local sqlite database and callback function reads from the same source. This method scales well since you are not constrained by Airflow's internal XCom system. Just ensure appropriate data serialization is employed when storing it in the database.

**3. Using the Task Instance's `.set_extra()`:**

A lesser known yet highly effective approach involves using the `task instance`’s `.set_extra()` method. This allows you to associate additional data directly with the task instance. During the callback, this extra data becomes available through the context. This method is suitable when the object is tightly linked to a specific task and not necessarily intended for reuse by other task.

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

def main_task_set_extra(**context):
    # Suppose this is where we generate an object
    my_object = {"data": "specific task data", "status": "pending"}
    #Set the object as the extra parameter for the task instance
    context['ti'].set_extra('my_custom_data', my_object)

    return 'done'

def callback_task_get_extra(context):
  pulled_object = context['ti'].extra.get('my_custom_data')
  if pulled_object:
     print(f"Callback received the object: {pulled_object}")
  else:
      print(f"No extra data found associated to the task instance")


with DAG(
    dag_id='extra_callback_example',
    start_date=days_ago(2),
    schedule_interval=None,
    catchup=False
) as dag:
    main_task = PythonOperator(
        task_id='main_task_extra',
        python_callable=main_task_set_extra,
        provide_context=True
    )
    callback_task = PythonOperator(
        task_id='callback_task_extra',
        python_callable=callback_task_get_extra,
        provide_context=True,
        on_success_callback=callback_task_get_extra
        )

    main_task >> callback_task
```

In this example, the main task uses `context['ti'].set_extra('my_custom_data', my_object)` to store data. Then, the callback can retrieve it using `context['ti'].extra.get('my_custom_data')`. The extra dictionary is not intended to be used for data sharing between tasks, but instead, as a lightweight method to attach metadata associated to a task instance.

**Recommendations and further learning**

When it comes to selecting which method to use it ultimately depends on your specific requirements. XCom is the standard method for general inter-task communication, while database or object storage becomes necessary for large volumes of data. For task-specific data, `set_extra` provides a very useful alternative.

For a deeper understanding, I would strongly recommend diving into *“Data Pipelines with Apache Airflow”* by Bas Pijl, which goes into significant detail about best practices. Moreover, the official Apache Airflow documentation offers in-depth guides on XCom and other core concepts of data management. Furthermore, exploring the *“Designing Data-Intensive Applications”* by Martin Kleppmann can provide a theoretical understanding of data pipelines and how to architect robust systems that will make choices like these easier.

In summary, while passing objects to callback methods in Airflow might seem complicated at first, these approaches, once mastered, provide powerful solutions for more elaborate and data-intensive workflows. The trick is to pick the method that best matches your specific data management and task dependency needs. Remember, the correct solution is the one that works reliably, is easy to maintain, and scales with your growing needs.

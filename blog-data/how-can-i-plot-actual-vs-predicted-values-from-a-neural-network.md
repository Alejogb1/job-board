---
title: "How can I plot actual vs predicted values from a neural network?"
date: "2024-12-16"
id: "how-can-i-plot-actual-vs-predicted-values-from-a-neural-network"
---

Okay, let's tackle this. It's a common task, and one I've certainly found myself addressing countless times over the years. Plotting actual versus predicted values from a neural network is essential for understanding model performance, diagnosing issues, and ultimately, refining your approach. I've seen projects where this step was overlooked, and the result was often a misinterpretation of the network's true capabilities – or lack thereof. Getting it right is absolutely crucial.

Fundamentally, what we're doing is creating a visual representation of how well the network's outputs align with the ground truth data. It's a straightforward concept, but the devil is, as always, in the implementation details. We need to ensure our plot is clear, informative, and easy to interpret. This means careful selection of plot type, clear labeling, and, if needed, additional visualizations to pinpoint specific issues.

The most common way to approach this is with a scatter plot. Each point represents a single data instance, with the x-coordinate being the actual (ground truth) value, and the y-coordinate the corresponding predicted value generated by your neural network. A perfectly performing model would have all points falling precisely on the diagonal (y=x). Deviation from this line is what we want to see, understand, and ultimately reduce.

Before diving into code snippets, it's important to emphasize that pre-processing your data appropriately for visualization is key. This might include scaling, clipping outliers, or transforming your data to better reveal trends. It’s not just about running the prediction and plotting raw numbers; it’s about crafting a visual narrative of the network's behavior.

Now, let's jump into some practical examples using python and the `matplotlib` library, which I tend to use due to its flexibility and extensive customizability. Also, I will use numpy for handling the numerical arrays.

**Example 1: Basic Scatter Plot**

This is the simplest scenario, assuming you have numpy arrays readily available for both your actual and predicted values.

```python
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have 'actual_values' and 'predicted_values' as numpy arrays
# In practice these arrays come from a model's test output

# Let's simulate some data for this example.
actual_values = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
predicted_values = np.array([1.2, 1.8, 3.1, 3.8, 5.2, 6.3, 7.1, 8.2, 8.8, 10.1])


plt.figure(figsize=(8, 6))  # Adjust figure size for better visibility
plt.scatter(actual_values, predicted_values, alpha=0.7) # 'alpha' controls transparency
plt.xlabel("Actual Values", fontsize=12)
plt.ylabel("Predicted Values", fontsize=12)
plt.title("Actual vs Predicted Values - Basic Scatter Plot", fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6) # Add grid for easier interpretation
plt.plot(actual_values, actual_values, color='red', linestyle='--', label='Ideal Prediction')  # Diagonal line
plt.legend() # Add legend
plt.show()
```

This example does several things: it generates the basic plot, adds labels, a title, a grid for better readability and a 'ideal prediction' diagonal line with a corresponding legend. The `alpha` parameter controls the transparency of the markers, which is especially useful when plotting large datasets where points might overlap.

**Example 2: Adding Error Metrics to the Plot**

While the basic scatter plot tells a story, it doesn't quantitatively show our error. Let's enhance the previous plot to include Mean Absolute Error (MAE) and Mean Squared Error (MSE), which are common metrics used to assess model error.

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Assume we have arrays
actual_values = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
predicted_values = np.array([1.2, 1.8, 3.1, 3.8, 5.2, 6.3, 7.1, 8.2, 8.8, 10.1])


mae = mean_absolute_error(actual_values, predicted_values)
mse = mean_squared_error(actual_values, predicted_values)

plt.figure(figsize=(8, 6))
plt.scatter(actual_values, predicted_values, alpha=0.7)
plt.xlabel("Actual Values", fontsize=12)
plt.ylabel("Predicted Values", fontsize=12)
plt.title("Actual vs Predicted Values with Error Metrics", fontsize=14)
plt.grid(True, linestyle='--', alpha=0.6)
plt.plot(actual_values, actual_values, color='red', linestyle='--', label='Ideal Prediction')
plt.text(min(actual_values), max(predicted_values) - 1, f'MAE: {mae:.2f}', fontsize=10, bbox=dict(facecolor='white', alpha=0.5)) # Add text with formatted metrics
plt.text(min(actual_values), max(predicted_values) - 2, f'MSE: {mse:.2f}', fontsize=10, bbox=dict(facecolor='white', alpha=0.5))
plt.legend()
plt.show()

```

Here, we’ve computed MAE and MSE using scikit-learn's metric functions and added them as text annotations on the plot. Positioning text carefully is important to ensure readability without obscuring data points. These metrics provide a quick quantitative assessment alongside the visualization.

**Example 3: Plotting Residuals**

Sometimes the discrepancies between actual and predicted values (residuals) are more telling than a straightforward actual vs predicted plot. This example demonstrates how to plot residuals which highlights over and underestimations.

```python
import matplotlib.pyplot as plt
import numpy as np

# Assume we have arrays
actual_values = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
predicted_values = np.array([1.2, 1.8, 3.1, 3.8, 5.2, 6.3, 7.1, 8.2, 8.8, 10.1])

residuals = predicted_values - actual_values

plt.figure(figsize=(8, 6))
plt.scatter(actual_values, residuals, alpha=0.7)
plt.xlabel("Actual Values", fontsize=12)
plt.ylabel("Residuals (Predicted - Actual)", fontsize=12)
plt.title("Residual Plot", fontsize=14)
plt.axhline(0, color='red', linestyle='--') # Horizontal line at residual = 0
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()
```

By plotting the residuals against the actual values, we can immediately identify systematic biases. For instance, if the residuals are predominantly positive or negative for a certain range of actual values, it suggests the network might be over- or underestimating within that region. The horizontal line at zero provides a clear reference.

In terms of recommended resources, for a deeper understanding of neural network performance analysis, I would strongly recommend *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This is a comprehensive textbook and a good bedrock. For more specific insights into visualization techniques, *Information Visualization: Perception for Design* by Colin Ware, provides foundational principles. Also, consider delving into the practical aspects of *Python Data Science Handbook* by Jake VanderPlas, specifically the chapters that explore matplotlib.

These examples represent only a starting point, and the type of visualization you'll ultimately use will depend on the characteristics of your data and the specific aspects of model performance you wish to examine. Remember that the best visualizations are informative, clear, and directly address the questions you're trying to answer about your model's performance. The key is always iteration and careful attention to what the visuals are telling you. I hope this helps you.

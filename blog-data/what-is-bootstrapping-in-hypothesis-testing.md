---
title: "What is bootstrapping in hypothesis testing?"
date: "2025-01-26"
id: "what-is-bootstrapping-in-hypothesis-testing"
---

Bootstrapping, in the context of hypothesis testing, provides a powerful alternative to traditional parametric methods, particularly when assumptions about the underlying population distribution are questionable or when dealing with complex, non-standard statistics.  My experience with large-scale A/B testing across diverse user segments has repeatedly highlighted situations where the standard t-tests and ANOVA models fall short due to real-world data deviating from normality. This is where bootstrapping has proven invaluable.

At its core, bootstrapping is a resampling technique. Instead of relying on theoretical distributions and their associated assumptions, it generates a large number of simulated datasets by repeatedly sampling *with replacement* from the original observed data. This process allows us to empirically estimate the sampling distribution of a statistic, like a mean difference or a correlation coefficient, even if the population distribution is unknown. Effectively, we're using our observed data as a proxy for the true population, and by repeatedly resampling from it, we're exploring the range of plausible values for our statistic under different random draws.

The process typically unfolds as follows:

1. **Original Sample:** We start with our observed dataset, often denoted as *D*.
2. **Resampling:** We repeatedly draw, with replacement, samples of the same size as *D* from *D*. These resamples are termed "bootstrap samples".
3. **Statistic Calculation:** For each bootstrap sample, we compute the statistic of interest (e.g., mean difference between two groups, correlation coefficient). This generates a distribution of the statistic across all bootstrap samples.
4. **Estimation:** This distribution of the statistic now serves as an approximation of the true sampling distribution. We can then calculate confidence intervals or p-values based on this distribution.

In hypothesis testing, bootstrapping is most frequently used to assess the statistical significance of an observed effect or difference. Instead of relying on a t-distribution or chi-squared distribution, we're relying on the empirical distribution generated by our resamples. This is particularly helpful when traditional statistical tests' assumptions are violated (e.g., non-normal data, small sample sizes, heteroscedasticity).

Let's consider a few concrete code examples to illustrate this process. These will be in Python, utilizing `numpy` for numerical calculations and `scipy` for some statistical utilities.

**Example 1: Bootstrapping the Mean Difference**

```python
import numpy as np

def bootstrap_mean_diff(group_a, group_b, n_resamples=10000):
  """Calculates the bootstrapped distribution of the mean difference."""
  observed_diff = np.mean(group_a) - np.mean(group_b)
  all_data = np.concatenate((group_a, group_b))
  n_a = len(group_a)
  diffs = []
  for _ in range(n_resamples):
      resampled = np.random.choice(all_data, size=len(all_data), replace=True)
      resampled_a = resampled[:n_a]
      resampled_b = resampled[n_a:]
      diffs.append(np.mean(resampled_a) - np.mean(resampled_b))
  return np.array(diffs), observed_diff

# Example Usage:
group_a_data = np.array([22, 28, 25, 31, 29, 26])
group_b_data = np.array([18, 20, 24, 23, 21, 19])

bootstrap_dist, observed_difference = bootstrap_mean_diff(group_a_data, group_b_data)

# Calculate a p-value (one-sided, assuming we expect A > B)
p_value = np.mean(bootstrap_dist <= observed_difference)
print(f"One-sided p-value for observed difference {observed_difference:.2f}: {p_value:.3f}")
```

This code defines a function `bootstrap_mean_diff` that takes two arrays (representing the two groups) as input. It calculates the observed mean difference. It then resamples from the combined dataset and calculates the mean difference for each resample. The return includes an array of these resampled differences. A one-sided p-value is then calculated and printed.

**Example 2: Bootstrapping a Correlation Coefficient**

```python
import numpy as np
from scipy.stats import pearsonr

def bootstrap_correlation(x, y, n_resamples=10000):
  """Calculates the bootstrapped distribution of the Pearson correlation."""
  observed_corr, _ = pearsonr(x, y) # Disregard the p-value from scipy
  n = len(x)
  corrs = []
  for _ in range(n_resamples):
    indices = np.random.choice(np.arange(n), size=n, replace=True)
    x_resampled = x[indices]
    y_resampled = y[indices]
    corr, _ = pearsonr(x_resampled, y_resampled)
    corrs.append(corr)
  return np.array(corrs), observed_corr

# Example usage
x_data = np.array([1, 2, 3, 4, 5, 6])
y_data = np.array([2, 3, 5, 7, 8, 9])
bootstrap_corr, observed_correlation = bootstrap_correlation(x_data,y_data)

# Calculate a two sided p-value:
p_value = np.mean(np.abs(bootstrap_corr) >= np.abs(observed_correlation))
print(f"Two-sided p-value for observed correlation {observed_correlation:.2f}: {p_value:.3f}")

```

This example demonstrates bootstrapping for a correlation coefficient. The `pearsonr` function from `scipy.stats` is used to calculate the observed Pearson correlation. The bootstrapping is performed by resampling pairs of indices and calculating the correlation from these resamples. Again, we calculate and print the two-sided p-value.

**Example 3: Bootstrapping the Median Difference**

```python
import numpy as np

def bootstrap_median_diff(group_a, group_b, n_resamples=10000):
    """Calculates the bootstrapped distribution of the median difference."""
    observed_diff = np.median(group_a) - np.median(group_b)
    all_data = np.concatenate((group_a, group_b))
    n_a = len(group_a)
    diffs = []
    for _ in range(n_resamples):
        resampled = np.random.choice(all_data, size=len(all_data), replace=True)
        resampled_a = resampled[:n_a]
        resampled_b = resampled[n_a:]
        diffs.append(np.median(resampled_a) - np.median(resampled_b))
    return np.array(diffs), observed_diff

# Example Usage:
group_a_data = np.array([22, 28, 25, 31, 29, 26, 100]) # added an outlier
group_b_data = np.array([18, 20, 24, 23, 21, 19])


bootstrap_dist, observed_difference = bootstrap_median_diff(group_a_data, group_b_data)

# Calculate a p-value (two-sided)
p_value = np.mean(np.abs(bootstrap_dist) >= np.abs(observed_difference))
print(f"Two-sided p-value for observed median difference {observed_difference:.2f}: {p_value:.3f}")

```
Here, we've adapted the first example to compute the difference in *medians* rather than means. I've added an outlier to `group_a_data` to highlight where this non-parametric method excels when compared to the parametric approach used in example 1, as the median is far less sensitive to outliers. A two-sided p-value is calculated and printed.

For further exploration and robust understanding, the following resources are highly recommended: "An Introduction to the Bootstrap" by Efron and Tibshirani, which lays the theoretical foundation, and "Statistical Inference via Data Science: A ModernDive into R & Python" by Çetinkaya-Rundel, Baumer, and Bray, which provides a more practical data science focused perspective. Various online tutorials and courses on statistical inference and data science often include dedicated sections on bootstrapping.

Here’s a comparative table of various statistical methods:

| Name                  | Functionality                                                                          | Performance                  | Use Case Examples                                                          | Trade-offs                                                                                        |
|-----------------------|----------------------------------------------------------------------------------------|------------------------------|----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| **t-test**             | Compares means of two groups.                                                          | Efficient with normal data. | Comparing average user spend between two website layouts.                   | Requires normality and equal variances; sensitive to outliers.                                       |
| **ANOVA**              | Compares means of multiple groups.                                                     | Efficient with normal data. | Comparing test scores across three different teaching methods.             | Requires normality and equal variances; less robust to outliers than bootstrapping, harder for post hoc. |
| **Chi-squared test**    | Tests association between categorical variables.                                       | Efficient with large samples | Analyzing association between user type and preference for a specific feature. | Relies on asymptotic approximation; requires reasonable sample size; less useful for small sample.  |
| **Bootstrapping**      | Estimates sampling distributions of complex statistics through resampling.             | Computationally intensive.  | Testing differences in medians, percentiles, or any statistic with non-normal data or small sample sizes. | Computationally slower; requires careful implementation to avoid subtle errors; results can vary slightly between runs. |
| **Permutation testing** | Tests hypotheses by randomly shuffling data labels; suitable for non-standard statistics.   | Computationally intensive.  | Testing for differences in distributions; hypothesis testing when distribution isn't normal.     | Computationally slower, particularly with larger datasets; may not be as interpretable as bootstrapping. |

In conclusion, while t-tests, ANOVA, and chi-squared tests are valuable for common scenarios when their assumptions hold, bootstrapping is optimal when those assumptions are not met, such as when dealing with non-normal data, small samples, or non-standard statistics. Permutation tests share some similarities, and may be preferred for specific distribution-based hypotheses, but for estimating confidence intervals and p-values on virtually any statistic, bootstrapping remains a highly robust and adaptable choice within the broader hypothesis testing toolkit.

---
title: "LLM Observability and Prompt Iteration with LangSmith & Quotient AI"
date: "2024-11-29"
id: "6983"
---

dude so this video was like a whirlwind tour of llm app development and observability the guy's building this rad platform cats with bats—think a udemy or coursera killer but for coding vids—and he's _also_ super into ai tinkers a group for people geeking out over large language models the whole point of the vid was to show how crucial tracing is for building and debugging llm apps think of it as putting a super detailed gps tracker on your entire ai system

 so key moments five at least five right

1. **the "why" of tracing**: he starts by hammering home that when you're dealing with llm apps—especially in production—you can't just stare at logs anymore stuff's happening too fast and you need automated ways to monitor performance and catch errors he even mentions how llms are kinda unpredictable so spotting patterns in the data is golden

2. **lsmith and friends**: this is where he shows off lsmith—an observability platform think of it as a fancy dashboard that visualizes all the inner workings of your ai system in real-time he also name drops other similar tools like helicone and agent ops it's like a super detailed map of your code's journey each "run" represents a single task and the whole thing is structured like a tree you can see how long each step took how many tokens were used—the whole shebang it's wild seeing this in action in the video

3. **performance vs optimization**: this is a really smart point he makes early on you might initially go for the most powerful llm (gpt-4, etc) to get the best results quickly but he argues that once that's working you need to dig into optimizing your code and choosing smaller more efficient models for specific tasks this saves cash and makes things faster

4. **tracing example with lsmith**: he walks through a practical example using lsmith to trace an app function that summarizes an article the code's pretty simple actually here's a snippet it uses a decorator from the lsmith library to automatically log the call:

```python
from lsmith import traceable

@traceable(name="summarize_article")
def summarize_article(article_text):
    response = openai.Completion.create(
        engine="text-davinci-003",  # or whatever model you're using
        prompt=f"Summarize this article:\n\n{article_text}",
        max_tokens=150,
        n=1,
        stop=None,
        temperature=0.5,
    )
    return response.choices[0].text.strip()

# example usage
article = "this is a long article with lots of words and stuff"
summary = summarize_article(article)
print(summary)
```

that little `@traceable` decorator does all the magic it automatically sends the details of that function call to the lsmith dashboard neat right? he also shows how you can add metadata and custom tags to make your traces even more useful

5. **prompt iteration with quoti**: the second half of the vid focuses on quoti—a tool for managing and iterating on prompts he highlights how you can move your prompts out of your code and into a centralized system this allows for collaboration between developers and prompt engineers and facilitates A/B testing and prompt optimization it even has a cool feature that uses ai to help improve your prompts based on feedback and you can see how this works with a code snippet:

```python
from quoti import QuotiClient

client = QuotiClient(api_key="YOUR_API_KEY")
prompt_id = "YOUR_PROMPT_ID"  # get this from the quoti interface
prompt_data = client.get_prompt(prompt_id)

response = openai.Completion.create(
    engine="text-davinci-003",
    prompt=prompt_data["prompt"],
    # ...other parameters...
)

# Now send the response to Quoti for evaluation (if you want)
# client.evaluate_prompt(prompt_id, response, feedback="good" or "bad")

```

the key here is that your prompts aren't hardcoded—they're fetched from quoti dynamically this allows for changes without touching your main code. and one more bit of code—this one shows how he uses a wrapper around an openai client to easily trace llm calls without modifying the rest of his code

```python
from lsmith.openai import trace_openai

# Wrap your OpenAI client
traced_openai = trace_openai(openai)

# Now use traced_openai instead of openai directly
response = traced_openai.Completion.create(
    # ... your OpenAI call parameters ...
)

# No changes to your existing code, just the wrapper!
```

it's all about keeping things organized and making the debugging process easier he also throws in a uv install command (a rust-based package manager) which is super cool and shows off how easily he can run things without setting up a whole virtual environment basically pure awesomeness

the resolution is that tracing is _essential_ for anyone serious about building llm applications it's not just about finding bugs it's about understanding user behavior optimizing your system for speed and cost and even improving your prompts through data-driven iteration lsmith and quoti are presented as tools to help achieve all of this so yeah build that observability into your system right from the start you'll thank yourself later trust me

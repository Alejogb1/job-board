---
title: "LangSmith Benchmark: Tavily, Perplexity API, Gemini, and Exa LLM Comparison"
date: "2024-11-29"
id: "5895"
---

dude so this vid was like a total rollercoaster of web scraping awesomeness and mild panic attacks the guy's basically showing how to use firecrawl this super chill api for pulling data off websites it's aimed at peeps who don't wanna spend a zillion hours coding scrapers themselves think of it as like the laziest way to get website info without actually having to learn how to scrape effectively which is way cooler and saves way more time than the alternative

first things first he sets the scene he's got this python environment all set up you see him copying an api key classic and talks about how firecrawl's got sdk's for python and typescript so it's basically developer-friendly which is super nice he even mentions using it with langchain and llama index which instantly gets me stoked because i'm a big fan of integrating everything i can

then bam he drops the code like this:

```python
# installing the package
pip install fire-crawl

# creating a firecrawl client
from fire_crawl import FireCrawlClient
client = FireCrawlClient(api_key="YOUR_API_KEY")

# scraping a url  gets markdown by default
result = client.do_scrape("https://www.somewebsite.com")
print(result.content)

# crawling a site with depth 2
crawl_result = client.do_crawl("https://www.somewebsite.com", depth=2)
print(crawl_result.content)


# using the sitemap endpoint to get a structured list of urls
sitemap_urls = client.do_sitemap("https://www.somewebsite.com")
for url in sitemap_urls:
    print(url)

```

super straightforward right it's basically three lines to install and then just a handful of functions to scrape crawl and use their sitemap functionality love it simple elegant powerful

one of the key takeaways he emphasizes is the diff between `scrape` and `crawl` scrape's like a laser focus on a single url grab the data and bounce crawl's the deep dive it hits the main url then follows all the links it's like a spiderweb of data extraction he uses the analogy of finding info on a government website you'd use `crawl` to cover all the possible pages where the info might be hidden and you better pace yourself with some delays or you're gonna get banned

another big moment is when he talks about the wits ice cream shop it's a hilarious example imagine you're the owner a total noob with computers you want a chatbot but you don't wanna manually type in all your business details firecrawl can scrape your website for you it's genius

then it gets really interesting he's pulling data but it's all messy a huge block of markdown he says large language models llms love markdown which is true but it's still unstructured so he hits us with another code snippet this one for data extraction using an llm he uses something called quo ai which is awesome for prompt engineering this is where it got crazy

```python
# this is a simplified example  the actual prompt is more complex
from openai import OpenAI  # or whatever llm client you're using

llm = OpenAI(api_key="YOUR_LLM_API_KEY")

prompt = """
You are an expert data extraction AI.
Extract the following information from the given text into a JSON object:
business_name (string)
business_address (string)
operating_hours (string)

Text: """ + scraped_markdown

response = llm.chat.completions.create(
  model="gpt-3.5-turbo", # or whatever llm model
  messages=[{"role": "user", "content": prompt}],
  temperature=0,
)

extracted_data = json.loads(response.choices[0].message.content)  #assuming it returns json
print(extracted_data)

```

this is where the magic happens he feeds the messy markdown to an llm but with a super specific prompt it tells the llm exactly what kind of structured data it wants business name address hours etc it's all about giving the llm enough context and clear instructions it’s the key to getting great results with an llm

the third code snippet highlights a critical concept—using the sitemap api for intelligent crawling:

```python
from fire_crawl import FireCrawlClient

client = FireCrawlClient(api_key="YOUR_API_KEY")
sitemap = client.do_sitemap("https://www.example.com")

# prioritize urls containing relevant keywords
relevant_urls = [url for url in sitemap if "about" in url or "contact" in url]

for url in relevant_urls:
    results = client.do_scrape(url)
    #process results...
```

the `do_sitemap` endpoint is a game changer it provides a structured overview of a website's pages so instead of blindly crawling everything you can select only the relevant pages this saves tons of credits and time

he wraps things up by stressing the importance of evaluation he uses quo ai's feedback loop to refine his llm prompts it’s all about iterative improvement and he mentions other tools like langsmith for post-deployment monitoring which is something i’ve also been experimenting with it’s really interesting stuff

the overall resolution is clear firecrawl is a fantastic tool for efficiently getting data from websites but you need to master llms and careful prompt engineering to actually get usable data and you need to be mindful of rate limits and potential legal restrictions it’s a powerful combo of web scraping and llm magic he even mentions the pricing pretty reasonable and the fact that the founders of firecrawl are super active constantly improving the tool gives me confidence that it's gonna stick around for a while

the whole video is a great practical example it's not just theory it's hands-on code it showcases a workflow that's actually usable for real-world tasks he’s even got a little easter egg at the end a story of how firecrawl spun out of another project showing how things evolve in the tech world it’s all super inspiring

anyway that’s my totally informal and possibly rambling explanation of that firecrawl video if you have any questions just hit me up i’m still trying to wrap my head around all the llm prompt engineering stuff but firecrawl is definitely in my toolkit now

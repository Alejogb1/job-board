---
title: "Transforming Data Extraction with LLM Chains and Token Optimization"
date: "2024-11-29"
id: "9222"
---

dude so i watched this video on web scraping with llms and it was wild the whole point was to show how startups are using ai to basically build super-powered web scrapers like way beyond just pulling down some html it's all about getting structured data quickly efficiently and cheaply

first off the dude mentions perplexity you know how search engines try to give you the _best_ answer that's all about it if you can scrape all the latest data from websites you can give way more relevant answers i mean think about an lms for internal company docs being able to instantly search across everything with natural language queries that's a game changer

another thing he pointed out was this shift from just grabbing data to actually _understanding_ it he showed examples of tools like mendable and gina ai these aren't your grandpa's scrapers they clean up the mess they actually try to figure out what's important and give you it in a format that an llm can easily understand

he had a few visual cues like showing a little robot icon on a site's documentation that let you do natural language searches super cool and then he showed off tables in his terminal using the python prettytable library super handy for visualizing data and there was a ton of code flying around honestly it was a bit of a blur but i think i got most of it

two core concepts are tokenization and llm chains tokenization is how llms break down text into smaller chunks that they can process it's super important for cost because you're charged per token the fewer tokens the cheaper the call

look at this code snippet that calculates tokens using tiktoken this library from openai lets you figure out how many tokens a piece of text will use

```python
import tiktoken

encoding = tiktoken.encoding_for_model("gpt-4")
text = "this is a test string to count tokens with"
num_tokens = len(encoding.encode(text))
print(f"the number of tokens is: {num_tokens}")


# now let's  do some token counting with a bit more real world data
scraped_data = """this is a longer string that we are simulating as the content that's been scraped from a web page  it might contain various elements and we need to count all of this"""
num_tokens_scraped = len(encoding.encode(scraped_data))
print(f"number of tokens for scraped data: {num_tokens_scraped}")

# we can extrapolate the cost here but it requires knowing per token pricing for your chosen LLM
```

this is pretty straightforward you just choose your model's encoding and feed it your text the output is the number of tokens which is directly related to the cost of using the llm it's all about optimizing for fewer tokens to save money

then there are llm chains these are basically workflows where the output of one llm call becomes the input for the next he used it for data extraction from scraped content taking raw text and extracting just the pricing info he wanted it's like a pipeline where each step refines the data

here's some python code illustrating a basic llm chain for extracting pricing information

```python
import openai

def extract_pricing(text):
    response = openai.Completion.create(
        engine="text-davinci-003", # you would adapt this to your preferred model
        prompt=f"Extract pricing information from the following text:\n\n{text}\n\nReturn as a JSON object like this: `{{'price': 100, 'currency': 'USD'}}`",
        max_tokens=150,
        n=1,
        stop=None,
        temperature=0
    )
    try:
        return json.loads(response.choices[0].text.strip())  #try parsing as json
    except json.JSONDecodeError:
        return {"error":"could not parse json"}  # handle errors gracefully


#example use
scraped_text = "our basic plan is $9.99 per month the pro plan is $49.99 per month and enterprise is $999 per month"
pricing_info = extract_pricing(scraped_text)
print(pricing_info)

#another example with a bit more complex text, we'll see if it holds up
scraped_text_complex = "The pricing tiers are as follows: Starter - $10/month, Pro - $50/month, and Enterprise (contact sales for pricing)."
pricing_info = extract_pricing(scraped_text_complex)
print(pricing_info)
```

see how the prompt guides the llm to return specific data in a consistent format this is crucial for automation and keeping your data organized using different functions helps keep things organized and less prone to errors

he used three main web scraping tools beautifulsoup gina ai and mendable beautifulsoup is the classic it's simple but can be easily detected gina ai and mendable are newer tools that use llms to clean up the data before feeding it to the llm doing the analysis they are designed to be much more difficult for websites to detect as scrapers

here's a bit of code illustrating a simplified approach to using beautifulsoup

```python
import requests
from bs4 import BeautifulSoup


def scrape_website(url):
    response = requests.get(url)
    response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
    soup = BeautifulSoup(response.content, "html.parser")

    # find all p tags and extract text from each one
    paragraphs = soup.find_all("p")
    extracted_text = ""
    for p in paragraphs:
        extracted_text += p.get_text() + "\n"

    return extracted_text



url_to_scrape = "https://www.example.com"  # replace with your target URL
extracted_data = scrape_website(url_to_scrape)
print(extracted_data)

# obviously  this is a very basic approach and lacks error handling as well as efficient navigation. this is only meant as a conceptual overview of how you can use beautiful soup

```

this is a bare bones example in reality you'd need way more sophisticated error handling and targeted selection of elements but it shows the basic idea

the resolution was pretty clear using ai-powered web scrapers plus llm chains is far more efficient than using traditional methods and even though the initial cost of the llm might seem high the savings in time and effort massively outweigh it plus you get cleaner better structured data that's way easier to use and analyze

he even joked around a bit about the cost showing how much cheaper it was to use gina ai versus the other options he stressed how important it is to be specific in your llm prompts to get the best results because you can easily burn through tons of tokens if you're not careful

overall it was a fun informative look into the future of web scraping it's not just about grabbing data anymore it's about understanding and using it intelligently using llms gives you a massive advantage because it takes care of a lot of the hard work leaving you free to focus on the actual insights and analysis

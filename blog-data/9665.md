---
title: "How to Build a Smart AI Chat Companion with Memory Using Python"
date: "2024-11-29"
id: "9665"
---

dude so this vid was like a total deep dive into building your own freakin' ai buddy okay it's not like building a robot pal that serves you tea and biscuits (though that's my next project _wink_) but it's pretty rad

the whole point was showing us how to build a chatty ai using python and a bunch of sweet libraries like langchain and openai the guy basically built an ai that remembers things about you like your name your fave foods your stance on jump ropes (apparently some people _aren't_ fans) and keeps that info around even if you leave and come back to the chat it's way more advanced than just a simple chatbot think of it as building an ai with a _personality_ and a memory

first thing he did was show the setup it was totally straightforward i mean he was showing it on his screen but i'll summarize it for you: you need to git clone the repo create a virtual env (because you always do) then activate that env and install a bunch of packages using pip obviously you gotta set your openai api key in a .env file to avoid exposing it publicly - security is crucial then you run main.py and the fun starts

one of the key things he highlighted was using langchain's prompt templates this is genius because it's essentially templating a prompt for the ai to handle this way you can dynamically update elements within a prompt (like the conversation history or user data) instead of writing a new prompt every time he showed us how it looked on screen—all these prompts and template filling and you could literally see it all happen

the code snippet for a super basic prompt template looks something like this:

```python
from langchain.prompts import PromptTemplate

template = """You are a helpful AI assistant.  The user is {user_name} who likes {likes} and dislikes {dislikes}.

The conversation history is: {history}

Here's the current message: {message}

Respond helpfully to the message.
"""

prompt = PromptTemplate(
    input_variables=["user_name", "likes", "dislikes", "history", "message"],
    template=template,
)

user_data = {
    "user_name": "bob",
    "likes": ["pizza", "coding"],
    "dislikes": ["jump ropes", "broccoli"],
    "history": "bob: hi\nai: hello",
    "message": "what's up?",
}

final_prompt = prompt.format(**user_data)
print(final_prompt)
```

another super important takeaway was the use of threading because the dude didn't want to make the user wait ages for responses he had two agents running concurrently one is the main ai companion and the other is what he called a "user profile operator" the first agent chats with the user the second silently updates the user's profile data in the background this is slick because you get snappy responses while the ai updates the user's data no more staring at a loading screen like some kind of peasant!

here's a _super simplified_ idea of how the threading might look (obviously this isn't the full code but you get the vibe):

```python
import threading
import time

def ai_companion(user_input):
    # pretend this does some complex ai stuff
    time.sleep(2) # simulate processing time
    print(f"ai: you said: {user_input}")
    return f"ai response: {user_input} is a great input!"

def user_profile_update(user_input):
    # pretend this updates the database
    time.sleep(3) # simulate database update time
    print(f"user data updated with: {user_input}")

user_input = "hello"

ai_thread = threading.Thread(target=ai_companion, args=(user_input,))
profile_thread = threading.Thread(target=user_profile_update, args=(user_input,))

ai_thread.start()
profile_thread.start()

ai_thread.join()
profile_thread.join()
```

this snippet makes use of the `threading` library to handle the concurrent processes that happen behind the scenes so, even though the `ai_companion` and `user_profile_update` functions take time, the main program doesn't wait for them to finish completely before printing the response, making the whole process more responsive.

and finally he talked about openai functions which were pretty cool—it's basically a way to let the lm call functions based on the input—this was done using the `openai.FunctionCall` object. you give it a list of predefined functions the model can use and it dynamically picks which ones (and in which order) are best suited for your prompt in his case he used it to update the user's profile that means the ai can actually decide when to use the update function which is way smarter than just having it always run the user profile operator agent was an instance of an openai functions agent built with langchain. this meant it could decide when it was time to call the function that updates the user's info and the code is relatively compact. a super simplified example:

```python
import openai

functions = [
    {
        "name": "update_profile",
        "description": "Updates user profile information.",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "likes": {"type": "array", "items": {"type": "string"}},
            },
            "required": ["name"],
        },
    }
]


response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo-0613",
    messages=[{"role": "user", "content": "my name is alice and i like pizza"}],
    functions=functions,
)

if response.choices[0].finish_reason == "function_call":
    function_call = response.choices[0].message["function_call"]
    # process the function call here to update the user's profile
    print(f"function to call: {function_call['name']}")
    # do something with the arguments

```

this snippet shows the bare minimum this only shows the function call you'll have to handle the function call separately depending on the result from the openai model.

the whole thing wrapped up with the guy mentioning that he's making a course on building full-stack ai agents so from the backend python code using all this fancy stuff to the frontend (react or nextjs maybe) and even deployment on something easy like heroku pretty ambitious! it was a great overview of how you can create a reasonably sophisticated ai that's more than just a mindless chatbot but actually remembers its users

it really drove home the point that building intelligent ais isn't just about the model itself but it's about all the cool stuff surrounding it like how you handle prompts how you manage user data and how you make the whole thing responsive and efficient using techniques like concurrent processing

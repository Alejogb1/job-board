---
title: "Autonomous Web Scraping with Firecrawl and LangGraph"
date: "2024-11-29"
id: "3117"
---

yo dude so i just watched this rad video on agentic web scraping using langchain and firecrawl and it was _insane_ let me break it down for you it's basically about building a super-smart bot that can find specific stuff on websites way faster than any human could

setup/context:

the whole point of this video is to show how you can use langchain's langgraph agent framework along with firecrawl (an api for web scraping) to build a python bot that hunts down specific products on a website even if you only know the root domain and a keyword like seriously this thing navigates websites like a pro

references:

- remember the cats he showed at the start of the vid totally random but memorable
- he kept mentioning the "lodgic coach jacket black label" that was the target item for the scrape
- the whole thing was a python demo running in a terminal he showed the output of each stage of the scraping process so you could visually follow along

key ideas:

1. langchain's langgraph: imagine zapier but way cooler instead of pre-built zaps you define a workflow of "nodes" each node is like a mini-program that does a specific task the agent moves data (the "state") between these nodes like passing a baton in a relay race

for example in this web scraper:

- **node 1:** takes user input (website url and keyword) and sets up the initial state
- **node 2:** uses firecrawl's `sitemap` function to get _all_ the sub-urls from the website the key here is that it leverages firecrawl's ability to use keywords in the request to get more relevant links earlier on in the process saving tons of time and credits
- **node 3:** takes those sub-urls batches them up and uses firecrawl to scrape each batch individually getting the markdown content of each page
- **node 4:** evaluates the scraped content using keyword matching to see if the target product (our jacket) is there if yes it stops if no it goes back to the scraping node for the next batch it's pure genius

think of it like this the langgraph agent is the detective firecrawl provides the clues (urls and page content) and keyword matching is the magnifying glass

2. firecrawl's power: firecrawl is an api that makes web scraping super easy it gives you clean markdown output which is perfect for working with llms you can just send it a url and it returns all the links and page content without you having to deal with the messy details of html parsing robots.txt and all that jazz seriously it made it so much easier and faster than writing a web scraper from scratch

resolution:

the video ends with the agent successfully finding the jacket's page on canada.com the whole process is super efficient the use of keywords in the firecrawl api call dramatically improved the speed and efficiency of the search compared to a brute-force approach that would have to process all sub-urls sequentially

code snippets:

here's a super simplified version of what the code might look like it's not the exact code from the video but it captures the core idea of how the nodes work:

```python
# node 1: initialize state
initial_state = {
    "url": "canada.com",
    "keyword": "lodgic coach jacket black label",
    "found": False,
    "urls_to_scrape": [],
}

# node 2: get sub-urls using firecrawl (simplified)
import firecrawl #replace with your api key
client = firecrawl.Client("YOUR_FIRE_CRAWL_API_KEY")

response = client.sitemap(initial_state["url"], params={"keyword": initial_state["keyword"]})
initial_state["urls_to_scrape"] = response.get("urls",[])  # extract urls


# node 3: scraping function
def scrape_batch(urls):
    scraped_data = []
    for url in urls:
        try:
            response = client.scrape(url)
            scraped_data.append({"url": url, "content": response.content})
        except Exception as e:
            print(f"error scraping {url}: {e}")
    return scraped_data

# node 4: evaluation (simplified keyword matching)

def evaluate(scraped_data, keyword):
  for item in scraped_data:
    if keyword in item["content"]:
      return True, item["url"]  # product found
  return False,None  # product not found



#langchain agent orchestration (simplified concept)
# this part is heavily abstracted and uses langchain's specific functions
# the following code is mostly illustrative for educational purposes

while not initial_state["found"]:
  batch = initial_state["urls_to_scrape"][:3] #process batches of 3 urls at a time
  initial_state["urls_to_scrape"] = initial_state["urls_to_scrape"][3:]
  scraped_data = scrape_batch(batch)
  initial_state["found"], found_url = evaluate(scraped_data, initial_state["keyword"])
  if initial_state["found"]:
    print(f"Product found at: {found_url}")
    break
  #if no product found, loop continues with next batch of urls

```

this is a bare bones example the real implementation in the video uses langchain's agent framework for workflow management and error handling it's waaaay more robust than this simple code

another snippet illustrating the batch processing in node 3:

```python
def scrape_batch(urls, batch_size=3):
  """Scrapes URLs in batches."""
  results = []
  for i in range(0, len(urls), batch_size):
    batch = urls[i:i + batch_size]
    # ... (scrape each URL in the batch using firecrawl, handle errors) ...
    results.extend(scraped_batch)  # append the results from this batch to the main results list
  return results

# usage
urls_to_scrape = ["url1", "url2", "url3", "url4", "url5"]
scraped_data = scrape_batch(urls_to_scrape, batch_size=2)
print(scraped_data)

```

this code shows how to efficiently handle scraping multiple urls by dividing them into smaller batches processing each batch in turn this is essential for avoiding overwhelming the api or the system's resources

a final example showing a super simple keyword check in the evaluation node:

```python
def evaluate(scraped_content, keyword):
    """Checks if the keyword is present in the scraped content."""
    return keyword.lower() in scraped_content.lower()

# example
scraped_content = "this is some text containing the keyword lodgic coach jacket black label"
keyword = "lodgic coach jacket black label"
found = evaluate(scraped_content, keyword)
print(f"Keyword found: {found}")

```

this is a basic keyword check the real evaluation in the video would likely involve more sophisticated techniques like using an llm to understand the context of the scraped text to ensure accurate detection of the product even if the exact keyword isn't present

overall the video is a masterclass in agentic web scraping it shows how you can combine powerful tools like langchain and firecrawl to build bots that can do surprisingly complex tasks with minimal code it's way more efficient than traditional web scraping methods because of the clever use of keywords to prioritize relevant links plus the whole langgraph workflow is super visually intuitive making it easy to understand and debug highly recommend checking out the actual video and his course if you want to dive deeper peace!

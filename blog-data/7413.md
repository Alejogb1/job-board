---
title: "AI-Powered Web Scraping with Schema-Driven Data Extraction"
date: "2024-11-29"
id: "7413"
---

yo dude so check it

i watched this video about scraping websites with ai and it was kinda wild the dude's basically building a super-powered web scraper that doesn't break every time a website changes its layout which is like the biggest headache ever right think of it as an automated ninja that grabs data from any website you throw at it without needing to constantly rewrite the scraping code—like a seriously chill way to handle dynamic websites. it's all powered by a combo of langchain openai functions playwright beautiful soup and a sprinkle of python magic. the whole point? build a super flexible scraper you can adapt super fast

first things first the guy's whole setup is totally awesome. he's using playwright to do the actual web scraping and the main visual cue? he's showing a browser window opening and navigating to the wall street journal then appsumo he uses that to grab all the raw html then sends that raw html to openai for processing the open ai bit is handled using langchain acting as a fancy wrapper around openai’s functions which adds another layer of awesomeness. then he uses beautiful soup as a secondary scraper but he notes that its not as robust because websites tend to block that way of scraping. another thing he stressed was the importance of using the 4000 token limit on the openai responses, because even though there's a 16000 token model, he's keeping costs low. also, he's constantly talking about his neighbor’s trash blowing around. that is a fantastic visual cue and provides excellent background noise for the entire video

a couple of core concepts really stand out. one is the use of _schemas_ to define what data to extract. imagine you're after product details from an e-commerce site. instead of writing specific code to find each piece of info like title price and description you just define a simple schema like this:

```python
from pydantic import BaseModel

class Product(BaseModel):
    title: str
    price: float
    description: str
    url: str
    additional_info: str = "" #this is for any other data you want
```

then your ai powered scraper uses that schema to guide the extraction from the messy raw html the ai essentially gets instructions like "find all the products based on this structure grab their titles prices and descriptions" it's like giving the ai a blueprint it's way more efficient than manually targeting elements which also helps limit hallucinations the ai might have

the other big idea is using openai functions this is where the ai acts as a smart middleman instead of directly parsing the html itself the code sends the html to the llm and says "extract the product info from this html according to the schema" the openai function handles the complex parsing and data extraction way more reliably than just using simple regex or xpath and less prone to breaking. here's a glimpse of how the openai function call might look

```python
import openai

def extract_data(html: str, schema: str):
    response = openai.FunctionCall.create(
        model="gpt-3.5-turbo-0613",
        functions=[
            {
                "name": "extract_product_info",
                "description": "Extracts product information from HTML based on a schema",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "html": {"type": "string"},
                        "schema": {"type": "string"},  #Schema would be as a Json
                    },
                    "required": ["html", "schema"],
                },
            },
        ],
        function_call={"name": "extract_product_info", "arguments": f'{{"html": "{html}", "schema": "{schema}"}}'},
    )
    extracted_data = response['choices'][0]['message']['function_call']['arguments']
    return extracted_data

# Example usage:
html_content = "<html> ... your HTML ... </html>" #this is the output from the playwright scraper
schema_json = Product.schema_json(indent=4) #this converts the Pydantic schema to JSON

extracted_data = extract_data(html_content, schema_json)
#The result is then converted back into a Pydantic object
```

we’re basically telling openai "here's the task here's the data use your super-smart brain to do the hard stuff" and that's a fantastic thing especially if you're not familiar with the internal structure of the websites you're scraping.

the final piece is playwright itself. this is a bit more involved but basically it's like a headless browser it does all the webpage loading and javascript execution behind the scenes. it's why the guy's scraper is so robust to changes in website structure. it's like saying, "hey website i'm acting like a real browser so don't block me" here’s a little snippet showing basic playwright usage:

```python
from playwright.async_api import async_playwright

async def scrape_website(url: str):
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(url)
        html_content = await page.content()
        await browser.close()
        return html_content

# Example usage:
url = "https://www.example.com"
html = await scrape_website(url)
print(html) #This html is then passed to the openai function
```

and that’s essentially the whole thing the resolution is that you've built a crazy-flexible web scraper that can handle virtually any website and its changes using the power of ais and python it's not just about getting data it's about making a system that adapts and evolves with the websites it targets. it's a huge win for anyone who's ever dealt with the nightmare of constantly updating scraping scripts. the guy mentions building a fastapi server on top of this which would then allow you to use this code for front-end development, but that’s another topic for another day. pretty cool right

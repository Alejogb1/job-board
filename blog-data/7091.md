---
title: "How LLMs Use Real-Time Grading & Datasets to Redefine Quality"
date: "2024-11-29"
id: "7091"
---

dude so this video was like a total deep dive into evaluating LLMs using other LLMs it's nuts right the whole point was to figure out how good different search providers are not just by looking at the answers they spit out but by actually _grading_ their answers using another AI like a super meta-level analysis. think of it as AI judging AI totally wild stuff

okay so first things first visual cues man the guy’s screen was packed with code mostly python and he was constantly switching between different API clients perplexity google gemini EXA and tavali all these different search engines another major visual was the lsmith dashboard it was showing all these traces from his previous runs—kinda like a map of all the API calls he made finally a killer graph showed the evaluation results a direct comparison of how each search provider did at answering his test questions. that was pretty sweet

now some key concepts the whole shebang hinges on this idea of "LM as a judge" we're not just looking at whether an LLM gives the _right_ answer we’re using a second LLM to assess the _quality_ of the response is it well-written concise accurate insightful all that jazz it’s like having a smart grading robot instead of a human.

he also talked about two types of evaluations offline and online offline is like taking a bunch of past API calls analyzing them all at once and online is grading responses _as they happen_ in real time like imagine a chatbot getting an instant score for every response super useful for monitoring performance and making sure everything's running smoothly

the second big idea is building a dataset in lsmith lsmith is this awesome tool basically a super fancy logbook for tracking all your LLM calls it lets you see all the inputs outputs and timing info for each request but the cool part is you can use it to create datasets by grabbing these traces and organizing them then we can actually use these traces to grade the whole thing is then about having a reference response alongside the LLM's response so we can directly compare them but even if we didn't have perfect reference answers the second LLM could still judge the responses based on how good they seem—like assessing its “vibe” or whatever

okay so let’s get to the resolution the guy basically built a system to objectively assess several search APIs using an LLM as a judge he used the lsmith tool to track calls create datasets and run his evaluations he even tossed in a little something about a handy library called judges that provides pre-built evaluation prompts based on existing research papers—makes the whole process a lot simpler

now for some actual code this is super casual obviously but it captures the essence

```python
# setting up the perplexity client, this is simplified
import openai  # assuming they used openai as a wrapper

openai.api_key = "YOUR_PERPLEXITY_API_KEY"

def perplexity_query(query):
  response = openai.Completion.create(
    model="text-davinci-003", #or whatever perplexity offers
    prompt=f"Answer the following question using web search:\n{query}",
    max_tokens=150,
  )
  return response["choices"][0]["text"]

# example query
result = perplexity_query("what's the capital of france")
print(result)
```

see how simple that is it’s a basic wrapper around an API call he did similar things for other providers the core idea is always the same—send a query get a response

next a snippet showing how he might fetch traces from lsmith

```python
from langchain.smith import SmithClient  # or whatever the relevant import is

client = SmithClient.from_env()

# fetch traces with certain tags
traces = client.fetch_traces(tags=["search_battle"])

# build a dataset from the traces
dataset = []
for trace in traces:
    dataset.append({
      "query": trace.inputs["query"],
      "reference": trace.reference, # assumed structure  he had his own schema
      "response": trace.outputs["response"]
    })
```

basically pulling data and structuring it for the next step the evaluator

last but not least the evaluator itself he used GPT-4 as the judge

```python
import openai

openai.api_key = "YOUR_OPENAI_API_KEY"

def evaluate_response(query, reference, response):
    prompt = f"""
    You are a teacher grading a student's exam.
    Here's the question: {query}
    Here's the correct answer: {reference}
    Here's the student's answer: {response}

    Grade the student's answer from 0 to 1.
    0: Completely incorrect or contradictory.
    1: Completely accurate.
    Provide a short explanation for your grade in JSON format: `{{"score": 0.8, "reason": "Mostly accurate, but some minor details were missing."}}`
    """
    response = openai.Completion.create(
        model="gpt-4",
        prompt=prompt,
        max_tokens=150,
    )
    # parse the JSON response to extract score and reason  error handling omitted for brevity

    return response["choices"][0]["text"]

# example
evaluation = evaluate_response("What day is november 19th 2024", "November 19th, 2024", "October 24th, 2024")
print(evaluation)
```

this is a simplified version but shows the core idea prompting GPT-4 to act as a judge this is where the magic happens the LLM decides how good each response is and then the results are aggregated and visualized

that's the gist of it man a wild ride through the world of LLM evaluation totally mind blowing stuff and way more involved than i initially thought. hope this helps let me know if you want more deets on any specific part peace!

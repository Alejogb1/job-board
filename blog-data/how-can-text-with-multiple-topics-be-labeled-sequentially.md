---
title: "How can text with multiple topics be labeled sequentially?"
date: "2024-12-23"
id: "how-can-text-with-multiple-topics-be-labeled-sequentially"
---

Let’s tackle this. I've wrestled with multi-topic text labeling quite a bit over the years, especially back when I was building a document routing system for a legal firm. The challenge wasn't just identifying the topics *present* within a single document, but also figuring out the *sequence* of those topics. Simple multi-label classification just wouldn't cut it; we needed something that could understand the flow of discourse. This led me down a pretty deep rabbit hole into sequence labeling techniques.

The core issue, as you've hinted, is the inherent sequential nature of text. Unlike, say, classifying an image which is generally treated as a single entity, text progresses linearly, and topics often shift and transition as the narrative unfolds. Treating the entire document as a bag-of-words or applying a naive multi-label approach loses this critical sequential information. We need models that can understand the interdependencies between words and phrases, and how they contribute to the overall topic progression.

There are several effective techniques we can leverage here. My personal favorite for a project like this, because it strikes a balance between performance and computational cost, is using a combination of word embeddings with recurrent neural networks, particularly LSTMs (Long Short-Term Memory) or GRUs (Gated Recurrent Units). These architectures are explicitly designed to process sequential data, remembering previous states to inform their processing of subsequent input.

Here’s the basic idea. We begin by representing each word in our text as a vector, a process known as word embedding. Techniques like Word2Vec, GloVe, or even more advanced transformer-based embeddings (like those generated by BERT) are suitable for this. These embeddings transform words into a high-dimensional space where semantically similar words are positioned closer together. Then, the sequence of word embeddings is fed into the LSTM or GRU network. The recurrent nature of the network allows it to capture temporal dependencies, effectively "reading" the text sequentially and keeping track of the context at each point in the sequence.

The output of the RNN at each time step is a vector that represents the understanding of the text up to that point. These output vectors are then typically fed into a fully connected layer followed by a softmax activation, allowing us to predict, for each word in the text, the probability of each possible topic label. It’s crucial that the final output layer outputs a probability distribution for every word position, allowing for multiple topics along the sequence. We're essentially generating a sequence of labels, matching the sequence of input words.

To illustrate this, let’s consider a simplified Python example using Keras and TensorFlow. This is, of course, a highly abstracted example and would require significant tuning and data preparation in a real application. However, it serves to highlight the core mechanisms.

```python
import tensorflow as tf
from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Sample data - simplified for illustration
texts = [
    "This is a sentence about finance, then some legal stuff, and then back to financial markets.",
    "The contract details were complex. The financial implications were significant.",
    "The meeting was about the budget. Afterwards, the law firm discussed strategy."
]
labels = [
    ["finance", "finance", "finance", "legal", "legal", "legal", "finance", "finance", "finance"],
    ["legal", "legal", "legal", "legal", "finance", "finance", "finance"],
    ["finance", "finance", "finance", "legal", "legal", "legal", "legal"]
]

# 1. Tokenize and prepare the data
tokenizer = Tokenizer(num_words=1000) # Restrict vocabulary for simplicity
tokenizer.fit_on_texts(texts)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(texts)
max_length = max([len(seq) for seq in sequences])
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')

label_set = set()
for sublist in labels:
  label_set.update(sublist)

label_to_index = {label: i for i, label in enumerate(sorted(label_set))}
num_labels = len(label_to_index)
encoded_labels = np.zeros((len(labels), max_length, num_labels))
for i, sublist in enumerate(labels):
  for j, label in enumerate(sublist):
      encoded_labels[i,j,label_to_index[label]] = 1.0

# 2. Define the model
model = Sequential([
    Embedding(len(word_index) + 1, 128, input_length=max_length), # Word embeddings
    LSTM(128, return_sequences=True), # LSTM layer for sequence learning
    TimeDistributed(Dense(num_labels, activation='softmax')) # Output layer with softmax
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# 3. Train the model (simplified)
model.fit(padded_sequences, encoded_labels, epochs=20, verbose=0)

# 4. Example prediction (simplified)
test_text = "More legal stuff, followed by budget discussions."
test_seq = tokenizer.texts_to_sequences([test_text])
test_padded = pad_sequences(test_seq, maxlen=max_length, padding='post')
predictions = model.predict(test_padded)

# Map predictions back to labels
predicted_labels = [sorted(label_set)[np.argmax(pred)] for pred in predictions[0]]
print (f"The predicted labels are: {predicted_labels}")

```

The `TimeDistributed` layer is especially crucial here. It allows us to apply the same `Dense` layer independently to each time step of the LSTM output.

While RNNs have their strengths, I’ve found that for longer texts or those with complex dependencies, transformer-based models often provide better results. Models like BERT, RoBERTa, or similar architectures can handle longer-range dependencies through their attention mechanisms. The core idea is similar: we feed the text into the transformer and then use a classification head on top to predict labels at each word token. Transformers usually perform sub-word tokenization, which is more nuanced than word level.

Here’s another Python snippet, now demonstrating using a pre-trained BERT model via the `transformers` library:

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import numpy as np

# Load a pretrained tokenizer and model for token classification
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForTokenClassification.from_pretrained("bert-base-uncased", num_labels=3)  # Using num_labels=3 for the example

label_to_index = {'finance': 0, 'legal': 1, 'other':2}
index_to_label = {0: 'finance', 1: 'legal', 2: 'other'}

# Prepare the text
texts = [
    "This is a sentence about finance, then some legal stuff.",
    "The contract details were complex. The financial implications were significant.",
     "The meeting was about the budget. Afterwards, the law firm discussed strategy."
]

labels = [
    ['finance', 'finance', 'finance', 'finance', 'finance', 'legal', 'legal', 'legal'],
    ['legal', 'legal','legal','legal', 'finance', 'finance', 'finance', 'finance','finance'],
    ['finance', 'finance','finance','finance', 'legal', 'legal', 'legal','legal','legal']
]


def encode_labels(tokenized_inputs, labels):
  encoded_labels = []
  for i,label_sequence in enumerate(labels):
      word_ids = tokenized_inputs.word_ids(batch_index=i)
      previous_word_idx = None
      label_ids = []
      for word_idx in word_ids:
        if word_idx is None or word_idx == previous_word_idx:
            label_ids.append(-100)
        else:
            label_ids.append(label_to_index[label_sequence[word_idx]])
        previous_word_idx = word_idx
      encoded_labels.append(label_ids)
  return encoded_labels


tokenized_inputs = tokenizer(texts, padding=True, return_tensors="pt",is_split_into_words=False)
encoded_labels = encode_labels(tokenized_inputs,labels)
encoded_labels = torch.tensor(encoded_labels)

# Example usage
model.train()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
for i in range(5):
  optimizer.zero_grad()
  outputs = model(**tokenized_inputs,labels=encoded_labels)
  loss = outputs.loss
  loss.backward()
  optimizer.step()


def predict(text):
  model.eval()
  tokenized_input = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
  with torch.no_grad():
      outputs = model(**tokenized_input)
  predictions = torch.argmax(outputs.logits, dim=2).cpu().numpy()
  predicted_labels = []
  for i, prediction in enumerate(predictions[0]):
       word_ids = tokenized_input.word_ids(batch_index=i)
       previous_word_idx = None
       for idx, word_idx in enumerate(word_ids):
           if word_idx is None or word_idx == previous_word_idx:
             continue
           else:
              predicted_labels.append(index_to_label[prediction[idx]])
           previous_word_idx = word_idx
  return predicted_labels

test_text = "More legal stuff, followed by financial budget discussions."
predicted_labels = predict(test_text)
print (f"The predicted labels are: {predicted_labels}")
```
(Note: This example requires `transformers` and PyTorch to be installed).

Finally, it's worth mentioning Conditional Random Fields (CRFs). CRFs are a powerful technique used in sequence labeling where, rather than labeling tokens independently, the model learns the dependencies between adjacent labels. They are often used on top of the output of LSTMs or transformers, further enhancing the accuracy by considering label sequences holistically. For example, in the legal and finance text above, a CRF could ensure that the 'legal' label is more likely to follow another 'legal' label than 'finance', if that dependency is present in the data, rather than performing a purely independent classification. Implementing CRFs directly can be tricky, but libraries like `sklearn-crfsuite` simplify this process considerably.

```python
from sklearn_crfsuite import CRF
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from nltk import word_tokenize
from sklearn.preprocessing import LabelEncoder
import nltk
nltk.download('punkt')

# Sample Data (simplified to work with CRF)
texts = [
    "This is a sentence about finance, then some legal stuff, and then back to financial markets.",
    "The contract details were complex. The financial implications were significant.",
    "The meeting was about the budget. Afterwards, the law firm discussed strategy."
]
labels = [
    ["finance", "finance", "finance", "legal", "legal", "legal", "finance", "finance", "finance","finance","finance","finance"],
    ["legal", "legal", "legal", "legal", "finance", "finance", "finance","finance","finance"],
    ["finance", "finance", "finance", "finance", "legal", "legal", "legal", "legal", "legal","legal","legal"]
]

# Preprocess for CRF
def word2features(sent, i):
  word = sent[i]
  features = {
    'bias': 1.0,
    'word': word.lower(),
    'word[-3:]': word[-3:].lower(),
    'word[-2:]': word[-2:].lower(),
    'word.isupper()': word.isupper(),
    'word.istitle()': word.istitle(),
    'word.isdigit()': word.isdigit(),
  }
  if i > 0:
    prev_word = sent[i-1]
    features.update({
        '-1:word': prev_word.lower(),
        '-1:word[-3:]': prev_word[-3:].lower(),
        '-1:word[-2:]': prev_word[-2:].lower(),
        '-1:word.isupper()': prev_word.isupper(),
        '-1:word.istitle()': prev_word.istitle(),
        '-1:word.isdigit()': prev_word.isdigit(),
    })
  else:
      features['BOS'] = True
  if i < len(sent)-1:
    next_word = sent[i+1]
    features.update({
        '+1:word': next_word.lower(),
        '+1:word[-3:]': next_word[-3:].lower(),
        '+1:word[-2:]': next_word[-2:].lower(),
        '+1:word.isupper()': next_word.isupper(),
        '+1:word.istitle()': next_word.istitle(),
        '+1:word.isdigit()': next_word.isdigit(),
    })
  else:
    features['EOS'] = True
  return features

def sent2features(sent):
    return [word2features(sent, i) for i in range(len(sent))]
def sent2labels(labels):
   return labels

# Process data
tokenized_texts = [word_tokenize(text) for text in texts]
X = [sent2features(sent) for sent in tokenized_texts]
y = [sent2labels(label) for label in labels]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train CRF Model
crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)
crf.fit(X_train, y_train)

# Evaluate
y_pred = crf.predict(X_test)
print(classification_report(y_test,y_pred))


# Example prediction
test_text = "More legal stuff, followed by budget discussions."
test_sent = word_tokenize(test_text)
test_features = sent2features(test_sent)
predicted_labels = crf.predict([test_features])[0]
print (f"The predicted labels are: {predicted_labels}")
```
(This example requires `scikit-learn`, `sklearn-crfsuite` and `nltk`).

For a more rigorous understanding of sequence labeling with LSTMs, I highly recommend reading *“Recurrent Neural Network Based Language Models”* by Mikolov et al. (2010). It provides fundamental insights into the architecture and principles of recurrent networks applied to textual data. For deep dives into transformers, "Attention is All You Need" by Vaswani et al. (2017) is a must-read. This paper introduced the transformer architecture that underlies many modern NLP models, and finally, for sequence labeling with CRFs, "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data" by Lafferty et al. (2001) provides a thorough explanation of the underlying theory and mechanics.

In my experience, these approaches, with proper tuning and data preparation, can effectively capture the sequential transitions of topics within a text, enabling robust sequential text labeling. It's a fascinating field where theoretical concepts are rapidly transformed into practical solutions.

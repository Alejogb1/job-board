---
title: "Hybrid Search with Pinecone, Weaviate, pgvector, and Reranking"
date: "2024-11-29"
id: "5023"
---

dude so this video was a total rollercoaster a wild ride through the world of vector databases and hybrid search basically this guy's building a search app and he's been wrestling with the whole "how do i actually find stuff" problem for ages he's tried everything from naive rag which is like throwing darts blindfolded at a haystack to multihop queries which sounds way more strategic but still kinda sucked the whole point of the video was to show how combining keyword search with similarity search – what he calls hybrid search – totally kicks ass

one of the first things that jumped out was him talking about perplexity that $2 billion (or is it a billion? who even knows anymore) search engine he points out that their ceo basically spilled the tea – vector embeddings alone aren't a magic bullet the real work happens _before_ you do fancy reranking with semantic similarity getting a solid base of search results is the hard part this was a huge takeaway – it's not just about shiny embeddings it's about the whole damn pipeline

another key moment was when he introduced the dataset – a bunch of legal cases from kaggle a thousand rows worth of case details perfect for testing search strategies he was clearly not a lawyer so this was pretty cool he showed the csv which was like super simple case id case name case text verdict the standard stuff

the whole thing hinges on embeddings these are basically numerical representations of text that let you compare documents based on semantic meaning instead of just word matches he uses openai's embeddings models specifically v3 large and v3 small this is where things get fun he talked a bunch about dimensionality reduction – taking those huge 3000+ dimensional embeddings and shrinking them down to 256 while still getting great results this saves a ton of money on vector database storage and i totally get that

here's a snippet showing how he generated embeddings using the openai api and langchain langchain handles the dimensionality reduction magic for him because math is hard okay

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma

#your openai key here obviously
openai_api_key = "YOUR_API_KEY"

embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key, model="text-embedding-ada-002")

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

texts = ["this is some text","and this is some more text"]

docs = text_splitter.split_documents(texts)

db = Chroma.from_documents(docs, embeddings)

# now you can query the db
query = "what's this about"
results = db.similarity_search(query)

print(results)
```

then he moved on to vector databases – these are basically specialized databases for storing and searching embeddings he tried three: pine cone wev8 and his personal favorite – postgres with the pgvector extension i think he really likes the postgres setup – its like the poor man’s solution for this kind of stuff it works but may be slower

here’s a glimpse of his pine cone code note how he’s using both dense (from openai) and sparse (from bm25) vectors for hybrid search

```python
import pinecone

# your pinecone api key
pinecone.init(api_key="YOUR_PINECONE_API_KEY", environment="YOUR_PINECONE_ENVIRONMENT")

index = pinecone.Index("your-index-name")

# example dense and sparse vectors
dense_vector = [0.1, 0.2, 0.3]  # from openai
sparse_vector = [1, 0, 0]       # from bm25

# upserting
index.upsert(vectors=[
    ( "id1", dense_vector, {"sparse": sparse_vector})
])

# querying (hybrid search using dot product)
query_dense = [0.05, 0.1, 0.15]
query_sparse = [0, 1, 0]

results = index.query(vector=query_dense,sparse_vector = query_sparse, top_k=1, include_metadata=True)
print(results)

```

he showed how to set up the pine cone client, create an index, upsert the embeddings and then do both pure similarity search and hybrid search – pine cone lets you use a fusion function to combine the dense and sparse vector scores i felt this part was well explained

another cool piece of code involved batching his embeddings requests to openai – which cut the processing time from 50 seconds to less than a second smart move here’s a super simplified example

```python
import openai

openai.api_key = "YOUR_API_KEY"

texts = ["text1", "text2", "text3", "text4", "text5"]  # a list of texts

# batching the requests
response = openai.Embedding.create(input=texts, model="text-embedding-ada-002")

embeddings = [item['embedding'] for item in response['data']]
print(embeddings)
```

he also showed postgres with pgvector + vex for a similar approach it was cool to see how you can achieve much of the same functionality without the monthly cloud bills

finally he used gina ai for reranking – this was his final step he merged the results from his different search methods sent them to gina and let its model re-order them based on relevance he mentioned coherence was another choice but gina's api was a total pain in the ass it lacks documentation but in the end it worked

the resolution? hybrid search completely smoked pure similarity search combining keyword and semantic similarity gave him far more accurate results especially when searching for very specific information within his legal document dataset – this proved that sometimes the old school methods combined with modern AI techniques can yield powerful results this wasn’t just some theoretical exercise he showed it working across different vector databases – proving his point convincingly

it was a long video but it was full of practical tips and relatable struggles his casual style made it easy to follow even for someone like me – a total noob he made a pretty convincing case for hybrid search and highlighted the importance of considering the entire search pipeline not just the fancy embeddings plus he saved a bunch of money by reducing dimensions i'm definitely going to try some of this stuff in my own projects and honestly i now want to learn more about the intricacies of vector databases that was an awesome tutorial

---
title: "How can I log Keras `fit` output to a file?"
date: "2025-01-30"
id: "how-can-i-log-keras-fit-output-to"
---
Monitoring training progress is essential in deep learning, particularly for long-running experiments.  Directly capturing the output of Keras' `fit` method to a file enables post-hoc analysis, debugging, and tracking of model performance across training epochs without requiring constant monitoring of the console.  This is something I've routinely implemented when training models on remote servers, where access to a live console is not always feasible.  While Keras doesn't offer a direct file output parameter within `fit`, we can achieve this functionality by leveraging callbacks and Python's standard file I/O capabilities.

The core principle involves creating a custom callback that intercepts the logs generated by `fit` at the end of each epoch or batch. These logs, which typically include metrics like loss, accuracy, and validation metrics, are then written to a specified file. The `Callback` class from `tensorflow.keras.callbacks` provides the necessary framework for this. We extend this class and override the `on_epoch_end` and `on_batch_end` methods to perform the logging operation. It's crucial to understand that `on_epoch_end` provides aggregated metrics for the complete epoch, while `on_batch_end` provides metrics for the batch. Selecting which method to use depends on the desired frequency of logging. I typically prefer logging at the end of each epoch, as this is sufficient for most tracking needs and reduces the log file size.

Here is a code example that demonstrates writing epoch-level metrics to a file:

```python
import tensorflow as tf
import os

class FileLogger(tf.keras.callbacks.Callback):
    def __init__(self, log_file_path):
        super(FileLogger, self).__init__()
        self.log_file_path = log_file_path
        self.file = None

    def on_train_begin(self, logs=None):
        self.file = open(self.log_file_path, 'w')
        header = 'Epoch, Loss, Accuracy'
        if self.params.get('metrics') is not None:
            for metric_name in self.params['metrics']:
                if metric_name not in ['loss', 'accuracy']:
                     header += f', {metric_name}'

        if self.validation_data is not None:
            header += ", val_loss, val_accuracy"
            if self.params.get('metrics') is not None:
                 for metric_name in self.params['metrics']:
                    if metric_name not in ['loss', 'accuracy']:
                        header += f', val_{metric_name}'


        self.file.write(header + '\n')

    def on_epoch_end(self, epoch, logs=None):
      logs = logs or {}
      log_values = f"{epoch+1},{logs.get('loss',0):.4f},{logs.get('accuracy',0):.4f}"
      if self.params.get('metrics') is not None:
          for metric_name in self.params['metrics']:
             if metric_name not in ['loss', 'accuracy']:
                  log_values += f',{logs.get(metric_name,0):.4f}'

      if self.validation_data is not None:
        log_values += f",{logs.get('val_loss',0):.4f},{logs.get('val_accuracy',0):.4f}"

        if self.params.get('metrics') is not None:
            for metric_name in self.params['metrics']:
                if metric_name not in ['loss', 'accuracy']:
                    log_values += f',{logs.get(f"val_{metric_name}",0):.4f}'


      self.file.write(log_values + '\n')


    def on_train_end(self, logs=None):
      if self.file:
        self.file.close()


# Example usage
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])
dummy_data = tf.random.normal((100,10))
dummy_labels = tf.random.uniform((100,1),minval=0,maxval=2,dtype=tf.int32)
log_file = 'training_log.csv'


if os.path.exists(log_file):
    os.remove(log_file)

model.fit(dummy_data, dummy_labels, epochs=5, callbacks=[FileLogger(log_file)])
```

In this example, I've created a `FileLogger` callback that, upon initialization, receives the path to the log file. The `on_train_begin` method opens the log file in write mode and writes a comma separated header. The header includes 'Epoch, Loss, Accuracy' as a minimum, but will append the names of other metrics being tracked if they exist. The same logic applies to validation metrics which will be added to the header if the model is being validated. The `on_epoch_end` method extracts the epoch number and the current values of loss, accuracy and any additional metrics being tracked as specified by the user, writing them to the file in CSV format, as well as writing the validation loss and metrics if validation is occurring. Finally the `on_train_end` method closes the file after training. I have included the removal of the file if it exists at the start to prevent appending to previous training logs, but it could be configured as needed. This approach enables simple processing of the resulting log data with tools like Pandas.

Sometimes, capturing metrics at a higher frequency, such as at the end of each batch, can be valuable for detailed performance analysis. This is particularly relevant when training with very large datasets and small batch sizes, where significant performance variation may occur between batches. The following code provides an example of logging at the end of each batch by using the `on_batch_end` method:

```python
import tensorflow as tf
import os

class BatchFileLogger(tf.keras.callbacks.Callback):
    def __init__(self, log_file_path):
        super(BatchFileLogger, self).__init__()
        self.log_file_path = log_file_path
        self.file = None

    def on_train_begin(self, logs=None):
        self.file = open(self.log_file_path, 'w')
        header = 'Batch, Loss, Accuracy'
        if self.params.get('metrics') is not None:
             for metric_name in self.params['metrics']:
                if metric_name not in ['loss', 'accuracy']:
                     header += f', {metric_name}'


        self.file.write(header + '\n')


    def on_batch_end(self, batch, logs=None):
      logs = logs or {}
      log_values = f"{batch+1},{logs.get('loss',0):.4f},{logs.get('accuracy',0):.4f}"

      if self.params.get('metrics') is not None:
          for metric_name in self.params['metrics']:
                if metric_name not in ['loss', 'accuracy']:
                    log_values += f',{logs.get(metric_name,0):.4f}'


      self.file.write(log_values + '\n')


    def on_train_end(self, logs=None):
        if self.file:
             self.file.close()

# Example usage
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy','Precision', 'Recall'])
dummy_data = tf.random.normal((100,10))
dummy_labels = tf.random.uniform((100,1),minval=0,maxval=2,dtype=tf.int32)
log_file = 'batch_log.csv'

if os.path.exists(log_file):
    os.remove(log_file)

model.fit(dummy_data, dummy_labels, epochs=5, batch_size = 32, callbacks=[BatchFileLogger(log_file)])
```

This `BatchFileLogger` operates similarly to the previous example but leverages the `on_batch_end` method instead of `on_epoch_end`. Each batch's metrics are written to the specified file. This can produce a larger log file, but provides a more granular view of the training process. Itâ€™s worth noting that the validation metrics are not directly accessible within the batch callback. If you require validation metrics to be logged with this frequency, you would need to use the `validation_split` parameter within `fit`, or provide `validation_data` and perform validation on each batch in the `on_batch_end` method. I typically prefer to use epoch based metrics for validation, as this tends to be more representative of the model's true performance.

Further customization can be introduced, such as modifying the logging format, writing different metrics, or using a different file format. For instance, you might choose to log the learning rate at each epoch or incorporate timestamps.  The next example adds a timestamp and a learning rate to the log file:

```python
import tensorflow as tf
import os
import datetime

class AdvancedFileLogger(tf.keras.callbacks.Callback):
    def __init__(self, log_file_path):
        super(AdvancedFileLogger, self).__init__()
        self.log_file_path = log_file_path
        self.file = None

    def on_train_begin(self, logs=None):
        self.file = open(self.log_file_path, 'w')
        header = 'Timestamp, Epoch, Loss, Accuracy, Learning Rate'
        if self.params.get('metrics') is not None:
            for metric_name in self.params['metrics']:
                if metric_name not in ['loss', 'accuracy']:
                     header += f', {metric_name}'

        if self.validation_data is not None:
            header += ", val_loss, val_accuracy"
            if self.params.get('metrics') is not None:
                 for metric_name in self.params['metrics']:
                    if metric_name not in ['loss', 'accuracy']:
                        header += f', val_{metric_name}'

        self.file.write(header + '\n')

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        timestamp = datetime.datetime.now().isoformat()
        learning_rate = self.model.optimizer.lr.numpy()
        log_values = f"{timestamp},{epoch+1},{logs.get('loss',0):.4f},{logs.get('accuracy',0):.4f},{learning_rate:.6f}"
        if self.params.get('metrics') is not None:
           for metric_name in self.params['metrics']:
                if metric_name not in ['loss', 'accuracy']:
                     log_values += f',{logs.get(metric_name,0):.4f}'

        if self.validation_data is not None:
            log_values += f",{logs.get('val_loss',0):.4f},{logs.get('val_accuracy',0):.4f}"

            if self.params.get('metrics') is not None:
                for metric_name in self.params['metrics']:
                     if metric_name not in ['loss', 'accuracy']:
                          log_values += f',{logs.get(f"val_{metric_name}",0):.4f}'


        self.file.write(log_values + '\n')

    def on_train_end(self, logs=None):
        if self.file:
             self.file.close()

# Example usage
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
               loss='binary_crossentropy', metrics=['accuracy','Precision', 'Recall'])
dummy_data = tf.random.normal((100,10))
dummy_labels = tf.random.uniform((100,1),minval=0,maxval=2,dtype=tf.int32)
log_file = 'advanced_log.csv'

if os.path.exists(log_file):
    os.remove(log_file)


model.fit(dummy_data, dummy_labels, epochs=5, callbacks=[AdvancedFileLogger(log_file)])
```

This example demonstrates how to expand on the logging functionality by capturing a timestamp and the current learning rate of the optimizer. I have used the same logic as in the first example for logging metrics. In my experience, this approach provides a comprehensive record of training progress, especially for longer training runs.

For further study, I recommend reviewing the Keras documentation on callbacks, particularly the `Callback` class and its associated methods. The official TensorFlow documentation also provides insights into accessing optimizer state and metric information during training. A solid understanding of file I/O in Python is beneficial for customizing logging behaviour, for example exploring the use of the `csv` module. Exploring more advanced techniques with file handlers can also provide ways to control memory consumption and allow for more flexible logging techniques. Finally, a good reference on general deep learning practices can clarify what are key statistics to track during training and the most efficient ways to represent them.

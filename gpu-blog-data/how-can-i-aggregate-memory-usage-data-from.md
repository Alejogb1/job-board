---
title: "How can I aggregate memory usage data from Python's memory-profiler?"
date: "2025-01-30"
id: "how-can-i-aggregate-memory-usage-data-from"
---
Memory profiling in Python, particularly with tools like `memory-profiler`, often generates per-line reports that are excellent for pinpointing individual allocations but less suitable for understanding aggregate usage patterns over time or across function calls. Direct analysis of the raw output becomes cumbersome when dealing with complex programs. My experience in optimizing large-scale data processing pipelines has shown the necessity for programmatic aggregation to make this data actionable. Therefore, we need strategies to extract, summarize, and interpret the per-line data generated by `memory_profiler`.

The core challenge is that `memory-profiler`’s `@profile` decorator outputs data directly to standard output or a file. It does not inherently provide structured data suitable for immediate manipulation. Thus, we require a parsing and aggregation layer. The basic workflow I've found successful involves three steps: using `memory_profiler`, parsing the output, and finally aggregating the parsed information.

Let's begin with the typical usage of `memory-profiler`. The tool is implemented as a decorator, meaning you can add `@profile` to any Python function to enable memory tracking. When this decorated function executes, `memory-profiler` writes detailed, line-by-line reports indicating memory consumption at each point of execution. The raw output format consists of line numbers, memory consumption, and code snippets, typically separated by whitespace.

The first approach I employed, and still find beneficial for simple aggregation tasks, is based on simple text processing with regular expressions. The `memory_profiler` output format is consistent enough to allow for straightforward parsing. Here’s an example showing how to accomplish this. I am assuming you already have the `memory-profiler` installed ( `pip install memory-profiler`).

```python
import re
from collections import defaultdict

def parse_memory_log(log_text):
  """Parses memory profiler log output and returns aggregated data.
  
  Args:
    log_text: A string containing the memory profiler output.
  
  Returns:
    A dictionary where keys are function names and values are lists of memory 
    consumption values (in MB) from each execution.
  """
  
  function_data = defaultdict(list)
  current_function = None
  
  for line in log_text.splitlines():
    function_match = re.match(r"Filename: .*\sFunction: (\w+)\s.*", line)
    if function_match:
      current_function = function_match.group(1)
      continue

    memory_match = re.match(r"\s*\d+\s+([\d\.]+)", line)  
    if memory_match and current_function:
      memory_usage = float(memory_match.group(1))
      function_data[current_function].append(memory_usage)
  
  return function_data

# Example usage:
if __name__ == '__main__':
  log_output = """
Filename: example.py
  Line #    Mem usage    Increment  Line Contents
================================================
     3    5.207 MiB    0.000 MiB  @profile
     4                             def my_function():
     5    5.207 MiB    0.000 MiB      x = 10
     6    5.207 MiB    0.000 MiB      y = 20
     7    5.207 MiB    0.000 MiB      z = x + y
     8    5.207 MiB    0.000 MiB      return z

Filename: example.py
  Line #    Mem usage    Increment  Line Contents
================================================
    11    5.207 MiB    0.000 MiB  @profile
    12                             def other_function():
    13    5.207 MiB    0.000 MiB      a = [1,2,3,4,5]
    14    5.207 MiB    0.000 MiB      b = len(a)
    15    5.207 MiB    0.000 MiB      return b
    
Filename: example.py
  Line #    Mem usage    Increment  Line Contents
================================================
     3    5.207 MiB    0.000 MiB  @profile
     4                             def my_function():
     5    5.207 MiB    0.000 MiB      x = 10
     6    5.207 MiB    0.000 MiB      y = 20
     7    5.207 MiB    0.000 MiB      z = x + y
     8    5.207 MiB    0.000 MiB      return z
"""
  
  aggregated_data = parse_memory_log(log_output)
  for func_name, memory_values in aggregated_data.items():
    print(f"Function: {func_name}, Memory Usage: {memory_values}")
```

In this code snippet, the `parse_memory_log` function employs regular expressions to extract function names and corresponding memory usage values. The `defaultdict` structure allows collecting memory measurements from multiple calls to the same function without special handling of existing keys. The regular expression `r"Filename: .*\sFunction: (\w+)\s.*"` identifies lines indicating the start of a new function's profile, while `r"\s*\d+\s+([\d\.]+)"` extracts the memory usage value in MB. This approach is effective for many cases, providing a useful aggregate view. The output produced will group the memory usage values by the function in which they were measured, like:

```
Function: my_function, Memory Usage: [5.207, 5.207]
Function: other_function, Memory Usage: [5.207]
```

The previous approach relies on parsing text, which can be somewhat brittle if the formatting of `memory-profiler`'s output changes. A more robust solution involves subclassing the `LineProfiler` class used by `memory-profiler` to capture the information directly in a structured format. This is my preferred method for more complex profiling scenarios. Below is an example implementation:

```python
from memory_profiler import LineProfiler

class CustomLineProfiler(LineProfiler):
    def __init__(self):
        super().__init__()
        self.function_data = defaultdict(list)

    def _add_line(self, frame, lineno, mem_usage, increment):
      func_name = frame.f_code.co_name
      self.function_data[func_name].append(mem_usage)

    def get_aggregated_data(self):
        return self.function_data

# Example usage:
if __name__ == '__main__':
  def my_function():
      x = 10
      y = 20
      z = x + y
      return z

  def other_function():
    a = [1,2,3,4,5]
    b = len(a)
    return b

  profiler = CustomLineProfiler()
  profiler.enable_by_count()

  my_function()
  other_function()
  my_function()
  
  profiler.disable_by_count()
  aggregated_data = profiler.get_aggregated_data()
  for func_name, memory_values in aggregated_data.items():
    print(f"Function: {func_name}, Memory Usage: {memory_values}")
```

Here, I subclass the standard `LineProfiler` and override its `_add_line` method. The overridden method now collects the memory usage and associated function names into an internal data structure (`self.function_data`). This avoids parsing text output and directly obtains the required information. After profiling, `get_aggregated_data()` returns the dictionary. This approach offers better type safety and resilience to changes in `memory-profiler`'s output formatting, and outputs the same type of information as the previous example.

Both of the previous examples give the memory usage at each line call, however they do not provide an aggregate view, i.e. an average memory usage. We can easily implement the latter. This modified version calculates the *average* memory consumption for each function:

```python
from memory_profiler import LineProfiler
from collections import defaultdict

class AvgMemoryLineProfiler(LineProfiler):
    def __init__(self):
        super().__init__()
        self.function_data = defaultdict(lambda: {'total_usage': 0.0, 'count': 0})

    def _add_line(self, frame, lineno, mem_usage, increment):
        func_name = frame.f_code.co_name
        self.function_data[func_name]['total_usage'] += mem_usage
        self.function_data[func_name]['count'] += 1

    def get_average_data(self):
      average_data = {}
      for func_name, values in self.function_data.items():
        average_data[func_name] = values['total_usage'] / values['count']
      return average_data

# Example usage:
if __name__ == '__main__':
  def my_function():
      x = 10
      y = 20
      z = x + y
      return z

  def other_function():
    a = [1,2,3,4,5]
    b = len(a)
    return b

  profiler = AvgMemoryLineProfiler()
  profiler.enable_by_count()

  my_function()
  other_function()
  my_function()
  
  profiler.disable_by_count()
  aggregated_data = profiler.get_average_data()
  for func_name, avg_memory in aggregated_data.items():
      print(f"Function: {func_name}, Average Memory Usage: {avg_memory:.3f} MiB")
```

In this version, the `_add_line` method now accumulates the total memory usage and the call count for each function. The `get_average_data` method then divides the total usage by the count to calculate the average memory usage. This produces output in the format of:

```
Function: my_function, Average Memory Usage: 5.207 MiB
Function: other_function, Average Memory Usage: 5.207 MiB
```

When dealing with memory-intensive applications, I've found these aggregate views indispensable. It allows a focus on hotspots without the noise of individual line-by-line fluctuations. The subclassing approach, while requiring some initial setup, is superior in the long run due to its improved stability. For deeper investigation into specific functions, I typically revert to the standard `memory-profiler` output.

For resources on enhancing your understanding, I'd recommend exploring documentation on the Python standard library's `collections` module for more advanced data structures, the `re` module for detailed information on regular expressions, and the `memory-profiler`’s own documentation to better grasp its mechanisms. There are also excellent materials available on object oriented programming techniques that will allow you to more readily implement such subclasses. These resources offer comprehensive insights into related concepts and will improve your ability to tackle similar data analysis challenges.

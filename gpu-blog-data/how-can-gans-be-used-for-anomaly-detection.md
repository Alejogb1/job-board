---
title: "How can GANs be used for anomaly detection?"
date: "2025-01-30"
id: "how-can-gans-be-used-for-anomaly-detection"
---
Generative Adversarial Networks (GANs), primarily known for their image generation capabilities, can be effectively leveraged for anomaly detection by exploiting their underlying mechanisms of learning and reproducing data distributions. My experience in developing machine learning solutions for industrial automation has shown that GANs provide a nuanced approach to identifying deviations from normal system behavior, often surpassing the limitations of traditional statistical methods.

At their core, GANs consist of two neural networks, a generator (G) and a discriminator (D), engaged in an adversarial game. The generator attempts to create synthetic data samples that mimic the real training data distribution, while the discriminator’s task is to distinguish between these synthetic samples and the real ones. Through iterative training, the generator becomes increasingly proficient at producing realistic outputs, and the discriminator becomes better at identifying fakes. This process implicitly captures the structure and patterns of the training data. In anomaly detection, this learned representation of “normal” data becomes the key to identifying “abnormal” data points.

The central idea is to train the GAN solely on normal data. Once trained, the generator’s learned representation will be optimized to create samples that lie within the normal data distribution. An anomalous input, which was not part of the training data, will likely be poorly reconstructed by the generator. Consequently, the discriminator, trained to recognize normal samples generated by the generator, will also find this reconstructed anomalous sample to be of poor quality. Thus, anomalies are characterized by both high reconstruction errors and poor discriminator scores.

A crucial aspect of this methodology is the use of reconstruction error. After training, when a new data point is input into the trained generator, the network will attempt to create a sample that resembles its learned distribution. For normal data, the reconstructed output should be very similar to the input. However, an anomalous data point, lying far outside the training data distribution, will be poorly reconstructed. The difference between the input and the reconstructed output can then be quantified, often by utilizing a distance metric such as mean squared error (MSE). A higher reconstruction error indicates a more probable anomaly. This concept is also applicable when the training data represents time-series data, where sequences outside the normal range of movement or parameter fluctuation are identified as anomalies.

Additionally, discriminator scores provide a further indication of anomalies. Normal samples will typically have high discriminator scores because they are similar to the generator’s outputs from training data. Conversely, anomalies will not be effectively generated, resulting in low discriminator scores. Hence, both the reconstruction error and discriminator score need to be examined in order to make a final decision on whether or not the sample is anomalous. Combining the reconstruction error and discriminator scores provides a more robust and reliable anomaly detection technique.

Here are three code examples demonstrating this approach, using conceptualized Python code utilizing libraries like TensorFlow or PyTorch:

**Example 1: Anomaly Detection on 2D Data Using MSE**

This example illustrates anomaly detection using a simple GAN trained on 2D data points and focuses solely on the reconstruction error.

```python
import numpy as np
import tensorflow as tf

# Generate synthetic normal 2D data (replace with your actual data)
normal_data = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], 500)

# Define Generator (simplified for demonstration)
def make_generator(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
        tf.keras.layers.Dense(2, activation='linear')
    ])
    return model

# Define Discriminator (simplified)
def make_discriminator(input_dim):
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  return model


# Define GAN model
latent_dim = 2
generator = make_generator(latent_dim)
discriminator = make_discriminator(2)


#Training function (simplified - real implementations should utilize more optimized loss and gradient descent techniques).
def train_gan(generator, discriminator, data, latent_dim, epochs = 500):
    generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002)
    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)
    for epoch in range(epochs):
      for batch in data:
        noise = tf.random.normal(shape=(batch.shape[0], latent_dim))
        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
          gen_images = generator(noise)
          disc_real_output = discriminator(batch)
          disc_fake_output = discriminator(gen_images)

          gen_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(disc_fake_output), disc_fake_output)
          disc_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(disc_real_output), disc_real_output) + \
                       tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(disc_fake_output), disc_fake_output)
        gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)
        gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
        generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))
        discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))


    return generator, discriminator



# Train the GAN
data = tf.data.Dataset.from_tensor_slices(normal_data).batch(32)
trained_gen, trained_disc = train_gan(generator, discriminator, data, latent_dim)

# Anomaly detection using reconstruction error (MSE)
def detect_anomaly(gen, input_point):
    noise = tf.random.normal(shape=(1, latent_dim))
    reconstructed_point = gen(noise)
    error = tf.reduce_mean(tf.square(input_point - reconstructed_point)).numpy()
    return error

# Example anomaly
anomaly_point = np.array([[5, 5]], dtype=np.float32)
reconstruction_error = detect_anomaly(trained_gen, anomaly_point)
print(f"Reconstruction Error for Anomaly Point: {reconstruction_error}")

normal_test_point = np.array([[0, 0]], dtype = np.float32)
reconstruction_normal_error = detect_anomaly(trained_gen, normal_test_point)
print(f"Reconstruction Error for Normal Point: {reconstruction_normal_error}")

```

This example demonstrates the core concept of using reconstruction error. The generator reconstructs data based on its learned distribution, so anomalies will naturally have higher reconstruction errors. Note that this is a simplified representation, where the latent vector is used for generating data and thus the reconstruction process is indirect.

**Example 2: Anomaly Detection on Time-Series Data**

This example illustrates how a GAN can detect anomalies in a time-series signal by using recurrent layers (LSTMs).

```python
import tensorflow as tf
import numpy as np
# Generate synthetic normal time series data (replace with your data)
sequence_length = 20
normal_timeseries = np.random.rand(1000, sequence_length, 1)

# Define Generator
def make_timeseries_generator(latent_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(None, latent_dim)),
        tf.keras.layers.LSTM(128, return_sequences=False),
        tf.keras.layers.Dense(sequence_length, activation='linear'),
        tf.keras.layers.Reshape((sequence_length,1))

    ])
    return model

# Define Discriminator
def make_timeseries_discriminator():
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(None, 1)),
        tf.keras.layers.LSTM(128, return_sequences=False),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

latent_dim = 1
generator = make_timeseries_generator(latent_dim)
discriminator = make_timeseries_discriminator()


#Training function (similar training to 1)
def train_gan(generator, discriminator, data, latent_dim, epochs = 1000):
    generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002)
    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)
    for epoch in range(epochs):
      for batch in data:
        noise = tf.random.normal(shape=(batch.shape[0], latent_dim))
        noise = tf.reshape(noise,(noise.shape[0], 1, noise.shape[1] ))
        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
          gen_images = generator(noise)
          disc_real_output = discriminator(batch)
          disc_fake_output = discriminator(gen_images)

          gen_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(disc_fake_output), disc_fake_output)
          disc_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(disc_real_output), disc_real_output) + \
                       tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(disc_fake_output), disc_fake_output)

        gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)
        gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
        generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))
        discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))

    return generator, discriminator
# Train the GAN
data = tf.data.Dataset.from_tensor_slices(normal_timeseries).batch(32)
trained_gen, trained_disc = train_gan(generator, discriminator, data, latent_dim)


# Anomaly detection using reconstruction error and discriminator score
def detect_anomaly_timeseries(gen, disc, input_sequence):
    noise = tf.random.normal(shape=(1, latent_dim))
    noise = tf.reshape(noise,(noise.shape[0], 1, noise.shape[1] ))
    reconstructed_sequence = gen(noise)
    error = tf.reduce_mean(tf.square(input_sequence - reconstructed_sequence)).numpy()
    disc_score = disc(reconstructed_sequence).numpy()[0][0]
    return error, disc_score

# Example anomaly
anomaly_timeseries = np.random.rand(1, sequence_length, 1) + 2  # Added a shift to make it an anomaly
reconstruction_error, discriminator_score = detect_anomaly_timeseries(trained_gen, trained_disc, anomaly_timeseries)
print(f"Reconstruction Error for Anomaly Time Series: {reconstruction_error}, Discriminator Score: {discriminator_score}")

normal_test_time_series = np.random.rand(1, sequence_length, 1)
reconstruction_error_normal, discriminator_score_normal = detect_anomaly_timeseries(trained_gen, trained_disc, normal_test_time_series)
print(f"Reconstruction Error for Normal Time Series: {reconstruction_error_normal}, Discriminator Score: {discriminator_score_normal}")
```

This demonstrates the applicability of GANs to temporal data, using LSTMs to capture sequential dependencies. We also include discriminator score to provide extra information.

**Example 3: Utilizing Discriminator Scores for Anomaly Detection**

This example demonstrates anomaly detection based solely on the discriminator scores after training. This approach is computationally cheaper than the reconstruction-based method but can also be less sensitive.

```python
import tensorflow as tf
import numpy as np

# Reusing 2D Data and generator/discriminator from Example 1

normal_data = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], 500)

# Define Generator (simplified for demonstration)
def make_generator(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
        tf.keras.layers.Dense(2, activation='linear')
    ])
    return model

# Define Discriminator (simplified)
def make_discriminator(input_dim):
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  return model


# Define GAN model
latent_dim = 2
generator = make_generator(latent_dim)
discriminator = make_discriminator(2)


#Training function (simplified - real implementations should utilize more optimized loss and gradient descent techniques).
def train_gan(generator, discriminator, data, latent_dim, epochs = 500):
    generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002)
    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)
    for epoch in range(epochs):
      for batch in data:
        noise = tf.random.normal(shape=(batch.shape[0], latent_dim))
        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
          gen_images = generator(noise)
          disc_real_output = discriminator(batch)
          disc_fake_output = discriminator(gen_images)

          gen_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(disc_fake_output), disc_fake_output)
          disc_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(disc_real_output), disc_real_output) + \
                       tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(disc_fake_output), disc_fake_output)
        gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)
        gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
        generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))
        discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))


    return generator, discriminator


# Train the GAN
data = tf.data.Dataset.from_tensor_slices(normal_data).batch(32)
trained_gen, trained_disc = train_gan(generator, discriminator, data, latent_dim)

# Anomaly detection using discriminator scores
def detect_anomaly_discriminator_score(disc, input_point):
    score = disc(np.array([input_point], dtype = np.float32)).numpy()[0][0]
    return score


anomaly_point = np.array([5, 5], dtype=np.float32)
discriminator_score_anomaly = detect_anomaly_discriminator_score(trained_disc, anomaly_point)
print(f"Discriminator Score for Anomaly: {discriminator_score_anomaly}")


normal_test_point = np.array([0, 0], dtype = np.float32)
discriminator_score_normal = detect_anomaly_discriminator_score(trained_disc, normal_test_point)
print(f"Discriminator Score for Normal Data: {discriminator_score_normal}")

```

This example showcases anomaly detection using the discriminator’s assessment of the input data sample. Normal data tends to have a higher discriminator score than anomalous samples. In this approach the anomaly detection is computationally cheaper as the generator does not need to be used.

For further learning, I recommend exploring resources on the following topics: *Generative Adversarial Networks*, *Anomaly Detection in Machine Learning*, and *Deep Learning with TensorFlow/PyTorch*. Books focusing on time-series analysis and practical machine learning case studies offer insights into realistic applications of these techniques. Documentation and tutorials from the deep learning libraries (TensorFlow, PyTorch) are invaluable in understanding and implementation details. Articles discussing GAN architectures, loss functions, and techniques for improving training stability can significantly enhance understanding of advanced anomaly detection methods.

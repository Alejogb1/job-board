---
title: "How to compare predicted classes to true labels in a multiclass classification model?"
date: "2025-01-26"
id: "how-to-compare-predicted-classes-to-true-labels-in-a-multiclass-classification-model"
---

In multiclass classification, evaluating model performance requires more than simple binary accuracy metrics. The challenge arises from needing to quantify not just overall correctness, but also the specific types of errors the model makes across multiple classes. Direct comparison between predicted classes and true labels demands understanding error distribution via appropriate metrics and visualization. I've wrestled with this extensively while building sentiment classifiers for various social media platforms; a nuanced approach becomes critical when dealing with a significant number of classes representing subtle emotional gradations.

My experience confirms that relying solely on accuracy is inadequate. In datasets with imbalanced class distributions, a model can achieve high accuracy by predominantly predicting the majority class, masking its inability to correctly classify minority classes. Therefore, when evaluating multiclass classification, I typically begin by considering class-specific metrics and then aggregate them to gain a comprehensive understanding.

The most foundational step involves transforming both the predicted and true labels into a format suitable for detailed comparison. Generally, true labels are represented as integers corresponding to each class. Predicted labels are typically generated by selecting the class with the highest predicted probability from the model’s output. This transformation is crucial because these discrete numerical representations allow us to use various evaluation metrics and create visualisations. To elaborate further, the predicted output is often in the form of probability distributions of every class. For instance, if the model was supposed to predict between three classes and input x produces probabilities such as \[0.2, 0.7, 0.1], the corresponding predicted class will be the index with the highest probability, 1 in this case.

The most basic comparison tool is the confusion matrix, a tabular representation where rows correspond to true labels, columns correspond to predicted labels, and the cells contain counts or percentages. This reveals not only the overall accuracy of a model, but also helps identify which classes are often confused with one another. When using larger numbers of classes, a heatmap representation of the confusion matrix becomes particularly helpful because it facilitates immediate visual assessment. The diagonal entries represent the number of correct predictions, while off-diagonal entries indicate misclassifications, immediately highlighting problematic areas of model performance.

Several common metrics can be derived from a confusion matrix:

*   **Precision:** Calculated as `True Positives / (True Positives + False Positives)`. Precision measures how many of the instances predicted as a given class are actually correct. I tend to rely on this more when false positives are more costly than false negatives.

*   **Recall (Sensitivity):** Calculated as `True Positives / (True Positives + False Negatives)`. Recall quantifies how many instances of a given class are correctly identified by the model. This is useful when failing to identify an instance of a class is more costly than incorrect class assignment.

*   **F1-score:** Calculated as `2 * (Precision * Recall) / (Precision + Recall)`. It is the harmonic mean of precision and recall, providing a balanced view of the model’s performance, especially in situations with unbalanced datasets.

*   **Support:** Represents the number of true occurrences for each class in a dataset. This value is not a performance metric, but understanding it for each class is fundamental when dealing with imbalanced datasets.

After calculating per-class metrics, I usually move to consider average performance across the dataset using either macro or weighted averages. The macro average computes the metric independently for each class and takes the average, while the weighted average weighs each class metric by the frequency of each class. When you are particularly sensitive to the models' performance of classes with few members, it is more useful to use the weighted average as it puts more emphasis on the more frequent classes.

The following examples illustrate different facets of the comparison process:

**Example 1: Calculating Confusion Matrix**

```python
import numpy as np
from sklearn.metrics import confusion_matrix

true_labels = np.array([0, 1, 2, 0, 1, 2, 0, 1, 0, 2])
predicted_labels = np.array([0, 1, 1, 0, 2, 2, 1, 1, 0, 2])

conf_matrix = confusion_matrix(true_labels, predicted_labels)

print("Confusion Matrix:")
print(conf_matrix)

# The resulting confusion matrix will show the following:
# [[3 1 0]
#  [0 2 1]
#  [0 1 2]]
# In row 0, we can see that class 0 was labeled correctly 3 times, and misclassified as class 1 once.
# Class 1, represented by the row index 1, was correctly predicted 2 times and misclassified as classes 1 and 2 once each.
# Class 2 (row 2) was correctly classified 2 times and misclassified once as class 1.
```

This snippet utilizes the `confusion_matrix` function from scikit-learn, creating a 2D array where the \[i, j] entry represents the number of instances that have true label i and predicted label j. This reveals all types of errors the model makes, and as I have observed in several of my projects, it is the first step towards a more comprehensive analysis. I print the confusion matrix itself in this example, but I normally use a heatmap visualization for more readability.

**Example 2: Computing Class-Specific Metrics**

```python
from sklearn.metrics import classification_report
import numpy as np

true_labels = np.array([0, 1, 2, 0, 1, 2, 0, 1, 0, 2])
predicted_labels = np.array([0, 1, 1, 0, 2, 2, 1, 1, 0, 2])

report = classification_report(true_labels, predicted_labels, output_dict = True)

print("Classification Report:")
for label in report:
    if label.isnumeric():
        print(f'Class {label}:')
        for metric in report[label]:
            print(f'    {metric}: {report[label][metric]:.2f}')

print('Weighted Avg:')
for metric in report['weighted avg']:
    print(f'    {metric}: {report["weighted avg"][metric]:.2f}')

# The resulting report will show the following:
# Class 0:
#   precision: 1.00
#   recall: 0.75
#   f1-score: 0.86
#   support: 4.00
# Class 1:
#   precision: 0.50
#   recall: 0.67
#   f1-score: 0.57
#   support: 3.00
# Class 2:
#   precision: 1.00
#   recall: 0.67
#   f1-score: 0.80
#   support: 3.00
# Weighted Avg:
#   precision: 0.87
#   recall: 0.70
#   f1-score: 0.78
#   support: 10.00
```

This script uses `classification_report` to generate a detailed breakdown of metrics per class, including precision, recall, and the F1-score, and weighted averages. By inspecting the metrics for each class, it's possible to pinpoint classes the model struggles to classify. This step is critical to guide further model improvement, as I frequently need to adjust my training data, feature representation or architecture based on these results.

**Example 3: Visualizing the Confusion Matrix**

```python
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

true_labels = np.array([0, 1, 2, 0, 1, 2, 0, 1, 0, 2])
predicted_labels = np.array([0, 1, 1, 0, 2, 2, 1, 1, 0, 2])
class_labels = ['Class A', 'Class B', 'Class C']

conf_matrix = confusion_matrix(true_labels, predicted_labels)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()
# This code produces a heatmap visualization that clearly demonstrates how often each class was correctly predicted and misclassified.
# The color intensity makes it easier to instantly see the most common misclassifications and it is a great way to communicate results to non-technical teams.
```

This example builds upon the first example by visualizing the confusion matrix as a heatmap. The use of `seaborn` and `matplotlib` helps create a visual representation which immediately makes trends more discernible. Visualizations are essential for communicating results effectively with people who are not deep into the technical details of the model.

In summary, comparing predicted classes to true labels in multiclass classification requires the calculation of per-class metrics, generating a confusion matrix (and possibly visualizing it), and using different average metrics in different contexts. There is no single "best" approach, but the above described methodology has proven useful for me during several projects. The selection of the right techniques and metrics should always consider the specific requirements and business objectives of the project.

For further study, I recommend consulting materials on statistical pattern recognition and machine learning model evaluation techniques. Consider the documentation for scikit-learn, especially focusing on the metrics module. Also, texts on data visualization, specifically those addressing multivariate data visualization, will be beneficial.

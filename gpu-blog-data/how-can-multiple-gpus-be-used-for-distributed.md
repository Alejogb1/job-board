---
title: "How can multiple GPUs be used for distributed training with TensorFlow Slim?"
date: "2025-01-30"
id: "how-can-multiple-gpus-be-used-for-distributed"
---
TensorFlow Slim, while providing a convenient abstraction layer for model building, does not inherently manage distributed training across multiple GPUs. Its primary purpose is streamlining model definition, not parallelization strategies. Distributing training across multiple GPUs, especially those residing on different machines, requires careful orchestration and a deep understanding of TensorFlow's distributed execution capabilities. I've spent a significant portion of my professional career optimizing large-scale deep learning models, and achieving efficient distributed training with Slim demands a shift in perspective beyond its inherent single-device usage.

The crucial concept is transitioning from TensorFlow Slim's convenience into leveraging TensorFlow's distributed computation features, specifically using `tf.distribute.Strategy`. Although Slim doesn't directly offer distributed APIs, we integrate its model building with distributed strategies. A typical single-GPU training script with Slim would define a model using Slim's layers and then execute training within a single `tf.Session`. In a distributed environment, we abandon the `tf.Session` model entirely in favor of a managed training loop using a chosen distribution strategy.

The primary challenge lies in dividing the computational graph and the data across available GPUs. This involves synchronizing parameter updates to ensure model consistency. TensorFlow provides `tf.distribute.MirroredStrategy` for multi-GPU training within a single machine, and `tf.distribute.MultiWorkerMirroredStrategy` for training across multiple machines, each potentially having multiple GPUs. Selection of the appropriate strategy depends on the specific infrastructure available. Furthermore, data loading must be optimized for distributed environments to prevent bottlenecks. Data should ideally be split, and each process should load only its assigned portion. TensorFlow Datasets is well suited for distributed processing.

My approach involves several key steps. First, configure a distribution strategy. Second, use the strategy's context to build the model (using Slim). Then, create a training loop that leverages the strategy for distributed computation. And finally, implement distributed data loading. I have found that proper handling of batch sizes is crucial â€“ often, the effective batch size needs to be adjusted to accommodate the distribution across GPUs.

Here are three specific code examples illustrating these principles.

**Example 1: Single Machine, Multi-GPU Training with `MirroredStrategy`**

This example demonstrates multi-GPU training within a single machine using `tf.distribute.MirroredStrategy`. This strategy replicates the model on each GPU and synchronizes gradients.

```python
import tensorflow as tf
import tensorflow.contrib.slim as slim
import numpy as np

def build_model(images, num_classes):
    net = slim.conv2d(images, 64, [3, 3], scope='conv1')
    net = slim.max_pool2d(net, [2, 2], scope='pool1')
    net = slim.flatten(net, scope='flatten')
    net = slim.fully_connected(net, 256, scope='fc1')
    net = slim.fully_connected(net, num_classes, activation_fn=None, scope='fc2')
    return net

def loss_function(logits, labels):
    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))

def train_step(strategy, optimizer, loss, grads, params):
    strategy.run(optimizer.apply_gradients,
                 args=(zip(grads, params),))

def distributed_training(num_gpus=2, batch_size = 32, num_classes=10, steps=100):
    strategy = tf.distribute.MirroredStrategy()
    global_batch_size = batch_size * strategy.num_replicas_in_sync

    with strategy.scope():
      inputs = tf.keras.layers.Input(shape=(28,28,3), dtype=tf.float32)
      logits = build_model(inputs, num_classes)
      labels = tf.keras.layers.Input(shape=(), dtype=tf.int64)
      loss = loss_function(logits, labels)
      optimizer = tf.keras.optimizers.Adam(0.001)
      params = slim.get_variables_to_restore()
      grads = optimizer.get_gradients(loss, params)

    @tf.function
    def distributed_step(images, labels):
        per_replica_loss = strategy.run(loss_function, args=(build_model(images, num_classes), labels))
        loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)
        params = slim.get_variables_to_restore()
        grads = strategy.run(optimizer.get_gradients, args=(loss, params))
        train_step(strategy, optimizer, loss, grads, params)
        return loss


    dummy_images = tf.random.normal((global_batch_size, 28, 28, 3))
    dummy_labels = tf.random.uniform(shape=(global_batch_size,), minval=0, maxval=num_classes, dtype=tf.int64)
    dataset = tf.data.Dataset.from_tensor_slices((dummy_images, dummy_labels)).batch(global_batch_size)
    dist_dataset = strategy.experimental_distribute_dataset(dataset)


    for i, (images, labels) in enumerate(dist_dataset):
        loss = distributed_step(images, labels)
        print(f'Step {i+1} loss = {loss}')
        if i >= steps - 1:
          break


if __name__ == "__main__":
  distributed_training()
```

Here, `tf.distribute.MirroredStrategy()` is instantiated to handle distribution.  The `with strategy.scope():` block ensures that model variables are created in a way suitable for replication. The loss calculation and gradient calculation, as well as parameter updates, are handled via `strategy.run()`, which distributes the work across all available GPUs.  A custom data pipeline is created for the dummy data set. The `strategy.experimental_distribute_dataset` API is used to create the distributed data iterator, which distributes data across available replicas. I would usually use a proper `tf.data.Dataset` for real workloads, but here, dummy data is sufficient to demonstrate the distribution mechanics. The `global_batch_size` is used, to reflect the aggregate batch size across all available workers.

**Example 2: Multi-Machine, Multi-GPU Training with `MultiWorkerMirroredStrategy`**

This example shows multi-machine multi-GPU training using `tf.distribute.MultiWorkerMirroredStrategy`.  This is more complex because it requires a cluster configuration, so I am only demonstrating the core principle.

```python
import tensorflow as tf
import tensorflow.contrib.slim as slim
import os
import json

def build_model(images, num_classes):
    net = slim.conv2d(images, 64, [3, 3], scope='conv1')
    net = slim.max_pool2d(net, [2, 2], scope='pool1')
    net = slim.flatten(net, scope='flatten')
    net = slim.fully_connected(net, 256, scope='fc1')
    net = slim.fully_connected(net, num_classes, activation_fn=None, scope='fc2')
    return net


def loss_function(logits, labels):
  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))


def train_step(strategy, optimizer, loss, grads, params):
    strategy.run(optimizer.apply_gradients,
                args=(zip(grads, params),))


def distributed_training(num_classes=10, steps=100):
  tf_config = os.environ.get('TF_CONFIG')
  if tf_config:
        tf_config = json.loads(tf_config)
        cluster_spec = tf_config['cluster']
        task_type = tf_config['task']['type']
        task_index = tf_config['task']['index']
  else:
      cluster_spec = {'chief': ['localhost:2222'], 'worker': ['localhost:2223']}
      task_type = 'chief'
      task_index = 0
  os.environ['TF_CONFIG'] = json.dumps({
      'cluster': cluster_spec,
      'task': {'type': task_type, 'index': task_index}
    })
  strategy = tf.distribute.MultiWorkerMirroredStrategy()


  with strategy.scope():
    inputs = tf.keras.layers.Input(shape=(28,28,3), dtype=tf.float32)
    logits = build_model(inputs, num_classes)
    labels = tf.keras.layers.Input(shape=(), dtype=tf.int64)
    loss = loss_function(logits, labels)
    optimizer = tf.keras.optimizers.Adam(0.001)
    params = slim.get_variables_to_restore()
    grads = optimizer.get_gradients(loss, params)

  @tf.function
  def distributed_step(images, labels):
      per_replica_loss = strategy.run(loss_function, args=(build_model(images, num_classes), labels))
      loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)
      params = slim.get_variables_to_restore()
      grads = strategy.run(optimizer.get_gradients, args=(loss, params))
      train_step(strategy, optimizer, loss, grads, params)
      return loss

  batch_size = 32
  global_batch_size = batch_size * strategy.num_replicas_in_sync
  dummy_images = tf.random.normal((global_batch_size, 28, 28, 3))
  dummy_labels = tf.random.uniform(shape=(global_batch_size,), minval=0, maxval=num_classes, dtype=tf.int64)
  dataset = tf.data.Dataset.from_tensor_slices((dummy_images, dummy_labels)).batch(global_batch_size)
  dist_dataset = strategy.experimental_distribute_dataset(dataset)

  for i, (images, labels) in enumerate(dist_dataset):
        loss = distributed_step(images, labels)
        print(f'Step {i+1} loss = {loss}')
        if i >= steps - 1:
          break

if __name__ == "__main__":
    distributed_training()
```
Here, the `tf.distribute.MultiWorkerMirroredStrategy` is used. Crucially, this strategy requires cluster configurations, usually provided via the `TF_CONFIG` environment variable.  The code constructs a dummy configuration for a 2-node cluster.  For actual distributed training, the environment variable must be set appropriately for each process, usually by some cluster management service. Notice that the core logic within the training loop remains very similar to example 1 - this is the power of the  `tf.distribute` API, abstracting away much of the underlying complexity.

**Example 3: Distributed Data Loading with `tf.data.Dataset`**

This example highlights proper usage of `tf.data.Dataset` in a distributed context.

```python
import tensorflow as tf
import tensorflow.contrib.slim as slim
import os
import json

def build_model(images, num_classes):
  net = slim.conv2d(images, 64, [3, 3], scope='conv1')
  net = slim.max_pool2d(net, [2, 2], scope='pool1')
  net = slim.flatten(net, scope='flatten')
  net = slim.fully_connected(net, 256, scope='fc1')
  net = slim.fully_connected(net, num_classes, activation_fn=None, scope='fc2')
  return net

def loss_function(logits, labels):
  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))

def train_step(strategy, optimizer, loss, grads, params):
    strategy.run(optimizer.apply_gradients,
                args=(zip(grads, params),))

def create_dataset(global_batch_size, num_classes, num_steps):
    dummy_images = tf.random.normal((global_batch_size * num_steps, 28, 28, 3))
    dummy_labels = tf.random.uniform(shape=(global_batch_size*num_steps,), minval=0, maxval=num_classes, dtype=tf.int64)
    dataset = tf.data.Dataset.from_tensor_slices((dummy_images, dummy_labels)).batch(global_batch_size)
    return dataset

def distributed_training(num_classes=10, steps=100):
    tf_config = os.environ.get('TF_CONFIG')
    if tf_config:
        tf_config = json.loads(tf_config)
        cluster_spec = tf_config['cluster']
        task_type = tf_config['task']['type']
        task_index = tf_config['task']['index']
    else:
      cluster_spec = {'chief': ['localhost:2222'], 'worker': ['localhost:2223']}
      task_type = 'chief'
      task_index = 0
    os.environ['TF_CONFIG'] = json.dumps({
      'cluster': cluster_spec,
      'task': {'type': task_type, 'index': task_index}
    })
    strategy = tf.distribute.MultiWorkerMirroredStrategy()

    with strategy.scope():
      inputs = tf.keras.layers.Input(shape=(28,28,3), dtype=tf.float32)
      logits = build_model(inputs, num_classes)
      labels = tf.keras.layers.Input(shape=(), dtype=tf.int64)
      loss = loss_function(logits, labels)
      optimizer = tf.keras.optimizers.Adam(0.001)
      params = slim.get_variables_to_restore()
      grads = optimizer.get_gradients(loss, params)

    @tf.function
    def distributed_step(images, labels):
        per_replica_loss = strategy.run(loss_function, args=(build_model(images, num_classes), labels))
        loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)
        params = slim.get_variables_to_restore()
        grads = strategy.run(optimizer.get_gradients, args=(loss, params))
        train_step(strategy, optimizer, loss, grads, params)
        return loss

    batch_size = 32
    global_batch_size = batch_size * strategy.num_replicas_in_sync
    dataset = create_dataset(global_batch_size, num_classes, steps)
    dist_dataset = strategy.experimental_distribute_dataset(dataset)

    for i, (images, labels) in enumerate(dist_dataset):
      loss = distributed_step(images, labels)
      print(f'Step {i+1} loss = {loss}')
      if i >= steps - 1:
          break

if __name__ == "__main__":
  distributed_training()
```

This version demonstrates a `create_dataset` method that encapsulates the dataset creation. The important part is that each process (worker or chief) participates in the data loading, making sure data is distributed. The core logic remains nearly identical to the multi-worker example, further cementing the idea that `tf.distribute` is an abstraction over different training configurations.

In summary, while TensorFlow Slim provides tools for model definition, it is the  `tf.distribute` API that enables distributed training, including usage with Slim-defined models. I recommend the TensorFlow documentation for further studying `tf.distribute.Strategy`, focusing on the available strategies and their trade-offs. Additionally, the official TensorFlow tutorials on distributed training are valuable for understanding the implementation details. There is also a comprehensive guide on creating custom training loops that is beneficial. These resources are essential for developing robust and scalable distributed deep learning applications.

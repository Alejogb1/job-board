---
title: "What causes errors in TensorFlow Federated's `update_struct` function?"
date: "2025-01-30"
id: "what-causes-errors-in-tensorflow-federateds-updatestruct-function"
---
TensorFlow Federated’s `update_struct` function, often encountered in custom federated algorithms, primarily fails due to mismatches between the expected structure of the aggregated model updates and the actual structure received. Having spent considerable time debugging nuanced federated training pipelines, I've observed these failures stemming from several interconnected causes. Understanding these underlying reasons requires a detailed look at the mechanics of federated learning and how `update_struct` is intended to operate.

`update_struct` serves as the core mechanism to merge locally computed model updates on a server. These updates are typically produced during the client-side training phase, where individual clients fine-tune local model copies based on their respective datasets. The function's responsibility is to apply these client updates to a server-held model, effectively aggregating and incorporating the learning from each client round into the global model state. This requires a precisely defined structure. The client updates must match the server model's trainable variables’ structure – both in terms of nesting (e.g., layers and their sub-components) and in the individual data types of each component (e.g., weights, biases). If the expected and received structures don’t precisely align, errors will occur during the update application. This is often reflected in a combination of `ValueError`s and other TensorFlow specific errors, especially when dealing with custom model structures.

One prevalent reason for errors is modifications to the model structure after its initialization but prior to aggregation. For example, if a client-side customization unintentionally reshapes or alters the type of a model's internal tensor, the update generated at the client side will no longer be structurally compatible with the server-side model’s trainable variables. This could happen, for instance, if a client attempts to perform a data-dependent modification of the model's architecture based on a characteristic of its local data. Such changes might not only alter tensor shapes but also introduce entirely new tensors to the model state, ones not existing in the server model. In some instances, an attempt might be made to modify the order of the model weights or even introduce a scaling during the client training procedure, which will break this structural agreement.

Another key source of error lies in inconsistent definition of the model architecture between the server and the client. This mismatch is more frequent in complex federated setups where the model construction logic differs. For instance, a common pattern involves loading pre-trained weights on the server and then expecting clients to start with the same, but sometimes there can be subtle differences in the initialization. It's crucial that the exact same initialization logic used on the server must be replicated, or its outputs must be passed identically to the clients at the beginning of a training round. Discrepancies may emerge due to differing versions of the custom models being utilized, or even if parameter settings within the client model creation logic differ (e.g., varying dropout rates or weight initialization schemes), leading to varying structure in the aggregated updates. In this context, any discrepancy in the initialization or architecture directly impacts the structure of the updates generated by the client and ultimately leads to errors in `update_struct`.

The use of wrapper functions or custom layer implementations, while extremely useful in modern deep learning, also pose a risk. These functions need to be implemented in a manner that ensures the output structure and data types are aligned with the intended structure at the server-side application. If these wrapper functions are not carefully implemented and tested, especially within the client-side computation context, they might subtly alter the structure of the model updates. Specifically, custom layers that alter tensor shapes, or which perform internal reshaping of data, can introduce structural changes to the gradient outputs, and that can invalidate the `update_struct` call on the server. The same logic applies to functions that perform explicit data type conversions or operations that might subtly alter the model update's underlying shape or data representation.

Here are three code examples illustrating common scenarios:

**Example 1: Client-side model modification**

```python
import tensorflow as tf
import tensorflow_federated as tff

# Server-side model definition
def server_model_fn():
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(10, input_shape=(5,)),
      tf.keras.layers.Dense(2)
  ])
  return tff.learning.from_keras_model(
      keras_model=model,
      loss_fn=tf.keras.losses.MeanSquaredError(),
      metrics=[tf.keras.metrics.MeanSquaredError()])


# Client-side model modification (incorrect!)
def client_model_fn():
  model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(10, input_shape=(5,)),
        tf.keras.layers.Dense(4)  # Different output dimension
    ])
  return tff.learning.from_keras_model(
      keras_model=model,
      loss_fn=tf.keras.losses.MeanSquaredError(),
      metrics=[tf.keras.metrics.MeanSquaredError()])

@tff.tf_computation
def client_update(model, dataset):
  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)
  with tf.GradientTape() as tape:
        output = model(tf.ones([1,5])) # Dummy forward pass
        loss = tf.keras.losses.MeanSquaredError()(output, tf.ones_like(output))
  gradients = tape.gradient(loss, model.trainable_variables)
  return gradients

server_model = server_model_fn()
client_model = client_model_fn()

@tff.federated_computation(server_model.model_weights.type_signature,
                          tff.type_at_clients(tff.SequenceType(tf.TensorSpec(shape=(1, 5), dtype=tf.float32))))
def run_federated_round(server_weights, dataset):
    client_grads = tff.federated_map(client_update, tff.federated_broadcast(server_weights), dataset)
    return tff.federated_mean(client_grads)
# ERROR: When tff.federated_mean tries to aggregate different gradients sizes from client, update_struct will fail due to structural mismatch
# During a real federated training run, update_struct fails due to structure mismatch.
```

In this scenario, the client model has a final dense layer with 4 units instead of 2, leading to a structural mismatch in the aggregated updates. The `tff.federated_mean` call will try to combine the different shapes, which will fail.

**Example 2: Different Layer Parameters**
```python
import tensorflow as tf
import tensorflow_federated as tff

# Server-side model definition
def server_model_fn():
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(10, input_shape=(5,), kernel_initializer="ones"),
      tf.keras.layers.Dense(2, kernel_initializer="ones")
  ])
  return tff.learning.from_keras_model(
      keras_model=model,
      loss_fn=tf.keras.losses.MeanSquaredError(),
      metrics=[tf.keras.metrics.MeanSquaredError()])


# Client-side model definition (incorrect!)
def client_model_fn():
  model = tf.keras.models.Sequential([
      tf.keras.layers.Dense(10, input_shape=(5,), kernel_initializer="zeros"),
      tf.keras.layers.Dense(2, kernel_initializer="ones")
  ])
  return tff.learning.from_keras_model(
      keras_model=model,
      loss_fn=tf.keras.losses.MeanSquaredError(),
      metrics=[tf.keras.metrics.MeanSquaredError()])


@tff.tf_computation
def client_update(model, dataset):
    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)
    with tf.GradientTape() as tape:
        output = model(tf.ones([1, 5]))
        loss = tf.keras.losses.MeanSquaredError()(output, tf.ones_like(output))
    gradients = tape.gradient(loss, model.trainable_variables)
    return gradients

server_model = server_model_fn()
client_model = client_model_fn()

@tff.federated_computation(server_model.model_weights.type_signature,
                         tff.type_at_clients(tff.SequenceType(tf.TensorSpec(shape=(1, 5), dtype=tf.float32))))
def run_federated_round(server_weights, dataset):
    client_grads = tff.federated_map(client_update, tff.federated_broadcast(server_weights), dataset)
    return tff.federated_mean(client_grads)

# Although the structure is same, different initializers lead to errors as updates are different values, hence a different structure.
# When running an actual training loop the update_struct will raise errors due to mismatch of the values (NaN, or different numerical ranges)

```

Here, both server and client models have the same structure, but the kernel initializer in the first dense layer is different between server and client models. Though the model structure is the same, the numerical ranges of the updated weights differ and lead to errors during aggregation.

**Example 3: Custom Layer causing update shape difference**

```python
import tensorflow as tf
import tensorflow_federated as tff

class CustomLayer(tf.keras.layers.Layer):
    def __init__(self, units):
        super(CustomLayer, self).__init__()
        self.units = units
        self.w = None

    def build(self, input_shape):
      self.w = self.add_weight("weight", shape=(input_shape[-1], self.units), initializer="ones")

    def call(self, inputs):
        return tf.matmul(inputs, self.w)

# Server-side model definition
def server_model_fn():
    model = tf.keras.models.Sequential([
        CustomLayer(10),
    ])
    return tff.learning.from_keras_model(
        keras_model=model,
        loss_fn=tf.keras.losses.MeanSquaredError(),
        metrics=[tf.keras.metrics.MeanSquaredError()])


# Client-side model definition
def client_model_fn():
    model = tf.keras.models.Sequential([
        CustomLayer(10),
        tf.keras.layers.Dense(2) #Added additional Dense layer
    ])
    return tff.learning.from_keras_model(
        keras_model=model,
        loss_fn=tf.keras.losses.MeanSquaredError(),
        metrics=[tf.keras.metrics.MeanSquaredError()])


@tff.tf_computation
def client_update(model, dataset):
  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)
  with tf.GradientTape() as tape:
      output = model(tf.ones([1, 5]))
      loss = tf.keras.losses.MeanSquaredError()(output, tf.ones_like(output))
  gradients = tape.gradient(loss, model.trainable_variables)
  return gradients

server_model = server_model_fn()
client_model = client_model_fn()


@tff.federated_computation(server_model.model_weights.type_signature,
                           tff.type_at_clients(tff.SequenceType(tf.TensorSpec(shape=(1, 5), dtype=tf.float32))))
def run_federated_round(server_weights, dataset):
    client_grads = tff.federated_map(client_update, tff.federated_broadcast(server_weights), dataset)
    return tff.federated_mean(client_grads)
# Again the structure of client and server gradients is not equal
# When a training loop runs the update_struct will fail.

```
In this example, a custom layer is implemented and used. While the server model uses this layer only once, the client version uses the custom layer and adds a dense layer, which introduces a mismatch in the structure of the updates generated on the client-side with the model structure on the server. This is one more very frequent case during usage of custom layers.

To avoid these kinds of problems, model structure consistency is paramount. The server and client model construction must always follow the same logic. I have found it helpful to centralize model creation into one function and call that on both the client and server sides. The usage of wrapper functions must be handled with utmost care, especially when they perform any kind of tensor reshaping or data type conversions, and they must be tested thoroughly.

For further understanding, I highly recommend exploring the TensorFlow Federated documentation specifically on custom algorithms and model definition, along with thorough testing with simplified model structures. Additionally, the official TensorFlow tutorials regarding custom layers and model subclassing are a valuable resource. It is also helpful to review the source code of `update_struct` itself to understand how it handles the input arguments. Careful planning and rigorous testing within a reduced scope can mitigate these types of errors, leading to more stable and successful federated training.

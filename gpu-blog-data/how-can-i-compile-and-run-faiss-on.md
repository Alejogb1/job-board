---
title: "How can I compile and run FAISS on GPU using FasterTransformer in C++?"
date: "2025-01-30"
id: "how-can-i-compile-and-run-faiss-on"
---
The successful compilation and execution of FAISS (Facebook AI Similarity Search) on a GPU, leveraging the acceleration capabilities of FasterTransformer, demands a precise understanding of the interplay between these libraries, their dependencies, and the underlying CUDA environment. My experience working on large-scale recommendation systems has frequently involved optimizing similarity searches, and integrating FAISS with FasterTransformer for GPU acceleration presented significant performance gains but also substantial configuration challenges. This process involves building FAISS with specific CUDA options, ensuring FasterTransformer’s CUDA kernels are compatible, and writing C++ code to seamlessly integrate the two libraries.

Firstly, understanding that FasterTransformer does not directly provide FAISS-specific functionality is crucial. Instead, FasterTransformer is a collection of highly optimized CUDA kernels particularly well-suited for Transformer-based model inference, while FAISS is an indexing and search library. The integration, therefore, is not a unified package; it involves utilizing FAISS for similarity search and potentially employing the faster matrix operations of FasterTransformer in some preprocessing steps, if applicable to the use case. For example, if the high-dimensional embedding vectors passed into FAISS are generated by a transformer, the transformer inference can be accelerated with FasterTransformer, but FAISS still manages the indexing and searching.

The compilation process for FAISS to support GPU operation hinges on detecting and leveraging the installed CUDA toolkit. The recommended approach involves using CMake with appropriately configured options during FAISS compilation. Crucially, the `USE_GPU` and `CUDA_TOOLKIT_ROOT_DIR` CMake variables must be explicitly defined. For instance, if your CUDA toolkit resides in `/usr/local/cuda`, then specifying `CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda` in CMake is essential. In my projects, I found it crucial to use the exact same CUDA version that the FasterTransformer library was compiled with to avoid runtime incompatibilities. Additionally, selecting the specific CUDA architecture of the target GPU using `CUDA_ARCH` is paramount to maximize performance, such as `CUDA_ARCH="80"` for Ampere-based GPUs.

Secondly, after compilation, runtime linking is critical. FAISS relies on CUDA libraries and hence, these libraries must be found by the operating system’s dynamic linker during program execution. Setting the `LD_LIBRARY_PATH` environment variable to include the CUDA library path, as well as any FAISS build output directories, is essential. This is frequently where runtime errors related to missing CUDA or FAISS shared objects occur.

Here are three code examples demonstrating different aspects of FAISS usage with GPU support:

**Example 1: Basic GPU FAISS Indexing and Searching**

```cpp
#include <faiss/IndexFlat.h>
#include <faiss/IndexIVFFlat.h>
#include <faiss/gpu/StandardGpuResources.h>
#include <faiss/gpu/GpuIndexIVFFlat.h>
#include <vector>
#include <iostream>

int main() {
    int d = 128; // Dimension of the vectors
    int nb = 10000; // Number of vectors in the database
    int nq = 10;  // Number of query vectors

    // Generate synthetic data
    std::vector<float> xb(nb * d);
    std::vector<float> xq(nq * d);
    for (int i = 0; i < nb * d; ++i) {
        xb[i] = static_cast<float>(rand()) / RAND_MAX;
    }
    for (int i = 0; i < nq * d; ++i) {
        xq[i] = static_cast<float>(rand()) / RAND_MAX;
    }

    // Initialize GPU resources
    faiss::gpu::StandardGpuResources res;

    // CPU index (base for GPU index creation)
    faiss::IndexFlatL2 index_cpu(d);
    index_cpu.add(nb, xb.data());

    // GPU Index
    faiss::gpu::GpuIndexFlatL2 index_gpu(&res, d);
    index_gpu.copyFrom(index_cpu);


    int k = 5;
    std::vector<float> distances(nq * k);
    std::vector<faiss::Index::idx_t> indices(nq * k);

    index_gpu.search(nq, xq.data(), k, distances.data(), indices.data());


    for (int i = 0; i < nq; ++i) {
        std::cout << "Query " << i << ": ";
        for (int j = 0; j < k; ++j) {
            std::cout << indices[i * k + j] << " ";
        }
        std::cout << std::endl;
    }
     
    return 0;
}
```

This example demonstrates the core steps: generating synthetic data, initializing GPU resources, creating a CPU-based index, transferring it to a GPU index, then performing a nearest-neighbor search on the GPU. The critical elements are `StandardGpuResources` for device memory management and using FAISS's `GpuIndex` classes. It’s essential to note that CPU indexes can be used as intermediate steps for GPU index creation.

**Example 2: Using an inverted file index (IVF) on the GPU**

```cpp
#include <faiss/IndexFlat.h>
#include <faiss/IndexIVFFlat.h>
#include <faiss/gpu/StandardGpuResources.h>
#include <faiss/gpu/GpuIndexIVFFlat.h>
#include <vector>
#include <iostream>

int main() {
    int d = 128; // Dimension of the vectors
    int nb = 10000; // Number of vectors in the database
    int nq = 10;  // Number of query vectors
    int nlist = 100; // Number of Voronoi cells

    // Generate synthetic data
    std::vector<float> xb(nb * d);
    std::vector<float> xq(nq * d);
    for (int i = 0; i < nb * d; ++i) {
        xb[i] = static_cast<float>(rand()) / RAND_MAX;
    }
     for (int i = 0; i < nq * d; ++i) {
        xq[i] = static_cast<float>(rand()) / RAND_MAX;
    }


     // Initialize GPU resources
    faiss::gpu::StandardGpuResources res;

    // CPU index
    faiss::IndexFlatL2 quantizer(d);
    faiss::IndexIVFFlat index_cpu(&quantizer, d, nlist);
    index_cpu.train(nb, xb.data());
    index_cpu.add(nb, xb.data());


     // GPU Index
    faiss::gpu::GpuIndexIVFFlat index_gpu(&res, d, nlist);
    index_gpu.copyFrom(index_cpu);

    int k = 5;
    std::vector<float> distances(nq * k);
    std::vector<faiss::Index::idx_t> indices(nq * k);

    index_gpu.search(nq, xq.data(), k, distances.data(), indices.data());

    for (int i = 0; i < nq; ++i) {
        std::cout << "Query " << i << ": ";
        for (int j = 0; j < k; ++j) {
            std::cout << indices[i * k + j] << " ";
        }
        std::cout << std::endl;
    }

    return 0;
}
```

This example showcases the use of an Inverted File index (IVF), frequently used for large-scale datasets. The crucial part here is training the CPU-based IVF index, copying it to a GPU version, and subsequently running the search operation. IVF indexing generally significantly increases the search performance for large datasets at the cost of a training step.

**Example 3:  Demonstrating Custom Search Parameters**

```cpp
#include <faiss/IndexFlat.h>
#include <faiss/IndexIVFFlat.h>
#include <faiss/gpu/StandardGpuResources.h>
#include <faiss/gpu/GpuIndexIVFFlat.h>
#include <vector>
#include <iostream>

int main() {
    int d = 128; // Dimension of the vectors
    int nb = 10000; // Number of vectors in the database
    int nq = 10;  // Number of query vectors
    int nlist = 100; // Number of Voronoi cells

    // Generate synthetic data
     std::vector<float> xb(nb * d);
    std::vector<float> xq(nq * d);
    for (int i = 0; i < nb * d; ++i) {
        xb[i] = static_cast<float>(rand()) / RAND_MAX;
    }
    for (int i = 0; i < nq * d; ++i) {
        xq[i] = static_cast<float>(rand()) / RAND_MAX;
    }

    // Initialize GPU resources
    faiss::gpu::StandardGpuResources res;

     // CPU index
    faiss::IndexFlatL2 quantizer(d);
    faiss::IndexIVFFlat index_cpu(&quantizer, d, nlist);
    index_cpu.train(nb, xb.data());
    index_cpu.add(nb, xb.data());


     // GPU Index
    faiss::gpu::GpuIndexIVFFlat index_gpu(&res, d, nlist);
    index_gpu.copyFrom(index_cpu);

    int k = 5;
    std::vector<float> distances(nq * k);
    std::vector<faiss::Index::idx_t> indices(nq * k);

    // Custom parameters (adjust nprobe for trade-off between speed and accuracy)
    faiss::IVFSearchParameters params;
    params.nprobe = 10; 
    
    index_gpu.search(nq, xq.data(), k, distances.data(), indices.data(), params);


     for (int i = 0; i < nq; ++i) {
        std::cout << "Query " << i << ": ";
        for (int j = 0; j < k; ++j) {
            std::cout << indices[i * k + j] << " ";
        }
        std::cout << std::endl;
    }


    return 0;
}
```

This example demonstrates the use of custom search parameters, specifically modifying `nprobe` for an IVF index. `nprobe` dictates how many clusters the search operation investigates, controlling the accuracy and speed trade-off. Fine-tuning these search parameters is critical to optimal performance for specific datasets and use cases.

In summary, while FasterTransformer doesn't directly interact with FAISS, careful compilation of FAISS with GPU support, using consistent CUDA versions and correct paths, and appropriately initializing the necessary resources are all critical. Resource recommendations for further investigation into FAISS include the official FAISS documentation and examples, and advanced CUDA programming guides focusing on optimal memory usage and kernel launching for GPU programming. The integration primarily revolves around carefully written C++ code that leverages the distinct functionalities of the respective libraries.

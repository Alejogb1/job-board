---
title: "How do I find the indices of corresponding values in another PyTorch tensor?"
date: "2025-01-30"
id: "how-do-i-find-the-indices-of-corresponding"
---
The core challenge in identifying corresponding values across PyTorch tensors hinges on efficiently handling potential disparities in tensor shapes and value distributions.  My experience working on large-scale image processing pipelines frequently necessitates this operation, particularly when comparing feature maps generated by different convolutional layers or aligning predictions with ground truth data.  A naive approach, involving nested loops, becomes computationally prohibitive for high-dimensional tensors.  Therefore, leveraging PyTorch's optimized functions is crucial for performance.

**1. Clear Explanation**

Finding corresponding indices requires a strategy that accounts for various scenarios.  The simplest case involves tensors of identical shape where a direct comparison is sufficient. However, situations involving tensors with differing shapes or non-unique values require more sophisticated techniques.  My approach typically involves utilizing `torch.where` or `torch.nonzero` in conjunction with broadcasting and boolean indexing to efficiently locate the indices of interest.  Furthermore, pre-processing steps, such as sorting or creating lookup tables, can significantly improve the efficiency for complex cases.  The optimal method depends heavily on the specific characteristics of the input tensors and the desired outcome.

**2. Code Examples with Commentary**

**Example 1: Identical Shape, Unique Values**

This scenario represents the simplest case.  Assume we have two tensors, `tensor_a` and `tensor_b`, both of shape (5,) containing unique integer values. The goal is to find the indices in `tensor_b` where the values match those in `tensor_a`.

```python
import torch

tensor_a = torch.tensor([1, 5, 2, 8, 3])
tensor_b = torch.tensor([8, 3, 1, 5, 2])

#Direct comparison using boolean indexing.  This assumes tensor_b has all values from tensor_a.
indices = torch.where(tensor_b.unsqueeze(1) == tensor_a)[0]

print(f"Indices in tensor_b matching tensor_a: {indices}")
#Output: Indices in tensor_b matching tensor_a: tensor([2, 3, 4, 0, 1])

#Error Handling for missing values
try:
    indices = torch.where(tensor_b.unsqueeze(1) == tensor_a)[0]
except RuntimeError as e:
    print(f"Error in index finding: {e}")

```

This code leverages broadcasting (`unsqueeze(1)`) to perform an element-wise comparison between `tensor_b` and each element of `tensor_a`. `torch.where` returns the indices where the comparison is true. The `try-except` block demonstrates robust error handling for cases where not all values in `tensor_a` exist in `tensor_b`, which would raise a RuntimeError.  In a production environment, more refined error handling or alternative strategies for handling missing values would be employed.

**Example 2: Different Shapes, Unique Values**

Here, we consider `tensor_a` of shape (3,) and `tensor_b` of shape (5,). We seek indices in `tensor_b` where values match those in `tensor_a`.  This requires a more nuanced approach using broadcasting and `torch.nonzero`.

```python
import torch

tensor_a = torch.tensor([1, 5, 2])
tensor_b = torch.tensor([[8, 3, 1], [5, 2, 9], [1, 5, 2], [4,7,1],[3,8,5]])

# Broadcasting for comparison
comparison_result = tensor_b.unsqueeze(2) == tensor_a.unsqueeze(0).unsqueeze(1)

# Finding non-zero indices using nonzero
indices = torch.nonzero(comparison_result)

# Extracting relevant indices from the result
row_indices = indices[:, 0]
col_indices = indices[:, 1]
print(f"Row indices: {row_indices}")
print(f"Column indices: {col_indices}")

#Output example (order might vary depending on tensor_b content):
# Row indices: tensor([0, 1, 2, 2, 2, 0, 1, 4])
# Column indices: tensor([2, 0, 1, 2, 0, 2, 1, 0])

```

Broadcasting facilitates comparing `tensor_a` with each element of `tensor_b`. `torch.nonzero` identifies the non-zero elements (matches), providing row and column indices.  The code then extracts these indices for clarity.

**Example 3: Non-Unique Values**

Handling non-unique values necessitates careful consideration. The previous methods would only return the first occurrence of each matching value.  To retrieve all occurrences, we can use a slightly different strategy.

```python
import torch

tensor_a = torch.tensor([1, 5, 2, 1])
tensor_b = torch.tensor([8, 3, 1, 5, 2, 1, 7])

indices = []
for i, val in enumerate(tensor_a):
  indices_val = torch.where(tensor_b == val)[0]
  indices.append(indices_val)


print(f"Indices in tensor_b for each value in tensor_a: {indices}")
# Output Example (Order might vary based on the content of tensor_b):
#Indices in tensor_b for each value in tensor_a: [tensor([2, 6]), tensor([3]), tensor([4]), tensor([2, 6])]

```
This example iterates through `tensor_a`, finding all indices in `tensor_b` that match each value. The resulting `indices` list contains multiple index tensors, one for each value in `tensor_a`. This addresses the limitation of only finding the first occurrence of a value.  For extremely large tensors, more optimized techniques involving hash tables or custom CUDA kernels might become necessary.


**3. Resource Recommendations**

For a deeper understanding of PyTorch tensor manipulation, I recommend consulting the official PyTorch documentation, focusing on the sections related to tensor indexing, broadcasting, and advanced operations.  Additionally, exploring the documentation on advanced indexing techniques, particularly boolean indexing and advanced slicing, is highly beneficial.  Finally, reviewing relevant chapters in introductory and intermediate-level books on PyTorch or deep learning would be a significant asset in reinforcing these concepts.  These resources will provide a comprehensive understanding of the different approaches and their nuances.

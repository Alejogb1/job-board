---
title: "How do I train Hugging Face models with an LM head?"
date: "2025-01-30"
id: "how-do-i-train-hugging-face-models-with"
---
Training Hugging Face models with a language modeling (LM) head involves fine-tuning pre-trained transformer models for tasks requiring next-token prediction, a fundamental aspect of language generation and understanding. I've often found this essential when adapting models for specific text styles or domains, such as my past project on generating legal documents from precedents. The core process pivots around preparing input data suitable for masked language modeling or causal language modeling, followed by configuring a trainer for the selected task.

The fundamental difference lies in the modeling objective. Masked language modeling (MLM), often used with models like BERT, involves masking a portion of the input sequence and asking the model to predict the masked tokens. Causal language modeling (CLM), used with models like GPT, focuses on predicting the next token in a sequence, based on all preceding tokens. Therefore, the data preprocessing and training configuration are dependent on the chosen task.

To understand this effectively, consider the Hugging Face `transformers` library's design. It provides pre-trained models with built-in LM heads that are readily accessible. When you load a model with `AutoModelForMaskedLM` or `AutoModelForCausalLM`, the model has the necessary output layer for predicting token probabilities. However, this only initiates the process; the critical work is in preparing data and configuring the `Trainer` object.

Letâ€™s examine the process with three concrete code examples.

**Example 1: Masked Language Modeling (MLM) with BERT**

This example focuses on fine-tuning a BERT model for MLM, a common method to improve contextual understanding on specific datasets.

```python
from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments
from datasets import load_dataset
import torch

# Load a pre-trained tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")

# Load a dataset (e.g., using a smaller sample)
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1000]")

# Tokenization and data preparation
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Data collator for MLM. This handles masking tokens during training
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./bert_mlm_output",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=1000,
    save_total_limit=2,
    prediction_loss_only=True,
    report_to="none",  # Removed wandb
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset,
)

# Train the model
trainer.train()
```

*Commentary:*
This code begins by loading a pre-trained BERT model and its corresponding tokenizer. It then loads a sample from the WikiText dataset. The `tokenize_function` prepares text by padding and truncating them to a maximum length. The key is the `DataCollatorForLanguageModeling`. This object takes tokenized inputs and applies masking at a specified probability (here, 15%). This dynamic masking is essential for effective MLM training. The `Trainer` is configured with common training arguments.  Finally, the model is trained using the defined training and the data collator. Note that I've set `report_to="none"` to remove `wandb` as requested in the original query. This simplification does not hinder functionality while matching given constraints.

**Example 2: Causal Language Modeling (CLM) with GPT-2**

This example demonstrates fine-tuning a GPT-2 model for causal language modeling. This is particularly relevant to text generation tasks where the goal is to predict the next word in a sequence.

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset
import torch

# Load a pre-trained tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Load a dataset (e.g., using a smaller sample)
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1000]")

# Tokenization and data preparation
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Data collator for CLM. No masking needed. Labels are shifted inputs
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2_clm_output",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=1000,
    save_total_limit=2,
    prediction_loss_only=True,
     report_to="none", # Removed wandb
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

```
*Commentary:*
This script largely mirrors the first example but with key differences. It loads a GPT-2 model and tokenizer. In this case, no masking is necessary; the model inherently predicts the next token based on the previous ones. Consequently, the `DataCollatorForLanguageModeling` is initialized with `mlm=False`, indicating a CLM objective. The labels for the model are the same as the input, shifted by one position to the left. The training and model setup follows a similar process to the first example, with parameters tuned for CLM based fine-tuning.

**Example 3: Custom Dataset and CLM**

The prior examples used a dataset directly from Hugging Face's datasets library. This final example shows how to use a custom dataset, a practical requirement in many applications.

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from torch.utils.data import Dataset, DataLoader
import torch
import random

# Dummy custom dataset class
class CustomTextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encodings = self.tokenizer(text, padding="max_length", truncation=True, max_length=self.max_length, return_tensors="pt")
        return encodings

# Load a pre-trained tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")


# Create a dummy dataset.  In reality this would come from a file
dummy_texts = ["This is the first example sentence.", "Here is a second short one.", "A longer sentence to test the tokenizer's behavior."] * 100

# Create instance of custom dataset
custom_dataset = CustomTextDataset(dummy_texts, tokenizer)


# Data collator for CLM. No masking needed. Labels are shifted inputs
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2_custom_clm_output",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=1000,
    save_total_limit=2,
    prediction_loss_only=True,
     report_to="none", # Removed wandb
)


# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=custom_dataset,
)

# Train the model
trainer.train()

```
*Commentary:*
This example showcases the use of a custom dataset. The `CustomTextDataset` class loads text data, tokenizes it, and ensures that the data can be used with the PyTorch `DataLoader`. The custom dataset class is initialized with example text. The core training configuration remains consistent with Example 2, demonstrating the flexibility of Hugging Face's `Trainer` class to work with various data source types. This approach is essential in real-world scenarios where data is rarely available in a format directly compatible with the library.

**Resources**

While specific URLs are disallowed, several resources would be beneficial for gaining deeper understanding.

First, the official Hugging Face `transformers` documentation provides in-depth explanations of all models, tokenizers, and the `Trainer` API. The documentation often contains practical examples.

Second, explore the numerous tutorials and blog posts available online that address specific aspects of LM fine-tuning with `transformers`. These can provide valuable insights into particular modeling techniques and common pitfalls.

Third, the source code of the `transformers` library itself provides the most detailed understanding of the underlying mechanisms. You can explore how the different modules work and customize functionality when required for specific needs. Studying the examples can also be highly informative for implementing tailored solutions. By exploring these resources, you should be well-equipped to handle a variety of LM fine-tuning tasks.

---
title: "How can Logstash be used to extract nested Airflow logs and send them to Elasticsearch?"
date: "2025-01-30"
id: "how-can-logstash-be-used-to-extract-nested"
---
Airflow's nested log structure presents a significant challenge for centralized logging and analysis.  My experience working on large-scale data pipelines highlighted the limitations of relying on Airflow's default logging mechanisms, particularly when dealing with complex DAGs and intricate task dependencies.  Directly parsing these nested JSON logs within Elasticsearch proved inefficient and cumbersome.  Logstash, however, provides a robust solution for this, enabling efficient extraction and normalization of nested fields before indexing into Elasticsearch.  This response details how I've successfully leveraged Logstash's capabilities to achieve this.


**1.  Clear Explanation:**

The key to extracting nested Airflow logs lies in Logstash's filter plugins, specifically the `json` and `mutate` filters. Airflow typically logs task information in a nested JSON format.  The `json` filter parses this JSON, making the nested fields accessible. The `mutate` filter then allows for restructuring and renaming of these fields for optimal Elasticsearch indexing.  Further, employing the `grok` filter can enhance this process by identifying and extracting specific patterns from less structured log entries.


The process involves creating a Logstash configuration file that defines the input, filters, and output. The input specifies the location of the Airflow logsâ€”typically files or a directory. The filter section employs the `json` filter to parse the nested JSON, the `mutate` filter to handle field manipulation (e.g., flattening nested structures, renaming fields for clarity, and data type conversion), and potentially the `grok` filter for handling log lines that aren't strictly JSON. Finally, the output section configures the connection to Elasticsearch, specifying the index name and other relevant parameters.


This approach avoids complex custom scripts or direct Elasticsearch queries for log parsing. Logstash handles the heavy lifting, transforming the raw Airflow log data into a structured format suitable for efficient search and analysis within Elasticsearch.  My experience indicates this method is scalable and significantly improves the performance of log analysis compared to other strategies I've attempted.


**2. Code Examples with Commentary:**


**Example 1: Basic JSON Parsing and Field Renaming:**

```ruby
input {
  file {
    path => "/path/to/airflow/logs/*.log"
    codec => "json"
  }
}

filter {
  json {
    source => "message"
  }
  mutate {
    rename => { "task_instance.try_number" => "try_number" }
    rename => { "task_instance.state" => "task_state" }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "airflow-logs-%{+YYYY.MM.dd}"
  }
}
```

* **Commentary:** This configuration reads JSON logs from a specified directory.  The `json` filter parses the `message` field, assuming the entire log message is valid JSON. The `mutate` filter renames nested fields like `task_instance.try_number` and `task_instance.state` for easier querying in Elasticsearch.  The output sends the processed data to a daily-indexed Elasticsearch instance. This approach is suitable if your Airflow logs are consistently well-formatted JSON.


**Example 2: Handling Non-JSON Logs with Grok:**

```ruby
input {
  file {
    path => "/path/to/airflow/logs/*.log"
  }
}

filter {
  grok {
    match => { "message" => "%{GREEDYDATA:log_message}" }
  }
  if [log_message] =~ /^{/ {
    json {
      source => "log_message"
    }
    mutate {
      remove_field => ["log_message"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "airflow-logs-%{+YYYY.MM.dd}"
  }
}
```

* **Commentary:** This example handles potential variations in log format.  It initially uses a `grok` filter to capture the entire log message regardless of structure. Then, a conditional statement checks if the captured message is JSON using a regular expression. If it's JSON, the `json` filter parses it, and the original `log_message` field is removed. This provides a more robust solution for situations where not all logs are perfect JSON.


**Example 3: Flattening Nested Structures:**

```ruby
input {
  file {
    path => "/path/to/airflow/logs/*.log"
    codec => "json"
  }
}

filter {
  json {
    source => "message"
  }
  mutate {
    flatten => ["task_instance.execution_date", "task_instance.dag_id"]
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "airflow-logs-%{+YYYY.MM.dd}"
  }
}

```

* **Commentary:** This illustrates the `flatten` option within the `mutate` filter.  This is crucial when dealing with deeply nested structures.  The example flattens the `task_instance.execution_date` and `task_instance.dag_id` fields, creating new fields accessible without navigating nested JSON paths in Elasticsearch queries. This significantly simplifies querying and improves performance.


**3. Resource Recommendations:**

I recommend consulting the official Logstash documentation, particularly the sections on the `json`, `mutate`, and `grok` filters.  A thorough understanding of Elasticsearch's mapping capabilities is also essential for optimal schema design to efficiently store and query the extracted Airflow log data.  Finally, explore Logstash's testing capabilities to ensure your configuration correctly parses and processes the logs before deploying it to a production environment.  Thorough testing is crucial to avoid unexpected issues during operation.

---
title: "Why does Maven failsafe fail in Azure Pipelines?"
date: "2025-01-30"
id: "why-does-maven-failsafe-fail-in-azure-pipelines"
---
Maven Failsafe, designed for integration testing, frequently exhibits unexpected failures within Azure Pipelines due to a confluence of factors often absent in local development environments. I've encountered this across numerous projects, and pinpointing the root cause requires a systematic understanding of the pipeline's execution context, resource constraints, and Failsafe’s behavior under these conditions.

The core issue stems from the discrepancy between the predictable environment of a developer's machine and the ephemeral, often resource-limited, nature of Azure Pipelines build agents. Failsafe, by default, initiates a new JVM for each test class to provide isolation, a behavior that can become problematic under the constraints of a pipeline agent. This is particularly true when executing a suite of integration tests that depend on external services, network resources, or large datasets.

A primary contributor to Failsafe failures in pipelines is insufficient resource allocation. Each forked JVM requires memory and processing power. The default agent's configuration may be inadequate to handle the concurrent execution of multiple JVMs, leading to OutOfMemoryErrors (OOM) or timeouts. Furthermore, networking issues, often masked in local environments, can become apparent within the pipeline. Differences in DNS resolution, firewall configurations, or inconsistent service accessibility are frequent culprits. The non-persistent nature of agents also means that any state accumulated during the test execution won't be readily available for subsequent builds. This includes cached dependencies, downloaded libraries, and temporary files used by the tests which can lead to inconsistencies between test runs.

Furthermore, the asynchronous nature of concurrent processes within pipelines can amplify non-deterministic behavior. Failsafe test executions often rely on the assumption of consistent resource availability. Pipelines, with their variable execution times and intermittent network conditions, can disrupt these assumptions, leading to seemingly random failures.

The error messages generated by Failsafe might not always be immediately transparent. Often, the stack trace only reveals the symptom rather than the underlying cause. OOM errors may occur without explicitly showing where the memory is being consumed. Connection timeouts may occur without indicating the exact service experiencing an issue. Analyzing the entire pipeline log, including the Maven output and the agent logs, is essential to diagnose the problem.

Let's consider some practical scenarios illustrated with code examples:

**Code Example 1: Forking Issues with Memory Constraints**

```xml
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-failsafe-plugin</artifactId>
    <version>3.0.0-M5</version>
    <configuration>
        <forkCount>1C</forkCount>
        <reuseForks>false</reuseForks>
        <argLine>-Xmx1024m -XX:+UseG1GC</argLine>
    </configuration>
    <executions>
        <execution>
            <goals>
                <goal>integration-test</goal>
                <goal>verify</goal>
            </goals>
        </execution>
    </executions>
</plugin>
```

This example demonstrates a typical Failsafe configuration. `forkCount` set to “1C” uses one JVM for each available processor. Although seemingly optimal locally, this can overconsume available resources in Azure Pipelines.  `reuseForks` being set to `false` ensures that every test class runs in a fresh JVM further increasing the overhead. The `argLine` is aimed at configuring the JVM with `1024mb` of maximum heap memory and the `G1GC` garbage collector. However, if the agent provides less than adequate memory, even this explicit setting may not prevent an OutOfMemory error. The error message will likely be an OOM error or some form of a test failure due to process termination and usually be accompanied with error codes in the range of `137`, `143` or `-9` for `SIGKILL` signals.  In my experience, these errors are not always consistently reproducible but are prevalent in pipelines due to varied available resources on the agent.

**Code Example 2:  Addressing Network Instability**

```xml
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-failsafe-plugin</artifactId>
    <version>3.0.0-M5</version>
    <configuration>
         <forkCount>1</forkCount>
         <reuseForks>true</reuseForks>
         <systemPropertyVariables>
             <service.retry.attempts>3</service.retry.attempts>
             <service.retry.interval>1000</service.retry.interval>
         </systemPropertyVariables>
    </configuration>
    <executions>
        <execution>
            <goals>
                <goal>integration-test</goal>
                <goal>verify</goal>
            </goals>
        </execution>
    </executions>
</plugin>
```

Here, `forkCount` is reduced to `1`, limiting the resource consumption by the forked JVMs. Moreover `reuseForks` is set to `true`, so same JVM is used for multiple test classes. This can reduce overall startup time, but introduces dependency on test execution order and test isolation. I have found that the use of system properties to configure retry logic is a key strategy. The `service.retry.attempts` and `service.retry.interval` properties enable tests to retry API calls in the event of intermittent network failures. These variables would need to be picked up by the testing framework, most commonly the test code itself, which would need an implementation to respect these configurations. The properties improve the reliability of the integration tests in a dynamic network environment. These network issues usually show up as `java.net.SocketTimeoutException` or `java.net.ConnectException` exceptions, with the underlying connection issues not being immediately obvious.

**Code Example 3: Implicit Dependency on System Properties and Environment**

```xml
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-failsafe-plugin</artifactId>
    <version>3.0.0-M5</version>
    <configuration>
         <forkCount>1</forkCount>
        <reuseForks>false</reuseForks>
        <environmentVariables>
            <DATABASE_URL>jdbc:postgresql://host:port/database</DATABASE_URL>
        </environmentVariables>
    </configuration>
    <executions>
        <execution>
            <goals>
                <goal>integration-test</goal>
                <goal>verify</goal>
            </goals>
        </execution>
    </executions>
</plugin>
```

This code demonstrates the usage of `environmentVariables` to provide settings to the test environment.  Here, the `DATABASE_URL` is defined. Failsafe can also pick up and propagate existing environment variables present within the agent's operating system. Implicit dependencies on environment variables can introduce subtle failures if, for example, a different configuration is present in the pipeline versus a local development machine. Test failures in this context can range from connection errors to data access errors, depending on how these variables are utilized in the integration tests. The logs may show exceptions indicating an inability to establish connection to the database or incorrect data being retrieved from it. It is important to ensure consistency between environments when using environment variables.

To mitigate these issues, I recommend adopting a multi-faceted approach:

1.  **Resource Optimization:** Configure the pipeline agent with sufficient CPU and memory. Experiment with reduced fork counts and enabling `reuseForks`, especially if test cases do not have inter-dependencies that could be compromised with the feature. Carefully configure JVM settings, specifically the heap size, and garbage collection to fit the pipeline agents limits.
2.  **Robust Test Design:**  Incorporate retry mechanisms in the test cases, especially for network calls. Improve error handling logic, logging, and failure analysis within integration tests. Avoid implicit dependencies on system properties or environment variables. Explicitly provide configuration to all service and external dependencies of the test. Implement mocks and stubs, where applicable, to avoid relying on unstable or non-deterministic network services.
3.  **Pipeline Awareness:**  Log all the steps in your pipeline, especially if using custom scripts or interacting with agents or environment variables directly. Review the entire build log, including Maven’s output and Agent logs. Examine timestamps and identify potential timeouts or resource constraint issues by comparing the logs. Consider implementing specific pipeline tasks before and after the test execution to set up the environment and verify any external services on which the integration test depends.

For further information, I would suggest exploring the official Maven documentation, especially the Failsafe Plugin section, and related content in the JUnit and TestNG websites. Additionally, the Azure DevOps documentation regarding agent capabilities and pipeline tasks would be crucial in better understanding the execution context. Examining documentation for networking, error handling, and dependency management can also lead to a more stable and robust testing solution. Thorough knowledge of these resources is essential in navigating the common pitfalls encountered in Azure pipeline and ensures a much more reliable test cycle.

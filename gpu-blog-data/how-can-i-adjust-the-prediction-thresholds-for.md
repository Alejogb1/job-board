---
title: "How can I adjust the prediction thresholds for multi-label classification using the fastai library?"
date: "2025-01-30"
id: "how-can-i-adjust-the-prediction-thresholds-for"
---
Multi-label classification, unlike single-label classification, assigns multiple class labels to a single instance. Therefore, direct application of single-label prediction probability thresholds is inappropriate. Instead, the fastai library allows for adjusting thresholds, not on the final probability, but on the individual class predictions generated by the model, typically after a sigmoid activation. My experience training medical image classifiers has highlighted the importance of fine-tuning these per-class thresholds, as the cost of false positives and false negatives often varies significantly depending on the specific label and clinical context.

To clarify, multi-label classifiers often produce a probability for *each* potential class. The model typically uses a sigmoid function as its final activation, which outputs values between 0 and 1. These represent the probability that a given label is present. Without threshold adjustment, a default threshold of 0.5 is typically used: any probability greater than or equal to 0.5 is considered positive for that class, while any probability less than 0.5 is considered negative. This often performs poorly when the data is imbalanced or the clinical implications differ dramatically between false positives and false negatives across labels. This is where post-processing threshold tuning becomes critical.

Fastaiâ€™s `Learner` object provides methods and attributes to access model predictions, enabling this per-class adjustment. The core technique involves inspecting predicted probabilities after calling the `get_preds` method or the `predict` method when you have specific examples. This provides you with a tensor of probabilities, where each column corresponds to a label, or a per-label prediction.  The subsequent stage requires implementing a custom function which takes these predicted probabilities as an input and returns the final, binary predictions based on adjusted thresholds.

Let's illustrate this with code. Assume you have trained a `Learner` called `learn` on a multi-label image classification task and that `learn.dls.vocab` provides the labels.

**Example 1: Initial Predictions and Default Thresholding**

This example showcases getting predictions with the default threshold of 0.5 and printing out the original probabilities and predicted classes.

```python
import torch
from fastai.vision.all import *

# Assume 'learn' is your pre-trained Learner object
test_dl = learn.dls.test_dl(learn.dls.train.items[:10], with_labels=False)  # Create a test data loader for a subset
preds, _ = learn.get_preds(dl=test_dl)  # Get predicted probabilities

# Convert the probabilities using default threshold and the corresponding label name
default_thresh_predictions = (preds >= 0.5).long()

for i in range(len(preds)):
    print(f"Original Probabilities for instance {i+1}: {preds[i]}")
    predicted_labels = [learn.dls.vocab[j] for j in torch.nonzero(default_thresh_predictions[i]).flatten()]
    print(f"Predicted labels with 0.5 threshold: {predicted_labels}")
```

*Commentary:* In this example, we retrieve predicted probabilities using the `get_preds` method.  We then transform those probabilities into binary predictions using the default threshold of 0.5. The `torch.nonzero` function returns indices where the prediction is 1. We then translate those indices to actual labels using the `learn.dls.vocab` to produce the output. This is the baseline without threshold adjustment. This shows how raw probabilities are transformed into class predictions by thresholding.

**Example 2: Custom Threshold Adjustment**

This example shows how to apply a per-label threshold by making a custom function.

```python
def apply_custom_thresholds(probabilities, thresholds):
    """
    Applies custom thresholds to multi-label predictions.

    Args:
      probabilities: A tensor of predicted probabilities (output from model).
      thresholds: A tensor or list of thresholds, one for each label.

    Returns:
      A tensor of binary predictions.
    """
    thresholds_tensor = torch.tensor(thresholds, device=probabilities.device)
    return (probabilities >= thresholds_tensor).long()

custom_thresholds = [0.2, 0.7, 0.3, 0.8, 0.6] # Example thresholds for five labels

# Ensure we have enough labels to use for thresholding
num_labels = len(learn.dls.vocab)
if num_labels < len(custom_thresholds):
   raise ValueError(f"Threshold array is larger than the number of available classes, {num_labels}")
# Pad or truncate threshold list in case of size mismatch
custom_thresholds = custom_thresholds[:num_labels] if num_labels < len(custom_thresholds) else custom_thresholds + [0.5] * (num_labels - len(custom_thresholds))


custom_threshold_predictions = apply_custom_thresholds(preds, custom_thresholds)


for i in range(len(preds)):
    print(f"Original Probabilities for instance {i+1}: {preds[i]}")
    predicted_labels = [learn.dls.vocab[j] for j in torch.nonzero(custom_threshold_predictions[i]).flatten()]
    print(f"Predicted labels with custom thresholds: {predicted_labels}")

```
*Commentary:* This example introduces a function `apply_custom_thresholds`, which receives both raw probabilities and a list of custom thresholds, one for each class. The code first ensures that the custom threshold list is the same size as the number of labels by truncating or padding if necessary. Then, a comparison operation returns a tensor of booleans which is then converted into a binary `long` tensor, giving the adjusted multi-label predictions. Notice, the individual class predictions are not a direct probability as a result, but the output of thresholding. Finally the prediction labels are printed using the vocabulary list.

**Example 3: Iterative Threshold Optimization**

This example outlines the conceptual framework of how one might optimize thresholds rather than simply selecting by intuition as in example 2. Note this code is *not* executable.

```python
# This example is conceptual, not fully functional.  Optimization of thresholds is complex and problem-specific.
# It's best to use a validation set to measure the impact of threshold changes

def evaluate_thresholds(learner, thresholds, validation_dataloader):
    """
        Evaluates the performance of the model with given thresholds.
        Returns metric(s), typically accuracy/F1-score

    """
    # Get predictions on the validation set
    preds, target = learner.get_preds(dl=validation_dataloader)
    adjusted_preds = apply_custom_thresholds(preds, thresholds)
    # compute your desired multi-label metric between adjusted_preds and target
    return metric(adjusted_preds, target)


def optimize_thresholds(learner, initial_thresholds, validation_dataloader, num_iterations=10, learning_rate=0.1):
     """
     Optimizes the thresholds iteratively. In practice, a more refined optimization approach is often needed.
     """

     current_thresholds = initial_thresholds
     for i in range(num_iterations):
        # 1. Evaluate performance with current thresholds
         current_score = evaluate_thresholds(learner, current_thresholds, validation_dataloader)

        # 2. Adjust threshold by small step, this is a simplified example, not gradient based
         for j in range(len(current_thresholds)):
           # make a small adjustment to one threshold, make a copy to prevent side effects
           temp_thresholds = current_thresholds[:]
           temp_thresholds[j] += learning_rate
           temp_score = evaluate_thresholds(learner, temp_thresholds, validation_dataloader)
           if temp_score > current_score:
              current_thresholds[j] += learning_rate
              current_score = temp_score
           else:
               temp_thresholds = current_thresholds[:]
               temp_thresholds[j] -= learning_rate
               temp_score = evaluate_thresholds(learner, temp_thresholds, validation_dataloader)
               if temp_score > current_score:
                   current_thresholds[j] -= learning_rate
                   current_score = temp_score

     return current_thresholds


# Initialization
initial_thresholds = [0.5] * len(learn.dls.vocab) # Start with default threshold
validation_dl = learn.dls.valid

# Optimization (conceptual)
optimized_thresholds = optimize_thresholds(learn, initial_thresholds, validation_dl)

#Apply optimized thresholds

preds, _ = learn.get_preds(dl=test_dl) # Get new predictions to apply new thresholds to
optimized_predictions = apply_custom_thresholds(preds, optimized_thresholds)

for i in range(len(preds)):
    print(f"Original Probabilities for instance {i+1}: {preds[i]}")
    predicted_labels = [learn.dls.vocab[j] for j in torch.nonzero(optimized_predictions[i]).flatten()]
    print(f"Predicted labels with optimized thresholds: {predicted_labels}")

```
*Commentary:* This third example shows a conceptual iterative process. `evaluate_thresholds` would be implemented to compute an appropriate metric and `optimize_thresholds` shows how one might attempt to search for optimal thresholds by iteratively adjusting them, in this case, based on a naive search over small changes.  It is important to remember that effective threshold optimization typically requires a more sophisticated approach, and is *heavily dependent on the performance metric chosen*. The key element to grasp is the concept of adjusting them iteratively and evaluating their impact on a validation set. This example illustrates how we move beyond fixed thresholds to seek optimal thresholds for each class. It is important to note, the code for this example is not readily functional; the `metric` function needs to be replaced by a function which calculates an appropriate metric like F1 score for multilabel classification.

**Resource Recommendations:**

To further understand the concepts, I recommend studying these areas:

*   **Multi-label classification evaluation metrics:** Familiarize yourself with metrics beyond simple accuracy, such as F1-score (micro and macro), precision, and recall. Understanding these will help in tuning thresholds specific to the performance requirements of a project.
*   **Sigmoid function and its applications:** Gaining a strong understanding of sigmoid function as a final activation in neural networks is essential. Review its range, and its role in generating probabilities. This allows you to interpret the raw output of the network effectively.
*   **Optimization techniques:** Specifically, techniques relevant to threshold selection. Search methods, grid-based search, or gradient-based optimization (if your metric is differentiable) could be used. Experimenting with different methods for threshold tuning can improve the overall performance of your classifier.

In conclusion, adjusting prediction thresholds for multi-label classification is essential for aligning model predictions with real-world requirements and class imbalances. The process includes examining output probabilities, implementing custom per-class thresholding, and exploring iterative optimization methods based on metrics meaningful to your specific task.

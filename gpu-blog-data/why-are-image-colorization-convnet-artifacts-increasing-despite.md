---
title: "Why are image colorization ConvNet artifacts increasing despite accuracy and loss improvements?"
date: "2025-01-30"
id: "why-are-image-colorization-convnet-artifacts-increasing-despite"
---
The perplexing persistence of visual artifacts in colorized images generated by convolutional neural networks (ConvNets), even amidst demonstrable gains in accuracy and loss metrics, stems from a fundamental disconnect between the network's objective function and human perceptual quality. While a ConvNet might effectively minimize pixel-wise differences between its colorized output and a ground truth image (resulting in improved metrics), it can simultaneously generate artifacts that, while numerically insignificant in the loss calculation, are visually salient and distracting to the human eye. This situation arises due to a confluence of factors, primarily the limitations of common loss functions, insufficient contextual information, and the inherent non-uniqueness of the colorization problem.

The problem is not one of a network failing to learn. It is learning to minimize the loss function that has been presented, and this loss is typically calculated on a per-pixel basis. This results in a system that's good at finding a 'average' coloration, but not necessarily a 'correct' or natural coloration for individual elements. Consider a common loss function like mean squared error (MSE), where the error for each pixel is calculated independently. While this encourages color values to move toward the target, it fails to capture holistic image characteristics such as texture, edges, and the spatial coherence of colors. Small but noticeable color inconsistencies, often appearing as splotches or unnatural color transitions, might only represent tiny increases in overall loss. They therefore become an acceptable trade-off for the model in its optimization process. The network learns a mapping where it averages, rather than truly colorizes, unless that averaging specifically produces a high loss state.

Furthermore, networks trained on pixel-wise losses are inherently myopic. They may lack the receptive field required to incorporate enough contextual information to perform effective colorization. For instance, a gray pixel representing a sky region might be colorized with a consistent shade of blue if sufficient surrounding context exists, but with the pixel-wise loss it is as likely to be colored an average shade between surrounding objects rather than the correct hue. This is especially apparent along object boundaries, where insufficient context can result in color bleeding or unnatural color gradients. The network may have learned an average edge, but not the specific color transition. Increasing network depth and receptive field can mitigate this issue, but at the cost of added computational complexity and training time.

The inherent ambiguity of the colorization task also exacerbates these artifact generation tendencies. Multiple plausible colorizations might exist for the same grayscale input, especially in regions with complex textures or shadows. A ConvNet optimized solely for pixel accuracy might inadvertently learn to favor an "averaged" or smoothed colorization, rather than the one that would be judged as most natural by a human observer. This is because it finds the average to be a lower-loss outcome for pixels in that region. A loss function may find many "correct" answers, each with a similar numerical loss, but only some are visually correct.

I have encountered this first-hand while working on a historical photo colorization project. We employed a U-Net architecture, a common choice for image-to-image tasks. Initially, using a standard mean squared error loss, our results showcased acceptable numerical metrics; however, closer inspection revealed significant color artifacts. While the overall color tone was decent, there was a distinct "patchiness," with some color regions appearing disconnected from their surroundings, and strange color gradients along edges.

Here's a simplified code snippet illustrating the common use of MSE loss:

```python
import torch
import torch.nn as nn

class ColorizationModel(nn.Module):
    def __init__(self):
        super(ColorizationModel, self).__init__()
        # Simplified convolutional layers (replace with your actual network)
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 3, kernel_size=3, padding=1)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.conv2(x)
        return x

model = ColorizationModel()
criterion = nn.MSELoss()
# Example Inputs (replace with your actual data loaders):
gray_image = torch.randn(1, 1, 256, 256)
target_color_image = torch.randn(1, 3, 256, 256)
output_image = model(gray_image)
loss = criterion(output_image, target_color_image)
print(f"MSE Loss: {loss.item()}")
```
In this example, the `nn.MSELoss` is calculated directly pixel-wise, making it susceptible to averaging and failing to account for holistic color relationships.

To address these issues, we experimented with perceptual loss functions that incorporate features from pre-trained convolutional networks, such as VGG. These pre-trained networks are trained on vast amounts of image data and capture more human-aligned perceptual characteristics than simple per-pixel differences. The loss is calculated not on the raw pixels but on features derived from intermediate layers of the VGG network. This encouraged the colorization network to generate images with improved texture details, colors which aligned better, and more natural edges, even when the per pixel differences were greater in MSE terms.

```python
import torch
import torch.nn as nn
import torchvision.models as models

class VGGPerceptualLoss(nn.Module):
    def __init__(self):
        super(VGGPerceptualLoss, self).__init__()
        vgg = models.vgg16(pretrained=True).features
        self.vgg_layers = nn.Sequential(*list(vgg.children())[:23])  # Use up to conv4_3 layer
        for param in self.vgg_layers.parameters():
            param.requires_grad = False

    def forward(self, x, y):
        x_features = self.vgg_layers(x)
        y_features = self.vgg_layers(y)
        return nn.functional.mse_loss(x_features, y_features)

vgg_loss = VGGPerceptualLoss()
# Assuming output_image and target_color_image from the previous example, scaled to range [0,1]
output_image_scaled = (output_image + 1) /2
target_color_image_scaled = (target_color_image + 1)/ 2
perceptual_loss = vgg_loss(output_image_scaled, target_color_image_scaled)
print(f"Perceptual Loss: {perceptual_loss.item()}")
```
This VGG perceptual loss aims to address the previously described problems. Using this loss function we observed that artifacts were reduced, and edges were more naturally colorized, demonstrating the benefits of a perceptual approach. Note that these images will still require a standard pixel-wise loss to allow full training to occur.

Finally, adding adversarial training further improved results by introducing a discriminator network which classified images as either generated or real. The generator network must therefore learn not to 'fool' the discriminator with low-loss, average-color images, but to produce images that are both numerically close to the target and also indistinguishable from it. This forces the generation to learn a more sophisticated understanding of realistic color distributions.

```python
import torch
import torch.nn as nn
import torch.optim as optim

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        # Simplified discriminator (replace with your actual network)
        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
        self.fc = nn.Linear(128 * 64 * 64, 1) # Assuming 256x256 images

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = x.view(x.size(0), -1)
        x = torch.sigmoid(self.fc(x))
        return x

discriminator = Discriminator()
discriminator_optim = optim.Adam(discriminator.parameters(), lr=0.0002)
# During training, for the discriminator:
real_output = discriminator(target_color_image_scaled)
fake_output = discriminator(output_image_scaled.detach())
# compute loss for both real and generated and then backpropagate. 

# And for the generator itself
fake_output_for_gen = discriminator(output_image_scaled) # don't detach for the generator
# compute loss for generated images.
```

While the above code snippets provide a practical demonstration of the core principles behind reducing image colorization artifacts, itâ€™s imperative to emphasize that real-world solutions often require a more sophisticated methodology involving loss weighting, hyperparameter tuning, and careful consideration of dataset biases.

For those seeking to delve deeper into this subject, I recommend exploring research papers on perceptual losses, Generative Adversarial Networks, and different forms of image-to-image translation. The work done in the area of style transfer will provide some context, as will methods of creating synthetic datasets using a known ground truth. In addition, looking at papers relating to attention-based mechanisms in ConvNets might yield better approaches for utilizing the contextual information of an image.

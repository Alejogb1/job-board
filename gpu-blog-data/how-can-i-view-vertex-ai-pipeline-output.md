---
title: "How can I view Vertex AI pipeline output?"
date: "2025-01-30"
id: "how-can-i-view-vertex-ai-pipeline-output"
---
Vertex AI pipelines, when executed, generate various forms of output crucial for understanding pipeline performance and the resulting artifacts. As a seasoned ML practitioner, I've encountered numerous scenarios where effectively viewing these outputs was pivotal to debugging, validation, and further iterative development. Accessing and interpreting this output requires a multifaceted approach, considering the diverse components within Vertex AI.

Fundamentally, pipeline output can be categorized into three primary areas: component logs, pipeline metadata, and pipeline artifacts. Component logs consist of standard output and standard error messages generated by each individual component within the pipeline. These logs are invaluable for diagnosing component-level errors or monitoring the progression of computationally intensive tasks. Pipeline metadata contains information concerning the pipeline’s execution, such as start and end times, status, and parameter values. Artifacts represent the tangible outputs of components, like trained models, processed datasets, or evaluation metrics. Properly viewing these diverse output types necessitates employing different tools and methodologies.

I’ve found that the primary interface for accessing pipeline output within Google Cloud Platform (GCP) is the Vertex AI section of the Cloud Console. Navigating to the Pipelines section and selecting a specific pipeline run provides a centralized view of key information. This view displays the overall pipeline status and a visual representation of the Directed Acyclic Graph (DAG), where each node corresponds to a component. By clicking on a specific component, one can directly access its individual execution details, including the status, start and end times, and most importantly, the component logs.

These logs are typically accessible in text format, displaying the stdout and stderr streams associated with the component's execution. I’ve often used this view to pinpoint precise execution errors, particularly within custom components. However, simply relying on the text console for extensive log analysis can be tedious. In cases involving more complex log structures or a large volume of data, cloud logging proves to be a superior alternative. Cloud Logging aggregates all logs from GCP resources, including Vertex AI pipelines, enabling powerful filtering and querying capabilities. By filtering logs based on the pipeline ID and component name, I can effectively isolate relevant log messages and perform more sophisticated analysis, such as identifying recurring error patterns or monitoring resource consumption.

Furthermore, I frequently find it necessary to view the pipeline’s associated metadata. The metadata provides a higher-level overview of the pipeline’s execution. The information on the run provides start and end times, status, and input parameters. More importantly, it provides links to the lineage graphs which display information about inputs and outputs from each component. Understanding these linkages is vital to maintain transparency of the pipeline operation and for reproducibility. Additionally, the metadata can be programmatically accessed using the Vertex AI SDK for Python, which I frequently use for automated reporting and pipeline orchestration.

Finally, accessing the output artifacts generated by the pipeline components is a crucial part of the development workflow. These artifacts are typically stored in Cloud Storage buckets specified during pipeline creation. The Cloud Console provides direct links to these output locations from within a specific pipeline run view. Alternatively, I often use gsutil command-line tool to manage artifacts from command line interface, specifically for downloading, uploading, and deleting datasets and models from storage buckets. The pipeline metadata also provides a direct link to the artifact registry. While artifacts are usually automatically registered upon component completion, managing them effectively in the artifact registry becomes crucial when dealing with complex experimentation and model management.

To solidify my experience, let’s explore some code examples demonstrating how to access various aspects of Vertex AI pipeline output using the Python SDK.

**Example 1: Retrieving component logs using the SDK**

```python
from google.cloud import aiplatform

# Replace with your project ID and pipeline run ID
PROJECT_ID = 'your-gcp-project-id'
PIPELINE_RUN_ID = 'your-pipeline-run-id'
COMPONENT_NAME = 'your-component-name'

aiplatform.init(project=PROJECT_ID)

pipeline_run = aiplatform.PipelineJob.get(PIPELINE_RUN_ID)

component_execution = None
for task in pipeline_run.task_details:
    if task.task_name == COMPONENT_NAME:
        component_execution = task
        break

if component_execution:
    print(f"Component {COMPONENT_NAME} execution found")
    for log in component_execution.execution.get_logs():
        print(log.log)

else:
    print(f"Component {COMPONENT_NAME} not found in run {PIPELINE_RUN_ID}")
```

This snippet illustrates how to retrieve the logs associated with a specific component within a given pipeline run. The `aiplatform.PipelineJob.get()` method retrieves the specified pipeline run object. Subsequent iteration through the tasks allows us to extract a particular component based on its name. Once obtained, the `component_execution.execution.get_logs()` method returns all associated logs. This enables programmatic extraction and analysis of component-level logs, streamlining debugging workflows. This process can be generalized to extract logs from multiple components simultaneously.

**Example 2: Accessing pipeline metadata using the SDK**

```python
from google.cloud import aiplatform

# Replace with your project ID and pipeline run ID
PROJECT_ID = 'your-gcp-project-id'
PIPELINE_RUN_ID = 'your-pipeline-run-id'


aiplatform.init(project=PROJECT_ID)

pipeline_run = aiplatform.PipelineJob.get(PIPELINE_RUN_ID)

print(f"Pipeline run display name: {pipeline_run.display_name}")
print(f"Pipeline run state: {pipeline_run.state}")
print(f"Pipeline run create time: {pipeline_run.create_time}")
print(f"Pipeline run start time: {pipeline_run.start_time}")
print(f"Pipeline run end time: {pipeline_run.end_time}")
print(f"Pipeline input parameters: {pipeline_run.inputs}")
print(f"Pipeline output parameters: {pipeline_run.output_parameters}")
print(f"Pipeline labels: {pipeline_run.labels}")

```

This code segment demonstrates accessing the pipeline's metadata using the SDK. The `pipeline_run` object contains attributes representing the pipeline status, timestamps, and input/output parameters, which can be viewed via standard print statements. This serves as a starting point for extracting information about the pipeline run such as lineage graph and other vital information for audit purposes. This information can be extracted with Python SDK to perform more advanced automation tasks.

**Example 3: Listing artifacts using the SDK**

```python
from google.cloud import aiplatform

# Replace with your project ID and pipeline run ID
PROJECT_ID = 'your-gcp-project-id'
PIPELINE_RUN_ID = 'your-pipeline-run-id'
COMPONENT_NAME = 'your-component-name'


aiplatform.init(project=PROJECT_ID)

pipeline_run = aiplatform.PipelineJob.get(PIPELINE_RUN_ID)

component_execution = None
for task in pipeline_run.task_details:
    if task.task_name == COMPONENT_NAME:
        component_execution = task
        break

if component_execution:
    for output in component_execution.execution.outputs:
        print(f"Artifact name: {output.output_name}")
        print(f"Artifact URI: {output.artifacts[0].uri}")
        print(f"Artifact type: {output.artifacts[0].metadata['schema_title']}")
else:
    print(f"Component {COMPONENT_NAME} not found in run {PIPELINE_RUN_ID}")
```

This code segment demonstrates how to programmatically retrieve artifact metadata from the execution of a component. The code loops through all of the outputs and prints associated artifacts' URI, name, and schema type. This is a convenient way to obtain the storage locations of all pipeline generated artifacts and is useful in automation. This process can be easily generalized to extract the output of all components by looping through each component.

In conclusion, viewing Vertex AI pipeline output is a critical aspect of developing, debugging and managing successful machine learning pipelines. I find utilizing both the Cloud Console and the Vertex AI Python SDK essential for this task. The Cloud Console provides a user-friendly interface for quickly inspecting logs and metadata, while the Python SDK allows for programmatic access, enabling greater flexibility and automation capabilities. Further knowledge of Cloud Logging, Cloud Storage and the artifact registry are essential for managing Vertex AI pipeline outputs effectively. I have benefited greatly by exploring the official Vertex AI documentation and API reference, and I frequently consult the “Google Cloud for ML Professionals” book series, along with the cloud architecture pattern books. Each of these provides specific and practical guidance on building, debugging and managing production-grade ML pipelines.

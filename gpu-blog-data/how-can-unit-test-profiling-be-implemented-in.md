---
title: "How can unit test profiling be implemented in TeamCity?"
date: "2025-01-30"
id: "how-can-unit-test-profiling-be-implemented-in"
---
TeamCity's inherent capabilities for unit testing don't directly include built-in profiling features for test execution.  My experience integrating performance analysis into our TeamCity CI/CD pipeline involved a layered approach, combining TeamCity's build steps with dedicated profiling tools. This necessitated a shift from relying solely on TeamCity's reporting to leveraging external profiling solutions and their integration with the build process.  This allows for granular performance data beyond simple pass/fail results for unit tests.

**1.  Clear Explanation of Implementation:**

The core strategy involves instrumenting the unit test execution process to capture performance metrics and then feeding this data into TeamCity for analysis and reporting.  This requires three main components: a profiling tool capable of generating suitable reports, a method of integrating this tool into the TeamCity build process, and finally, a mechanism for displaying the profiling data within TeamCity's interface or through external dashboards.

The most straightforward approach leverages command-line profiling tools.  These tools often produce reports in standard formats (e.g., XML, JSON) that can be readily parsed and incorporated into TeamCity's build artifacts.  Post-build steps within TeamCity can then process these reports, creating custom visualizations or integrating data into existing TeamCity reports using the TeamCity REST API.  Note that the complexity increases significantly if the goal is real-time profiling within the TeamCity build agent; this typically requires dedicated profiling agents and sophisticated integration.

For simpler needs, focusing on collecting and presenting aggregate profiling results (e.g., average execution time per test, slowest tests) from a complete test run is more feasible.  This avoids the overhead of real-time analysis and simplifies integration.  Furthermore, it is vital to ensure the profiling tool has minimal performance impact during test execution, as otherwise, the profiling process itself might skew the results and impede testing efficiency.


**2. Code Examples with Commentary:**

The following examples assume a .NET environment using NUnit and dotTrace as the profiling tool; adapting this to other environments (e.g., Java, JUnit, YourKit) would require substituting appropriate commands and libraries.

**Example 1:  Basic dotTrace Integration (Batch Mode):**

```bash
# This script runs NUnit tests and profiles them using dotTrace in batch mode.

# Path to dotTrace command-line tool. Adjust as needed.
dotTraceCmd="/path/to/dotTrace.exe"

# Path to NUnit test assembly. Adjust as needed.
testAssembly="/path/to/MyTests.dll"

# Run NUnit tests and profile them
"$dotTraceCmd"  /batch /output="profilingResults.tlog"  nunit3-console.exe "$testAssembly"

# Convert the profiling results into a user-friendly format (e.g., HTML)
# ... (additional dotTrace command-line commands to convert .tlog to .html) ...

# Upload profiling results as build artifact
# ... (TeamCity specific commands using the TeamCity API or build steps to publish the .html) ...
```

This script first runs NUnit tests, using dotTrace to profile them in batch mode, generating a `.tlog` file.  Subsequent commands (not shown explicitly for brevity) then process this `.tlog` to produce a human-readable report (e.g., an HTML file) which is then uploaded as a TeamCity build artifact.  The crucial aspect is the use of command-line arguments to control dotTrace's behavior and generate a structured report.


**Example 2:  Custom Reporting with PowerShell:**

```powershell
# PowerShell script to process profiling results and generate custom reports

# Path to the profiling results
$profilingResults = "profilingResults.html"

# Function to extract key performance metrics
function Get-TestMetrics {
  param(
    [string]$reportPath
  )
  # ... (Implementation to parse the HTML report and extract relevant data, e.g., test name, execution time) ...
  return $metrics
}

# Get test metrics
$metrics = Get-TestMetrics -reportPath $profilingResults

# Create a summary report
$summaryReport = "Summary Report:`n"
foreach ($metric in $metrics) {
  $summaryReport += "$($metric.TestName): $($metric.ExecutionTime) ms`n"
}

# Save the summary report
$summaryReport | Out-File -FilePath "test_metrics.txt"

# Publish the summary report as a TeamCity artifact
# ... (TeamCity specific commands to publish the file) ...
```

This PowerShell script demonstrates the processing of the profiling data.  The `Get-TestMetrics` function (the implementation of which is omitted for brevity) parses the HTML profiling report generated by dotTrace to extract relevant performance metrics like execution time for each test.  A summary report is then generated and published as a TeamCity artifact, giving a concise overview of test performance.


**Example 3:  Simple Execution Time Logging (No External Profiler):**

This approach avoids external profiling tools, focusing on basic timing within the tests themselves.

```csharp
// C# NUnit test example with embedded timing

[Test]
public void MyTest()
{
  var stopwatch = Stopwatch.StartNew();
  // ... your test code ...
  stopwatch.Stop();
  TestContext.Progress.WriteLine($"Test {TestContext.CurrentContext.Test.Name} execution time: {stopwatch.ElapsedMilliseconds} ms");
}
```

This example uses the `Stopwatch` class in C# to measure the execution time of each test. The results are then printed to the TeamCity build log via `TestContext.Progress.WriteLine()`. While simple, this only provides basic timing information and lacks the detailed call stack analysis offered by dedicated profiling tools. This method is suitable only for very basic performance monitoring within the tests themselves.

**3. Resource Recommendations:**

For detailed profiling information, consider consulting documentation for several commercial profiling tools including YourKit, IntelliJ's profiling tools (if you're working within the IntelliJ ecosystem), and other relevant profilers available for your specific development environment.  Familiarize yourself with the command-line interfaces of these tools to ensure seamless integration into your build process.  TeamCity's documentation on build steps and artifact publishing is essential for incorporating the profiling data into your CI/CD workflow.  Finally, exploring scripting languages (PowerShell, Bash, etc.) will enhance your ability to automate report generation and data extraction.  Understanding the nuances of your chosen unit testing framework (e.g., NUnit, JUnit, pytest) and its integration capabilities is paramount.

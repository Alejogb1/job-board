---
title: "How can code be optimized for L1 cache performance?"
date: "2025-01-26"
id: "how-can-code-be-optimized-for-l1-cache-performance"
---

The most impactful optimization for L1 cache performance often stems from understanding and minimizing cache misses, particularly compulsory and capacity misses. My experience optimizing high-performance financial algorithms has shown that this is not about magical compiler switches, but rather a fundamental restructuring of memory access patterns.

The L1 cache, typically a small, very fast memory integrated directly into the CPU core, operates on the principle of locality: both temporal (accessing the same data repeatedly) and spatial (accessing data in nearby memory locations). Cache misses, where the requested data isnâ€™t present in the L1 cache, force the CPU to fetch data from slower levels of memory (L2, L3, main memory), introducing significant latency. Therefore, optimizing for L1 involves designing algorithms and data structures to maximize cache hits.

Specifically, optimizing for L1 cache efficiency means focusing on the following core areas:

1.  **Data Layout:** How data is organized in memory heavily influences cache performance. Structuring data contiguously, and in the order it is likely to be accessed, improves spatial locality. Inefficient layouts lead to unnecessary cache line loading and evictions.
2.  **Access Patterns:** Linear or predictable access patterns allow the hardware to utilize prefetching, anticipating the data needed before it is requested, which helps minimize latency. Irregular, scattered accesses lead to frequent cache misses.
3.  **Working Set Size:** The working set refers to the amount of memory an application actively utilizes during a particular period. Keeping the working set small enough to fit within the L1 cache capacity is paramount. If the working set exceeds the cache capacity, frequent eviction and reloading cycles occur.
4.  **Loop Optimizations:** Loop structures are critical for many compute-intensive tasks. Optimizing how data is accessed within loops can significantly impact performance. Techniques such as loop tiling, loop unrolling and loop fusion often help improve spatial locality and reduce loop overhead.
5.  **Data Alignment:** Ensuring data is properly aligned on memory boundaries matching cache line sizes can improve data loading efficiency. Misaligned accesses might require multiple memory loads, impacting performance.

Let's illustrate these points with code examples:

**Example 1: Array Traversal**

Consider processing a two-dimensional array. The following C++ code iterates through the array in a row-major order:

```cpp
#include <iostream>
#include <chrono>
#include <vector>

int main() {
    const int rows = 1024;
    const int cols = 1024;
    std::vector<std::vector<int>> matrix(rows, std::vector<int>(cols, 1));
    int sum = 0;

    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < rows; ++i) {
        for (int j = 0; j < cols; ++j) {
            sum += matrix[i][j];
        }
    }
    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> duration = end - start;

    std::cout << "Sum: " << sum << std::endl;
    std::cout << "Time row-major: " << duration.count() << " seconds" << std::endl;

    sum = 0;
    start = std::chrono::high_resolution_clock::now();
     for (int j = 0; j < cols; ++j) {
        for (int i = 0; i < rows; ++i) {
            sum += matrix[i][j];
        }
    }
    end = std::chrono::high_resolution_clock::now();
    duration = end - start;
   std::cout << "Sum: " << sum << std::endl;
   std::cout << "Time column-major: " << duration.count() << " seconds" << std::endl;

    return 0;
}
```
This code demonstrates that row-major access (iterating through the array row by row) is more cache friendly than column-major access (iterating through the array column by column) due to how memory is laid out. The row-major traversal exhibits better spatial locality, as elements within the same row are stored contiguously in memory. When a cache line is loaded from memory, subsequent elements within the same row are already loaded in the cache, thus avoiding additional memory accesses. The column major will suffer from worse cache performance due to having to access non-contiguous memory locations.

**Example 2: Struct of Arrays vs. Array of Structs**

Consider a scenario where you have a collection of particle objects, each with properties like position (x, y, z) and velocity (vx, vy, vz). Storing them as an array of structs will have different cache characteristics than storing the data as a struct of arrays.

```cpp
#include <iostream>
#include <vector>
#include <chrono>

struct ParticleAoS {
    float x, y, z;
    float vx, vy, vz;
};

struct ParticleSoA {
    std::vector<float> x;
    std::vector<float> y;
    std::vector<float> z;
    std::vector<float> vx;
    std::vector<float> vy;
    std::vector<float> vz;
};


int main() {
    const int num_particles = 100000;
   
   // Array of Structs
    std::vector<ParticleAoS> particlesAoS(num_particles);
    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < num_particles; ++i) {
         particlesAoS[i].x += 1.0f;
        particlesAoS[i].y += 1.0f;
    }
    auto end = std::chrono::high_resolution_clock::now();
     std::chrono::duration<double> duration = end - start;
    std::cout << "Time Array of Structs: " << duration.count() << " seconds" << std::endl;
    
    // Struct of Arrays
    ParticleSoA particlesSoA;
    particlesSoA.x.resize(num_particles);
    particlesSoA.y.resize(num_particles);
    particlesSoA.z.resize(num_particles);
    particlesSoA.vx.resize(num_particles);
    particlesSoA.vy.resize(num_particles);
    particlesSoA.vz.resize(num_particles);

    start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < num_particles; ++i) {
        particlesSoA.x[i] += 1.0f;
        particlesSoA.y[i] += 1.0f;
    }
    end = std::chrono::high_resolution_clock::now();
    duration = end - start;
   std::cout << "Time Struct of Arrays: " << duration.count() << " seconds" << std::endl;
    return 0;
}
```
The "struct of arrays" (SoA) approach will generally exhibit superior performance over the "array of structs" (AoS) when the code accesses components of the struct sequentially. For example, when looping through all x coordinates and then all y coordinates, SoA provides better spatial locality because the individual float components are contiguous in memory. AoS will load x,y,z,vx,vy, and vz, even if the operation only utilizes x and y, thus wasting memory bandwidth.

**Example 3: Loop Tiling**

Consider the multiplication of two matrices. If the matrices are very large, it may be beneficial to break the computation into smaller "tiles".

```cpp
#include <iostream>
#include <vector>
#include <chrono>

void naiveMatrixMultiply(const std::vector<std::vector<int>>& A, const std::vector<std::vector<int>>& B, std::vector<std::vector<int>>& C, int N)
{
    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < N; ++j) {
            for (int k = 0; k < N; ++k) {
                C[i][j] += A[i][k] * B[k][j];
            }
        }
    }
}

void tiledMatrixMultiply(const std::vector<std::vector<int>>& A, const std::vector<std::vector<int>>& B, std::vector<std::vector<int>>& C, int N, int blockSize)
{
    for (int i = 0; i < N; i += blockSize) {
        for (int j = 0; j < N; j += blockSize) {
             for (int k = 0; k < N; k+= blockSize) {
               for (int ii = i; ii < std::min(i+blockSize, N); ++ii) {
                   for (int jj = j; jj < std::min(j+blockSize, N); ++jj) {
                        for (int kk = k; kk < std::min(k+blockSize, N); ++kk) {
                            C[ii][jj] += A[ii][kk] * B[kk][jj];
                        }
                    }
               }
            }
        }
    }
}
int main() {
    const int N = 1024;
    std::vector<std::vector<int>> A(N, std::vector<int>(N, 1));
    std::vector<std::vector<int>> B(N, std::vector<int>(N, 1));
    std::vector<std::vector<int>> C(N, std::vector<int>(N, 0));
    int blockSize = 32;

    auto start = std::chrono::high_resolution_clock::now();
    naiveMatrixMultiply(A,B,C, N);
    auto end = std::chrono::high_resolution_clock::now();
     std::chrono::duration<double> duration = end - start;
    std::cout << "Time Naive Matrix Multiplication: " << duration.count() << " seconds" << std::endl;

     for (int i = 0; i < N; ++i) {
       for (int j = 0; j < N; ++j){
          C[i][j] = 0;
       }
    }

    start = std::chrono::high_resolution_clock::now();
    tiledMatrixMultiply(A, B, C, N, blockSize);
    end = std::chrono::high_resolution_clock::now();
    duration = end - start;
   std::cout << "Time Tiled Matrix Multiplication: " << duration.count() << " seconds" << std::endl;
    return 0;
}
```
The naive implementation of matrix multiplication can lead to frequent cache misses. Loop tiling (blocking) improves the situation by operating on blocks of the matrix that fit within the L1 cache, maximizing reuse of the data once loaded into the cache.

**Recommendations for Further Study:**

To delve deeper into cache optimization, I recommend exploring the following areas and resources:

1.  **Computer Architecture Texts:** A thorough understanding of CPU cache hierarchies is essential. Look into textbooks that cover CPU microarchitecture, including the workings of L1, L2 and L3 caches, prefetching mechanisms, and cache coherence protocols.
2.  **Performance Analysis Tools:** Familiarize yourself with profiling tools specific to your target platforms. Performance counters (such as hardware performance monitors), debuggers and profilers can help identify bottlenecks related to memory access patterns and cache misses.
3.  **Compiler Optimization Guides:** Understand compiler optimization flags and how they influence code generation and memory access patterns. Experiment with different flags and understand the assembly output to gain a more detailed view of what your compiler is doing.
4.  **Operating System Concepts:** Learn about memory management within the operating system, including virtual memory, and the impact of page faults on application performance.
5.  **Algorithm Design:** Explore algorithms and data structures tailored for cache-conscious programming. This includes examining techniques like cache-oblivious algorithms that strive to perform well across diverse cache configurations.

Optimizing for L1 cache is a nuanced process, requiring a deep understanding of both hardware architecture and coding principles. It is often an iterative process that involves careful profiling, analysis, and experimentation. While there isn't a universal "magic bullet," a conscious focus on data locality, access patterns, and working set size will provide the largest performance gains.

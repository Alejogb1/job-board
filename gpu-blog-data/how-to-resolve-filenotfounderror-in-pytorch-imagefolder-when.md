---
title: "How to resolve FileNotFoundError in PyTorch ImageFolder when .ipynb_checkpoints files are found?"
date: "2025-01-30"
id: "how-to-resolve-filenotfounderror-in-pytorch-imagefolder-when"
---
The presence of hidden `.ipynb_checkpoints` directories within image dataset folders poses a common challenge when using `torchvision.datasets.ImageFolder` in PyTorch, triggering a `FileNotFoundError` during dataset loading. This occurs because `ImageFolder` recursively searches for image files within its root directory; however, these directories are not legitimate image containers and their inclusion disrupts the expected folder structure.

`ImageFolder`, by design, expects its immediate subdirectories to represent classes, and files within these subdirectories to be image files. When `.ipynb_checkpoints` folders, which are automatically generated by Jupyter notebooks to maintain checkpoint data, appear in that directory structure, `ImageFolder` mistakenly interprets them as class folders. Upon attempting to access an image file within these non-image containing folders, it fails, raising a `FileNotFoundError`. The error message usually includes the incorrect directory path, highlighting the origin of the issue.

There are several strategies to resolve this error and ensure a clean dataset loading process. The primary approach is to filter out or ignore these checkpoint directories prior to dataset creation. This can be achieved either by manipulating the file paths passed to `ImageFolder` or by subclassing `ImageFolder` to integrate a specific filter directly into its instantiation. Ignoring these directories directly while loading is essential to maintain the intended folder structure.

The initial, and arguably most straightforward, approach involves using the `glob` module in Python to gather image file paths, precluding the inclusion of `.ipynb_checkpoints` directories. This approach avoids `ImageFolder` encountering the offending directories in the first place.

```python
import glob
import os
from torchvision import datasets
from torchvision import transforms

def create_custom_imagefolder(root_dir, transform=None):
    image_paths = []
    labels = []
    class_names = sorted(os.listdir(root_dir))
    for idx, class_name in enumerate(class_names):
        if not class_name.startswith('.'): # avoid .ipynb_checkpoints
            class_path = os.path.join(root_dir, class_name)
            for img_path in glob.glob(os.path.join(class_path, '*.*')):
                image_paths.append(img_path)
                labels.append(idx)

    custom_dataset = datasets.DatasetFolder(root=root_dir,
                                           loader=lambda x: transform(datasets.folder.default_loader(x)) if transform else datasets.folder.default_loader(x),
                                           extensions=('.jpg', '.jpeg', '.png', '.gif', '.bmp'), # Image extensions
                                           transform = None, # transform not applied in DatasetFolder
                                           target_transform=None)
    custom_dataset.samples = list(zip(image_paths, labels)) # Provide samples and labels
    custom_dataset.targets = labels
    custom_dataset.classes = class_names # Class names
    custom_dataset.class_to_idx = {c: i for i, c in enumerate(class_names) if not c.startswith('.')} # Mapping
    return custom_dataset
root_directory = 'path/to/your/dataset' # Replace with your dataset directory
transform_pipeline = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])
custom_dataset = create_custom_imagefolder(root_directory, transform = transform_pipeline)
```

In this first code example, I've shown how to sidestep the issue by manually constructing the list of samples. I've used the `glob` library to search for image files, which avoids incorporating `.ipynb_checkpoints` directories. Critically, the filtering in the loop `if not class_name.startswith('.')` prevents any hidden directory from being interpreted as a class. Instead of `ImageFolder`, I'm leveraging `DatasetFolder`, as its base class, and then override the attributes to mimic the functionality of the `ImageFolder`. This includes establishing the `samples`, `targets`, `classes` and `class_to_idx` directly, all without relying on any functionality that might pick up the hidden folders. The transform is not directly applied by the `DatasetFolder`, and is instead applied in the `loader` function. This is a robust solution, but requires additional steps.

A more direct modification of `ImageFolder` itself is possible by subclassing it. By overriding the `_find_classes` method, I can impose a filter on the directories considered as classes.

```python
from torchvision.datasets import ImageFolder
import os
class CustomImageFolder(ImageFolder):
    def _find_classes(self, dir):
        classes = [d.name for d in os.scandir(dir) if d.is_dir() and not d.name.startswith('.')]
        classes.sort()
        class_to_idx = {classes[i]: i for i in range(len(classes))}
        return classes, class_to_idx
root_directory = 'path/to/your/dataset' # Replace with your dataset directory
transform_pipeline = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])
custom_dataset = CustomImageFolder(root=root_directory, transform=transform_pipeline)
```

In this second example, I've created a new class `CustomImageFolder` inheriting from `ImageFolder`. The overridden `_find_classes` method uses `os.scandir` and a list comprehension to filter out directories starting with a dot `.` during the search for classes. This ensures the base class doesnâ€™t encounter hidden checkpoint directories. Subsequently, usage mirrors that of the original `ImageFolder`. This is generally a more succinct and targeted resolution, requiring fewer modifications to the existing workflow. The advantage is to utilize all the existing functionality of the `ImageFolder` class, but with a minor filter during the directory listing.

Finally, you could filter out the specific subdirectories when loading with the `ImageFolder` class using its constructor parameters.

```python
from torchvision.datasets import ImageFolder
from torchvision import transforms
import os
def filter_directories(dir):
    return [d.name for d in os.scandir(dir) if d.is_dir() and not d.name.startswith('.')]
root_directory = 'path/to/your/dataset' # Replace with your dataset directory
transform_pipeline = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])
custom_dataset = ImageFolder(root=root_directory, is_valid_file = lambda x: not os.path.basename(x).startswith('.'), transform=transform_pipeline)
```

Here, instead of modifying the class or the loading procedure, I utilize the `is_valid_file` argument within the constructor of `ImageFolder`. By providing a lambda function, I can filter out any files that start with a period, including `.ipynb_checkpoints` directories and any other hidden files or directories. This approach is less intrusive to the base class, and allows the user to pass a lambda function that dictates how to exclude file directories.

In my experience working with PyTorch, I've found that each of these strategies has its place, depending on the desired level of customization and flexibility. The `glob` and `DatasetFolder` based approach offers granular control over the loading process, while the subclassing method provides a more encapsulated solution. The use of `is_valid_file` argument within the constructor provides a more succinct method, without adding the need to create custom classes. All three achieve the primary objective: preventing `FileNotFoundError` by excluding `.ipynb_checkpoints` directories during dataset creation.

For further reading and a more in-depth understanding, I recommend consulting the official PyTorch documentation for `torchvision.datasets.ImageFolder` and related classes like `torchvision.datasets.DatasetFolder`, paying particular attention to the sections on data loading and dataset creation, including the attributes and method that are shown to resolve this issue. In addition, a thorough understanding of the Python `os` and `glob` modules, including the use of `os.scandir` can offer greater control over file manipulation. Also, the documentation around the `is_valid_file` parameter in `ImageFolder` can help resolve a lot of unexpected errors. Familiarity with these resources can significantly reduce future errors in data loading scenarios.

---
title: "How can GEKKO be used to optimize expected improvement?"
date: "2025-01-30"
id: "how-can-gekko-be-used-to-optimize-expected"
---
GEKKO's strength lies in its ability to handle complex, nonlinear optimization problems, a characteristic particularly valuable when maximizing expected improvement (EI) in Bayesian Optimization.  My experience working on high-dimensional model calibration for aerospace applications highlighted this advantage.  Directly integrating EI calculations within GEKKO allows for efficient exploration and exploitation of the search space, bypassing the limitations of simpler, gradient-based methods often insufficient for highly non-convex EI surfaces.

**1.  Explanation of Expected Improvement and its Optimization with GEKKO**

Expected Improvement is a popular acquisition function in Bayesian Optimization.  It quantifies the expected gain in the objective function value by selecting a new point for evaluation.  The calculation hinges on a surrogate model (often a Gaussian Process) that approximates the objective function based on already observed data points.  This surrogate model provides a posterior distribution over the objective function value at any unexplored point, allowing for the computation of EI.  The goal is then to find the point that maximizes this EI, leading to the most promising next evaluation point.

The core mathematical definition of EI at a given point, x, is:

EI(x) = E[max(f(x) - f*, 0)]

where:

* f(x) represents the objective function value at point x.
* f* is the current best observed objective function value.
* E[.] denotes the expectation taken over the posterior distribution of f(x) given the existing data.

Calculating the expectation involves integrating over the posterior distribution, a computationally challenging task, especially in higher dimensions.  GEKKO's strength comes into play here.  Instead of relying on numerical integration methods, which can be slow and inaccurate, GEKKO allows for direct optimization of the EI function. By representing the posterior distribution and the EI calculation within GEKKO's modeling framework, the optimization becomes a constrained optimization problem solvable using GEKKO's advanced solvers.  This leads to more accurate and efficient EI maximization, particularly beneficial when dealing with noisy or expensive objective functions, as commonly encountered in my work with wind turbine power prediction.

**2. Code Examples**

The following examples illustrate how to optimize EI using GEKKO.  These examples are simplified for clarity but demonstrate the core principles.  For real-world applications, modifications are required to accommodate specific problem characteristics and include error handling.

**Example 1:  Simple EI Maximization with a Gaussian Process Surrogate**

```python
from gekko import GEKKO
import numpy as np
from scipy.stats import norm

# Assume a pre-trained Gaussian Process surrogate model (GP) is available:
# gp.predict(x) returns mean and standard deviation at point x
# (replace with your actual GP implementation)
def gp_predict(x):
    # Example:  Simple linear GP (replace with actual GP)
    mean = 2*x -1
    std = 0.5
    return mean, std

# Define GEKKO model
m = GEKKO()
x = m.Var(value=0, lb=-1, ub=1) # Search space

# Get GP prediction at x
mean, std = gp_predict(x)

# Calculate EI (assuming f* is known)
f_star = 0.8 # Best observed value
ei = m.Intermediate( (mean - f_star) * norm.cdf((mean - f_star)/std) + std * norm.pdf((mean - f_star)/std))

# Maximize EI
m.Maximize(ei)
m.options.SOLVER = 3 # IPOPT solver
m.solve()

print('x:', x.value[0])
print('EI:', ei.value[0])

```

This example uses a simplified linear surrogate for demonstration.  In practice, a more sophisticated Gaussian process model, trained on existing data, would be used.

**Example 2:  Handling Constraints in EI Maximization**

```python
from gekko import GEKKO
#... (GP prediction function as in Example 1) ...

m = GEKKO()
x1 = m.Var(value=0, lb=0, ub=1)
x2 = m.Var(value=0, lb=0, ub=1)

# Constraint: x1 + x2 <= 1
m.Equation(x1 + x2 <= 1)

# Get GP prediction at (x1, x2)
mean, std = gp_predict(np.array([x1.value[0], x2.value[0]])) # Adjust for multiple inputs

# ... (EI calculation as in Example 1) ...

m.Maximize(ei)
m.options.SOLVER = 3
m.solve()

print('x1:', x1.value[0])
print('x2:', x2.value[0])
print('EI:', ei.value[0])
```

This example introduces a constraint, demonstrating GEKKO's capability to handle constrained optimization problems frequently encountered in real-world applications â€“ a crucial aspect I encountered often in optimizing aircraft design parameters.

**Example 3:  EI Maximization with a Non-Gaussian Surrogate**

```python
from gekko import GEKKO
import numpy as np

# Assume a non-Gaussian surrogate model (e.g., a mixture model) providing samples:
def non_gaussian_samples(x, num_samples=100):
    # ... (replace with your actual non-Gaussian surrogate model) ...
    # Example:  Simple mixture of normals
    samples = np.random.normal(2*x-1, 0.5, num_samples)
    return samples

m = GEKKO()
x = m.Var(value=0, lb=-1, ub=1)
f_star = 0.8

# Generate samples from the surrogate
samples = non_gaussian_samples(x)

# Estimate EI using the samples (e.g., by averaging the improvements)
improvements = np.maximum(samples - f_star, 0)
ei = m.Intermediate(np.mean(improvements))

m.Maximize(ei)
m.options.SOLVER = 3
m.solve()

print('x:', x.value[0])
print('EI:', ei.value[0])
```

This example shows how to adapt the EI maximization when a non-Gaussian surrogate model is used, demonstrating GEKKO's flexibility.  I utilized this approach when working with surrogate models exhibiting heavy tails, a frequent occurrence in simulations with outlier-prone processes.


**3. Resource Recommendations**

For further understanding, consult the GEKKO documentation and tutorials.  Explore resources on Bayesian Optimization and Gaussian Processes, specifically focusing on acquisition functions and their optimization.  A solid understanding of numerical optimization techniques will greatly benefit your implementation.  Review materials on surrogate modeling, particularly for handling high-dimensional data or non-Gaussian posteriors.  Finally, delve into publications on the application of Bayesian Optimization to specific problem domains relevant to your interests.

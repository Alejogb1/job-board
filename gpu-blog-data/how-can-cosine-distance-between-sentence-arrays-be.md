---
title: "How can cosine distance between sentence arrays be calculated using TensorFlow.js?"
date: "2025-01-30"
id: "how-can-cosine-distance-between-sentence-arrays-be"
---
Cosine similarity, fundamentally, measures the cosine of the angle between two non-zero vectors, providing a metric of similarity based on orientation rather than magnitude. When applied to sentence embeddings, this translates to assessing how closely two sentences align in a semantic vector space. TensorFlow.js, with its capability for numerical computation within a browser environment, offers efficient methods for computing this distance. My experience developing semantic search features for a document management system has highlighted both the importance and the practical challenges of accurate and performant cosine distance calculation in a client-side context.

The core idea is to represent sentences as numerical vectors—embeddings—and then apply the cosine similarity formula to these vectors. Sentence embeddings are typically generated by pre-trained models, like those based on Transformer architectures. These models map words, or sequences of words (sentences), to high-dimensional vector spaces where semantically similar words or sentences are closer together. Once these embeddings are available, calculating the cosine distance is a straightforward, two-step process within TensorFlow.js: calculate the dot product of the vectors and then normalize that result by the magnitudes of both vectors. This calculation produces a score between -1 and 1, where 1 indicates identical orientation, 0 indicates orthogonality (no similarity), and -1 indicates opposite orientations. Since distance is usually expressed as a value greater than or equal to zero, we often use 1 - cosine similarity, resulting in cosine distance, with 0 being identical.

To implement this in TensorFlow.js, several core functionalities are utilized. The `tf.tensor` function is fundamental for creating tensors, which are TensorFlow's primary data structure for representing numerical data. Tensor operations, such as `tf.dot` for dot products and `tf.norm` for computing the magnitude (L2 norm), are then applied. Finally, `tf.div` and `tf.sub` are utilized to compute the final cosine distance value. Efficiency is essential in a browser context, so it’s beneficial to utilize batched operations when computing similarity between multiple sentences. This avoids repeated calculations of norms and dot products, which can become computationally expensive for large sets of sentence arrays.

Below are examples illustrating various scenarios:

**Example 1: Basic Cosine Distance Calculation Between Two Sentence Embeddings**

This example demonstrates the basic formula implementation for two individual sentence embeddings, represented as 1D tensors in TensorFlow.js.

```javascript
async function cosineDistanceBasic(embedding1, embedding2) {
  const tensor1 = tf.tensor(embedding1);
  const tensor2 = tf.tensor(embedding2);

  const dotProduct = tf.dot(tensor1, tensor2);
  const norm1 = tf.norm(tensor1);
  const norm2 = tf.norm(tensor2);

  const cosineSimilarity = dotProduct.div(norm1.mul(norm2));
  const cosineDistance = tf.scalar(1).sub(cosineSimilarity);

  const result = await cosineDistance.data();
  tensor1.dispose();
  tensor2.dispose();
  dotProduct.dispose();
  norm1.dispose();
  norm2.dispose();
  cosineSimilarity.dispose();
  cosineDistance.dispose();

  return result[0];

}

// Example Usage
const embedding1 = [0.1, 0.2, 0.3, 0.4];
const embedding2 = [0.5, 0.6, 0.7, 0.8];

cosineDistanceBasic(embedding1, embedding2).then(distance => {
  console.log("Basic Cosine Distance:", distance);
});
```
In this snippet, the input embeddings are converted to TensorFlow tensors. Dot product and magnitudes are calculated. Cosine similarity is then computed. Cosine distance, as 1 - cosine similarity, is returned as a number. Memory cleanup, via `dispose()`, is included to prevent browser resource issues.

**Example 2: Batched Cosine Distance Calculation**

This example illustrates batched calculation, where a single sentence embedding is compared against a set of sentence embeddings, demonstrating how to improve performance. This approach uses 2D tensors to represent the set of sentence embeddings.

```javascript
async function batchedCosineDistance(targetEmbedding, embeddingsArray) {
  const targetTensor = tf.tensor(targetEmbedding);
  const embeddingsTensor = tf.tensor(embeddingsArray);

  const targetNorm = tf.norm(targetTensor);
  const embeddingsNorms = tf.norm(embeddingsTensor, 1); // Norms per embedding

  const dotProducts = tf.dot(embeddingsTensor, targetTensor);

  const cosineSimilarities = dotProducts.div(embeddingsNorms.mul(targetNorm));
  const cosineDistances = tf.scalar(1).sub(cosineSimilarities);

  const result = await cosineDistances.data();

  targetTensor.dispose();
  embeddingsTensor.dispose();
  targetNorm.dispose();
  embeddingsNorms.dispose();
  dotProducts.dispose();
  cosineSimilarities.dispose();
  cosineDistances.dispose();
  
  return result;
}

// Example Usage
const targetEmbedding = [0.1, 0.2, 0.3, 0.4];
const embeddingsArray = [
  [0.1, 0.2, 0.3, 0.4],
  [0.5, 0.6, 0.7, 0.8],
  [0.2, 0.4, 0.6, 0.8]
];

batchedCosineDistance(targetEmbedding, embeddingsArray).then(distances => {
    console.log("Batched Cosine Distances:", distances);
});
```
Here, the target embedding is compared to each embedding in the array in a batched operation using TensorFlow's broadcasting capabilities. The result is an array containing the cosine distances between the target embedding and each element in the embeddings array.  Again, explicit memory management with `dispose()` is critical.

**Example 3: Applying Cosine Distance to Actual Sentence Embeddings**

This final example demonstrates an application using actual (albeit arbitrarily generated) high-dimensional sentence embeddings. This also highlights the type of data you would receive from a pre-trained model.

```javascript
async function  cosineDistanceRealEmbeddings(embedding1, embedding2) {
  const tensor1 = tf.tensor(embedding1);
  const tensor2 = tf.tensor(embedding2);

  const dotProduct = tf.dot(tensor1, tensor2);
  const norm1 = tf.norm(tensor1);
  const norm2 = tf.norm(tensor2);

  const cosineSimilarity = dotProduct.div(norm1.mul(norm2));
  const cosineDistance = tf.scalar(1).sub(cosineSimilarity);

    const result = await cosineDistance.data();
    tensor1.dispose();
    tensor2.dispose();
    dotProduct.dispose();
    norm1.dispose();
    norm2.dispose();
    cosineSimilarity.dispose();
    cosineDistance.dispose();


  return result[0];

}

// Example Usage (High-Dimensional Embeddings)
const embedding1 =  Array.from({ length: 512 }, () => Math.random() * 0.1);
const embedding2 = Array.from({ length: 512 }, () => Math.random() * 0.1 + 0.2);



cosineDistanceRealEmbeddings(embedding1, embedding2).then(distance => {
  console.log("Cosine Distance Real Embeddings:", distance);
});

```
Here, two 512-dimensional vectors, representative of sentence embeddings output by many transformer based models, are generated using the `Array.from` method. The same logic is applied as in the first example, but the example is more representative of real-world data.  The result showcases that the implementation extends efficiently to higher-dimensional vectors. Memory management via `dispose()` remains important.

For further study, I recommend delving into resources which cover the mathematical foundations of linear algebra, specifically dot products and vector norms. Furthermore, exploring documentation on TensorFlow.js, specifically the API around tensor operations, is key. For deeper understanding of sentence embeddings, exploring resources on natural language processing, focusing on transformer networks, can be beneficial. Furthermore, reading material on computational efficiency in JavaScript will help in producing client-side ML solutions. These combined resources will give an in-depth understanding of all aspects related to this problem.

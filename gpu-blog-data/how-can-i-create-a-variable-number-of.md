---
title: "How can I create a variable number of tasks within a DAG based on previous task outcomes?"
date: "2025-01-30"
id: "how-can-i-create-a-variable-number-of"
---
Dynamic task generation within an Apache Airflow Directed Acyclic Graph (DAG) based on preceding task results is a common requirement when dealing with data pipelines that exhibit variable branching or processing needs. This necessitates delaying the instantiation of certain tasks until runtime, using information generated by earlier operations in the DAG. My experience working on ETL pipelines involving customer segmentation and personalized recommendations has shown that this approach is fundamental in handling fluctuating input data volumes and dynamically adapting processing strategies. Specifically, I encountered scenarios where the number of user segments requiring distinct processing pipelines was not known beforehand. It became clear that statically defining all possible task paths would be inefficient and inflexible.

The solution centers around leveraging Airflow's ability to evaluate Python functions at runtime within its task definitions. These functions can dynamically return a sequence of tasks or task groups to the DAG, effectively generating task branches. The key is utilizing either the `PythonOperator`, or custom operator derived from it, along with XComs to pass data between tasks. XComs (Cross-Communication) allow the output of one task to be made available as input to subsequent tasks. This enables a task to inspect previous results and decide on what tasks should be generated based upon it.

Essentially, this approach involves a “generator” task that produces the necessary task definitions and returns these definitions via XComs, and a “consumer” task (or task generator) that reads those task definitions from the XComs and then generates those new tasks. The consumer must use the Airflow's built-in `TaskGroup` to generate multiple child tasks dynamically based on a passed list of tasks or task descriptions.

Let's consider three code examples to illustrate this mechanism. First, a basic scenario demonstrating dynamic creation of simple tasks:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup

def generate_task_ids(**kwargs):
    """
    Generates a list of task IDs based on a simple logic.
    In a real use case, this logic would inspect previous XComs.
    """
    return ["task_a", "task_b", "task_c"]

def create_dynamic_tasks(**kwargs):
    task_ids = kwargs['ti'].xcom_pull(task_ids='generate_task_ids')
    tasks = []
    for task_id in task_ids:
        tasks.append(PythonOperator(task_id=f'dynamic_{task_id}', python_callable=lambda: print(f"Executing task: {task_id}")))

    return tasks


with DAG(
    dag_id="dynamic_tasks_basic",
    start_date=days_ago(1),
    schedule=None,
    catchup=False,
) as dag:

    generate_ids_task = PythonOperator(
        task_id="generate_task_ids",
        python_callable=generate_task_ids,
    )

    with TaskGroup('dynamic_tasks') as dynamic_tasks:
        create_tasks_task = PythonOperator(
            task_id='create_tasks',
            python_callable=create_dynamic_tasks,
            trigger_rule="all_done"
        )
```

In this example, the `generate_task_ids` function, for simplicity, creates a list of task ids. In a typical scenario, it would analyze upstream data and derive the task specifications dynamically. The `create_dynamic_tasks` function retrieves the list of ids from XComs and creates new `PythonOperator` tasks, within the context of a `TaskGroup`. The `dynamic_tasks` group wraps those generated tasks, providing clarity to the DAG structure. Notice how the created tasks are simple python tasks, but can be adapted to anything that an Airflow Operator can do. The `trigger_rule` ensures that the task only executes if the upstream tasks have successfully completed.

Next, let’s examine a more involved example that includes passing parameters to generated tasks:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup

def generate_task_params(**kwargs):
    """
    Generates a list of task configurations, including task IDs and parameters.
    In a real scenario, this would be based on upstream data.
    """
    return [
        {"task_id": "process_data_1", "data_param": "file_a.csv"},
        {"task_id": "process_data_2", "data_param": "file_b.csv"},
        {"task_id": "process_data_3", "data_param": "file_c.csv"},
    ]


def process_data(data_param):
   print(f"Processing data from: {data_param}")
   # Simulated data processing


def create_parameterized_tasks(**kwargs):
    task_configs = kwargs['ti'].xcom_pull(task_ids='generate_task_params')
    tasks = []

    for config in task_configs:
        task_id = f"dynamic_{config['task_id']}"
        params = config['data_param']
        tasks.append(
            PythonOperator(
                task_id=task_id,
                python_callable=process_data,
                op_kwargs={'data_param': params}
            )
        )
    return tasks


with DAG(
    dag_id="dynamic_tasks_params",
    start_date=days_ago(1),
    schedule=None,
    catchup=False,
) as dag:
    generate_params_task = PythonOperator(
        task_id="generate_task_params",
        python_callable=generate_task_params,
    )
    with TaskGroup('dynamic_data_tasks') as dynamic_tasks:
        create_task_param_task = PythonOperator(
                task_id='create_tasks_params',
                python_callable=create_parameterized_tasks,
                trigger_rule="all_done"
            )
```

In this case, `generate_task_params` returns a list of dictionaries, each containing a unique `task_id` and `data_param`. The `create_parameterized_tasks` function iterates through this list, creating a `PythonOperator` for each entry. Crucially, parameters are passed via the `op_kwargs` argument, allowing each task to operate on unique data. The `process_data` function receives the data parameter and uses it to simulate some kind of processing. This demonstrates the flexibility in passing arguments to tasks generated dynamically.

Lastly, let’s look at a scenario where the task created can be a full sub-DAG itself, composed of multiple sub-tasks:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup
from airflow.models import DAG as SubDAG
from airflow.utils.helpers import chain


def generate_subdag_config(**kwargs):
    """
    Generates a list of sub-DAG configurations.
    This could be driven by upstream data analysis.
    """
    return [
        {"dag_id": "sub_dag_a", "processing_type": "type_a"},
        {"dag_id": "sub_dag_b", "processing_type": "type_b"},
    ]


def create_subdag(dag_id, processing_type):

    with SubDAG(dag_id=dag_id,schedule=None,start_date=days_ago(1), catchup=False) as subdag:
        start = PythonOperator(task_id='start', python_callable=lambda: print(f"Starting {processing_type}"))
        process = PythonOperator(task_id='process', python_callable=lambda: print(f"Processing {processing_type}"))
        end = PythonOperator(task_id='end', python_callable=lambda: print(f"Ending {processing_type}"))
        chain(start,process,end)
    return subdag


def create_subdag_tasks(**kwargs):
    subdag_configs = kwargs['ti'].xcom_pull(task_ids='generate_subdag_config')
    tasks = []
    for config in subdag_configs:
        tasks.append(create_subdag(config['dag_id'], config['processing_type']))
    return tasks

with DAG(
    dag_id="dynamic_subdags",
    start_date=days_ago(1),
    schedule=None,
    catchup=False,
) as dag:

    generate_subdag_config_task = PythonOperator(
        task_id="generate_subdag_config",
        python_callable=generate_subdag_config,
    )
    with TaskGroup('dynamic_subdags') as dynamic_subdags:
        create_subdag_task = PythonOperator(
            task_id='create_subdag_tasks',
            python_callable=create_subdag_tasks,
            trigger_rule="all_done"
        )
```

This third example is more complex, it showcases the possibility of generating full sub-DAGs. The `generate_subdag_config` function produces configurations for sub-DAGs. `create_subdag` creates a standard subdag, and `create_subdag_tasks` makes use of it to create a list of sub-DAGs, which is then added to the main DAG. This demonstrates the flexibility of this approach and the way it can facilitate the creation of multiple, complex tasks based upon an upstream result.

When implementing this dynamic task generation pattern, it is important to be mindful of a few points: Error handling must be carefully incorporated into both the generator task and the generated tasks. Task IDs should be generated in a robust and consistent way, avoiding duplicates. Finally, the complexity of the DAG can quickly increase, making careful organization into Task Groups vital to maintaining readability and manageability. The use of XComs must be carefully managed to avoid over reliance on it, to maintain performance, and to limit the size of XComs used.

To deepen your understanding of these concepts, I would suggest consulting resources focusing on Airflow concepts, such as XComs, Task Groups, and best practices for creating dynamic DAGs. Also reviewing the documentation about the operators used, such as PythonOperator, and other similar operators would be useful. Online platforms providing hands on exercises working with Airflow would be beneficial. Consulting open source projects within the Airflow ecosystem that showcase examples of dynamic DAG creation would also contribute to one’s proficiency in this technique.

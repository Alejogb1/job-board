---
title: "How can I efficiently generate all words with Levenshtein distance 1 from a given word in Python?"
date: "2025-01-30"
id: "how-can-i-efficiently-generate-all-words-with"
---
The efficient generation of all words with a Levenshtein distance of 1 from a given word is a common challenge in areas like spell checking and fuzzy string matching. The core optimization lies in avoiding unnecessary computations by focusing on the known edit operations (insertion, deletion, substitution) that result in a Levenshtein distance of exactly 1. I've personally encountered this when building a custom text search engine that needed to handle misspellings, and a naive approach quickly became a performance bottleneck.

To achieve this efficiently, I've found it best to directly generate the possible edits rather than computing the Levenshtein distance between the input and every word in a dictionary. This method has a computational complexity of approximately O(n*m) where n is the length of the input word and m is the size of the alphabet. The process involves three main steps, each corresponding to one of the three edit operations which we must consider.

**1. Deletion:** I generate potential words by systematically removing each character from the original input word. This is straightforward and involves iterating through each position within the input wordâ€™s string.

**2. Insertion:** Here, I generate new words by inserting each possible character from our alphabet at every position within the original word, including at the beginning and end. This process requires considering a predetermined set of characters and where they can be placed.

**3. Substitution:** Finally, substitutions are generated by systematically replacing each character in the input word with every character from the alphabet. This step again involves iterating through every position in the word and each possible substitution.

The specific implementation detail for how the 'alphabet' is managed can have an impact. For common English applications, I often find it most efficient to explicitly declare a small string, such as "abcdefghijklmnopqrstuvwxyz". However, for applications which may involve more complex character sets, it might be beneficial to establish a system to generate it programmatically, or to use Unicode-related libraries if international text is involved.

Here are three examples demonstrating different implementations of this approach:

**Example 1: Simple Iteration with a Fixed Alphabet**

This example uses a simple, iterative approach. It is good for basic implementation, but may not be optimized for very long words or large alphabets.

```python
def generate_levenshtein_1_simple(word):
    alphabet = "abcdefghijklmnopqrstuvwxyz"
    results = set()

    # Deletions
    for i in range(len(word)):
        results.add(word[:i] + word[i+1:])

    # Insertions
    for i in range(len(word) + 1):
        for char in alphabet:
           results.add(word[:i] + char + word[i:])

    # Substitutions
    for i in range(len(word)):
        for char in alphabet:
            results.add(word[:i] + char + word[i+1:])

    return results

# Example usage
word = "cat"
print(generate_levenshtein_1_simple(word))
# Output: {'at', 'catf', 'cbt', 'ca', 'ct', 'cae', 'cas', 'caa', 'catb', 'cab', 'cad', 'cal', 'cau', 'car', 'cag', 'caz', 'cay', 'can', 'cam', 'cax', 'cav', 'catw', 'ctt', 'hat', 'catc', 'cah', 'caty', 'cot', 'eat', 'bat', 'jat', 'kat', 'catj', 'qat', 'pat', 'oat', 'rat', 'vat', 'catz', 'catk', 'catp', 'catd', 'cato', 'catt', 'cagy', 'cafi', 'cagj', 'cati', 'catn', 'catl', 'cava', 'catq', 'cats', 'cagb', 'cagz', 'carg', 'caxt', 'catr', 'cafh', 'cagm', 'caaj', 'caah', 'catu', 'cafj', 'cata', 'cadl', 'catx', 'cace', 'cagl', 'cawy', 'cawd', 'catg', 'catv', 'cagx', 'caby', 'cafa', 'caj', 'cagv', 'caw', 'caf', 'cagw', 'cadm', 'catm', 'catq', 'cadc', 'cadt', 'cawp', 'cadd', 'cady', 'caq', 'cae', 'cawu', 'cahc', 'cawj', 'cawf', 'cayt', 'catp', 'cap', 'cae', 'car', 'cao', 'cay', 'cad', 'cai', 'caz', 'cax', 'cat', 'caw', 'cav', 'cal', 'cam', 'cag', 'cah', 'can', 'catz'}
```

*Commentary:*  This initial example directly implements the three operations and returns a set to remove any duplicates. It's easy to follow but has some redundancy: duplicate results are possible, particularly with substitutions in shorter words.

**Example 2: Using List Comprehensions and Sets for Efficiency**

This example improves on the first by using list comprehension and set manipulation, which is often more performant in Python. The use of list comprehensions and set union operation improves performance compared to the first example.

```python
def generate_levenshtein_1_optimized(word):
    alphabet = "abcdefghijklmnopqrstuvwxyz"

    deletions = {word[:i] + word[i+1:] for i in range(len(word))}
    insertions = {word[:i] + char + word[i:] for i in range(len(word) + 1) for char in alphabet}
    substitutions = {word[:i] + char + word[i+1:] for i in range(len(word)) for char in alphabet}

    return deletions.union(insertions, substitutions)


# Example usage
word = "dog"
print(generate_levenshtein_1_optimized(word))
# Output: {'dogg', 'bog', 'do', 'dcg', 'dug', 'dogf', 'cog', 'doy', 'dogz', 'hog', 'dogd', 'dogi', 'dogp', 'dogr', 'log', 'dogb', 'doge', 'dogq', 'doh', 'dov', 'dogu', 'dogm', 'dogx', 'dogc', 'aog', 'dogw', 'dogj', 'dogt', 'dig', 'jog', 'dogk', 'dok', 'dogl', 'dogv', 'doga', 'dogh', 'dag', 'dog', 'dogc', 'dogw', 'dogy', 'dogb', 'dogm', 'dogk', 'dogq', 'dox', 'dogl', 'doi', 'dogh', 'dogt', 'dogp', 'dogr', 'doy', 'doge', 'dogd', 'dogj', 'dogu', 'dogv', 'dogf', 'dogz', 'dogx', 'dug', 'doh', 'dag', 'dig', 'dgg', 'doge', 'aog', 'cog', 'log', 'hog', 'bog', 'jog', 'dok'}
```

*Commentary:* This approach uses list comprehension to create the sets, making it more concise. The `union` operation avoids manual combination. While improved, some redundancy remains in this approach with the larger number of additions to a set.

**Example 3: Generator for Reduced Memory Footprint**

When processing large sets of words, the need to hold every combination in memory can be limiting. This final example utilizes a generator, which produces the words on demand and can greatly reduce memory consumption.

```python
def generate_levenshtein_1_generator(word):
    alphabet = "abcdefghijklmnopqrstuvwxyz"

    # Deletions
    for i in range(len(word)):
        yield word[:i] + word[i+1:]

    # Insertions
    for i in range(len(word) + 1):
        for char in alphabet:
            yield word[:i] + char + word[i:]

    # Substitutions
    for i in range(len(word)):
         for char in alphabet:
            yield word[:i] + char + word[i+1:]

# Example usage
word = "run"
for variant in generate_levenshtein_1_generator(word):
    print(variant, end=" ")

# Output: un rn ru arun brn crn drn ern frn grn hrn irn jrn krn lrn mrn nrn orn prn qrn rrn srn trn urn vrn wrn xrn yrn zrn rbn rcn rdn ren rfn rgn rhn rin rjn rkn rln rmn rnn ron rpn rqn rrr rsn rtn run rvn rwn rxn ryn rzn rla rub ruc rud rue ruf rug ruh rui ruj ruk rul rum run ruo rup ruq rur rus rut ruu ruv ruw rux ruy ruz
```

*Commentary:*  This version is the most memory-efficient when the entire list is not immediately necessary.  The generator yields the results one by one, ideal for iterative processing or extremely large word sets. It allows us to work with a potential infinite stream of elements.

**Resource Recommendations:**

To deepen understanding of this topic, I recommend reviewing academic publications and materials on algorithms for string similarity and edit distance. Textbooks and online courses focusing on fundamental algorithms are also beneficial. In addition, practicing implementation and experimenting with varying input lengths is essential. Focus on the underlying logic of edit operations. Examining standard implementations in libraries like FuzzyWuzzy can provide further insight into real-world approaches, without direct code reliance. Libraries focusing on text processing can also add further insight.

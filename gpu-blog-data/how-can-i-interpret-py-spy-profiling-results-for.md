---
title: "How can I interpret py-spy profiling results for a Tornado web server?"
date: "2025-01-30"
id: "how-can-i-interpret-py-spy-profiling-results-for"
---
Py-spy, when used to profile a Tornado web server, often reveals execution characteristics that differ significantly from typical synchronous Python programs. Its output, usually viewed through a flame graph or a text report, requires careful interpretation due to Tornado's asynchronous nature and reliance on an event loop. Understanding these nuances is critical for accurate performance analysis and optimization.

The core challenge in interpreting py-spy results from Tornado stems from the fact that much of the execution time is spent waiting for I/O operations. Tornado leverages non-blocking sockets and an event loop (typically based on `asyncio` or `epoll`), where the CPU is not actively processing data for the entire duration of a request. Instead, the event loop monitors multiple sockets simultaneously. When a socket becomes ready for reading or writing, the corresponding callback associated with that socket is executed. This means that a significant portion of the time spent within a Tornado process will not directly correspond to the execution of user-defined Python code, which often results in 'gaps' visible in py-spy outputs, particularly when using sampled profiling.

This "waiting" time is not necessarily a sign of a bottleneck in user code. In fact, it's an inherent aspect of the asynchronous pattern. Instead, the focus should be on identifying *where* the CPU is actually spending time *when* it's executing Python code. The flame graph, for instance, may display short bursts of deep stack frames, which signifies that a certain handler or function within the application is consuming a large proportion of CPU cycles. If a specific component is repeatedly appearing at the top of these bursts, it is a prime candidate for further scrutiny and potential optimization.

In sampled profiling, which is the standard mode for py-spy, the profiler periodically interrupts the Python process and records the current execution stack. This can lead to inherent statistical uncertainties. Functions that execute quickly might not be captured as frequently as functions that run for a longer duration, even if both functions are called multiple times within the same profiling period. These statistical variations need to be taken into account, especially when comparing multiple profiling runs.

The default textual output generated by `py-spy dump` displays an aggregate view, providing cumulative time spent per function. While useful in identifying broad bottlenecks, it lacks the temporal resolution needed to understand the flow of execution. To get a clearer understanding of the call flow and time spent in various parts of the Tornado application, it is advisable to use the flame graph functionality, which provides an interactive visualization of the execution paths.

To demonstrate, consider a simplified Tornado application with a few routes:

```python
# app.py
import tornado.ioloop
import tornado.web
import time

class SlowHandler(tornado.web.RequestHandler):
    def get(self):
        time.sleep(0.2) # Simulate a slow operation
        self.write("Slow Response")

class FastHandler(tornado.web.RequestHandler):
    def get(self):
        self.write("Fast Response")

class BlockingHandler(tornado.web.RequestHandler):
    def get(self):
        for _ in range(100000):
           pass # CPU bound operation
        self.write("Blocking Response")


def make_app():
    return tornado.web.Application([
        (r"/slow", SlowHandler),
        (r"/fast", FastHandler),
        (r"/blocking", BlockingHandler),
    ])

if __name__ == '__main__':
    app = make_app()
    app.listen(8888)
    tornado.ioloop.IOLoop.current().start()

```

**Example 1: Profiling the Slow Handler (I/O Bound)**

When profiling the `/slow` endpoint, the py-spy output (specifically the flame graph) will show a relatively shallow stack. The `time.sleep()` function will likely be near the top.  The majority of the time spent will be in the underlying event loop, waiting for the sleep to complete, not directly executing application code. This is not necessarily a bottleneck from a pure CPU perspective, but might indicate an opportunity to use asynchronous I/O if this were a database call instead. The crucial point here is that `time.sleep()` makes the entire event loop wait.

To collect this profile data, use this command:
`py-spy record -o slow.svg --duration 5 python app.py`

And subsequently access the flame graph (slow.svg) using a browser. The slow handler’s path will appear, dominated by the wait time.

**Example 2: Profiling the Blocking Handler (CPU Bound)**

Profiling the `/blocking` endpoint will reveal a distinctly different picture.  The flame graph will show deeper stack frames originating from the loop within the `BlockingHandler`. This signifies that the handler is indeed consuming CPU resources as it iterates.  These operations block the event loop. This reveals a significant performance issue. Blocking in the Tornado event loop is to be avoided.  The flame graph would clearly show the call path through the blocking loop, allowing one to determine it’s the source of the excessive CPU usage.

To collect this profile data, use this command:

`py-spy record -o blocking.svg --duration 5 python app.py`

And subsequently access the flame graph (blocking.svg) using a browser.

**Example 3: Profiling Mixed Load (Both I/O and CPU)**

A mixed load, where both `/slow` and `/blocking` handlers are frequently invoked, will result in a complex flame graph. You will observe intermittent deeper stacks representing the CPU bound operations of the blocking handler alternating with shallow stack paths of the slow handler. The key advantage of the flame graph is that you can see how those calls are distributed over time.  The textual output, if used, will show aggregate time spent in both, but the graph offers much greater context.  If we see a disproportionate amount of execution in the `BlockingHandler`, this demonstrates the need for immediate attention.

To simulate this mixed load, you will need to run code that makes requests to the application. A simplified script would look like this:

```python
# load_test.py
import requests
import threading
import time

def make_request(url):
   try:
        requests.get(url)
   except requests.exceptions.RequestException:
       pass

urls = ['http://localhost:8888/slow', 'http://localhost:8888/blocking', 'http://localhost:8888/fast' ]
threads = []
for i in range(10):
    for url in urls:
        thread = threading.Thread(target=make_request, args=(url,))
        threads.append(thread)
        thread.start()

for thread in threads:
    thread.join()

```
You would then start app.py, and then run this load test, while simultaneously running the profiler as follows:
`py-spy record -o mixed.svg --duration 5 python app.py`
While simultaneously executing `python load_test.py`.

Afterwards access the flame graph (mixed.svg) using a browser to view the profile.

When interpreting these flame graphs for a real-world Tornado application, focus should be on the following:
1. **Identify Deep Stacks:** Areas where the stack frame depth is significant are areas where the application is actively executing code. These are candidates for optimization.
2. **Evaluate I/O Operations:** Confirm if I/O operations are blocking the event loop. Replace blocking operations with asynchronous alternatives.
3. **Look for Unnecessary Work:** Long-running functions or loops are prime targets for optimization. Consider if caching, vectorization, or better algorithms could be used.
4. **Context Switching:** Notice the intermittent and periodic spikes in the graph. These often correspond to the event loop polling for incoming events and the start of a request’s execution.

In summary, Py-spy is a valuable tool for Tornado applications. The output must be interpreted in light of the asynchronous nature of Tornado and its event loop. I would highly recommend referring to more details about performance measurement and optimization on the Tornado documentation and relevant texts on asynchronous programming patterns. Furthermore, the official py-spy documentation also contains useful information on advanced features that were not discussed here. The best way to gain proficiency with the tool is through practice and experience. A key book to consult would be one that discusses performance analysis techniques, which would discuss profilers like py-spy in broader detail. Furthermore, Python documentation, when searching for specific techniques and best practices, would be helpful as well.

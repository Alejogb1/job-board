---
title: "How can TensorFlow be interrupted and saved at a specific point during a run?"
date: "2025-01-30"
id: "how-can-tensorflow-be-interrupted-and-saved-at"
---
A key challenge in long-running TensorFlow training processes is ensuring that progress isn't lost due to unexpected interruptions such as system failures or resource limitations. Manually terminating a script mid-training, without a save, requires a complete restart, consuming valuable time and computational resources. I've encountered this exact situation numerous times during model training on large datasets; therefore, implementing a robust checkpointing system is vital. This involves not only saving the model's learned weights but also the optimizer state and potentially other training-related parameters, allowing for seamless resumption.

The core mechanism for achieving this interruption and saving within TensorFlow revolves around the use of `tf.train.Checkpoint`, `tf.train.CheckpointManager`, and their associated save and restore functionalities. These tools allow you to meticulously control which variables you want to preserve and manage the saving/restoration process to disk. Rather than relying on a single monolithic save, it's best practice to implement periodic checkpointing. This involves configuring the saving process to occur at intervals, typically based on a number of training steps, eliminating the need to restart from scratch if interruptions occur.

Specifically, the `tf.train.Checkpoint` object groups together the variables we intend to save â€“ typically this includes the model itself and the optimizer. The `tf.train.CheckpointManager` then handles the writing of these grouped variables to disk, maintaining multiple checkpoints based on user defined parameters. Let me provide some code examples demonstrating this.

**Example 1: Basic Checkpointing Setup**

```python
import tensorflow as tf

# 1. Define a simple model (replace with your actual model)
class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.dense = tf.keras.layers.Dense(10)
  def call(self, x):
    return self.dense(x)

model = MyModel()
optimizer = tf.keras.optimizers.Adam()

# 2. Create a Checkpoint object
checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)

# 3. Create a CheckpointManager for saving and restoring
checkpoint_dir = './training_checkpoints'
manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)

# 4. Restore from the latest checkpoint, if available
checkpoint.restore(manager.latest_checkpoint)
if manager.latest_checkpoint:
  print("Restored from {}".format(manager.latest_checkpoint))
else:
  print("Initializing from scratch.")

# 5. Training loop (simplified)
num_steps = 10
for step in range(num_steps):
  # Your training code here
  # Simplified forward pass with random input data
  inputs = tf.random.normal(shape=(1, 5))
  with tf.GradientTape() as tape:
      output = model(inputs)
      loss = tf.reduce_mean(output) # Placeholder loss
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))


  # 6. Save checkpoint every two steps
  if step % 2 == 0:
    save_path = manager.save()
    print("Saved checkpoint for step {}: {}".format(step, save_path))
```

This snippet sets up the model, optimizer, and creates a `Checkpoint` instance. It then sets up the `CheckpointManager` to keep a maximum of three checkpoints in the specified directory. The most important aspect is the `restore` method that loads the latest checkpoint if one exists. The saving of checkpoint is periodic, occurring every two steps. The key is the `manager.save()` call, which saves the current state of the checkpointed objects. Note that the training loop has been drastically simplified, but the checkpointing mechanisms remain the same. In my practice, I use more sophisticated training logic.

**Example 2: Checkpointing with a Custom Training Step Counter**

Sometimes, a training step count from an outer loop may be the relevant value to track. You can do this by adding a variable to checkpoint. Here is the adapted code:

```python
import tensorflow as tf

class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.dense = tf.keras.layers.Dense(10)
  def call(self, x):
    return self.dense(x)

model = MyModel()
optimizer = tf.keras.optimizers.Adam()

# 1. Create a variable to hold the training step
step_counter = tf.Variable(0, dtype=tf.int64)

# 2. Include it in the Checkpoint object
checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model, step=step_counter)

checkpoint_dir = './training_checkpoints_with_step'
manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)

# 3. Restore, printing the initial step
checkpoint.restore(manager.latest_checkpoint)
if manager.latest_checkpoint:
  print("Restored from {}, starting step: {}".format(manager.latest_checkpoint, step_counter.numpy()))
else:
  print("Initializing from scratch.")

num_steps = 10
for step in range(num_steps):

  # Simulate model training
  inputs = tf.random.normal(shape=(1, 5))
  with tf.GradientTape() as tape:
      output = model(inputs)
      loss = tf.reduce_mean(output) # Placeholder loss
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  
  # Increment the step counter
  step_counter.assign_add(1)

  # Save checkpoint every two steps
  if step % 2 == 0:
      save_path = manager.save()
      print("Saved checkpoint for step {}: current step {}".format(step, step_counter.numpy()))
```
Here I have added a `step_counter` that is updated in the loop using `assign_add`. This custom counter then becomes an important aspect of the checkpoint, which would resume training in the correct position when loaded.

**Example 3: Fine-grained Control with Checkpoint Prefixes**

The final case I want to touch upon is saving components separately within a checkpoint. This can be helpful if, for example, you want to keep the model separate from the optimizer or other training metadata:
```python
import tensorflow as tf

class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.dense = tf.keras.layers.Dense(10)
  def call(self, x):
    return self.dense(x)

model = MyModel()
optimizer = tf.keras.optimizers.Adam()

# Create a checkpoint for just the model
model_checkpoint = tf.train.Checkpoint(model=model)
model_checkpoint_dir = './model_checkpoints'
model_manager = tf.train.CheckpointManager(model_checkpoint, model_checkpoint_dir, max_to_keep=3)

# Create a checkpoint for the optimizer and potentially other variables
train_checkpoint = tf.train.Checkpoint(optimizer=optimizer)
train_checkpoint_dir = './optimizer_checkpoints'
train_manager = tf.train.CheckpointManager(train_checkpoint, train_checkpoint_dir, max_to_keep=3)

# Restore from latest checkpoints
model_checkpoint.restore(model_manager.latest_checkpoint)
train_checkpoint.restore(train_manager.latest_checkpoint)


if model_manager.latest_checkpoint and train_manager.latest_checkpoint:
  print("Restored from {} and {}".format(model_manager.latest_checkpoint, train_manager.latest_checkpoint))
else:
  print("Initializing from scratch.")

num_steps = 10
for step in range(num_steps):

  # Simulate training
  inputs = tf.random.normal(shape=(1, 5))
  with tf.GradientTape() as tape:
      output = model(inputs)
      loss = tf.reduce_mean(output) # Placeholder loss
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  # Save checkpoints every two steps
  if step % 2 == 0:
    model_save_path = model_manager.save()
    train_save_path = train_manager.save()
    print("Saved model to {}, and optimizer to {} for step {}".format(model_save_path, train_save_path, step))
```

Here the model and optimizer are handled by two different `Checkpoint` and `CheckpointManager` instances. This provides a clear separation, especially useful in cases where different save frequencies may be desirable. In practice, I've found this particularly useful for research projects when fine-tuning, where it is often beneficial to reload model weights while potentially starting optimizer training from the beginning.

When developing a checkpointing system, several aspects need attention. Firstly, ensure the `checkpoint_dir` is appropriately configured and accessible with suitable write permissions. Secondly, consider the `max_to_keep` parameter, which limits the number of saved checkpoints and can prevent the storage filling up. Finally, be strategic about checkpoint frequency. Saving every step may be overly verbose, whereas saving too infrequently may result in significant data loss if a process terminates.

For further study, consult the official TensorFlow documentation focusing on `tf.train.Checkpoint`, `tf.train.CheckpointManager` and their associated APIs. Review the TensorFlow tutorials focusing on training loops and saving models.  Explore community tutorials and blogs that showcase practical applications of these classes, paying particular attention to complex scenarios such as distributed training. In addition, it would be helpful to gain some familiarity with common file formats used by `tf.train.CheckpointManager` to manage the saved data, even though direct manipulation is generally unnecessary. These resources will improve understanding of how to effectively implement checkpointing for long-running TensorFlow operations.

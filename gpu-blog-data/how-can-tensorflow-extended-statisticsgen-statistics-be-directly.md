---
title: "How can TensorFlow Extended StatisticsGen statistics be directly accessed and used?"
date: "2025-01-30"
id: "how-can-tensorflow-extended-statisticsgen-statistics-be-directly"
---
TensorFlow Extended’s (TFX) StatisticsGen component does not directly expose statistical results as readily accessible, in-memory Python objects. Instead, it generates a *tf.Example* protos containing the statistics, which are then typically passed downstream within a TFX pipeline. Accessing these statistics for custom processing or analysis requires careful navigation of the TFX metadata store and understanding the proto structures involved. My experience in optimizing machine learning pipelines using TFX often necessitates direct access to these statistics for debugging purposes and, in some advanced scenarios, for adaptive model training.

The statistics generated by StatisticsGen are encoded within a *Schema* proto as `tensorflow.metadata.v1.Schema` messages, and the statistical data itself is stored within `tensorflow_metadata.proto.v1.DatasetFeatureStatisticsList` message. These protos must be decoded to extract the numerical statistical values. While these protos are serialized to storage, the metadata store offers the means to access them as Python objects. I've found that the typical TFX pipeline workflow handles this implicitly; however, when you need to circumvent the pipeline and perform direct examination, specific steps are necessary.

Here’s a breakdown of how to achieve this:

1. **Retrieve Artifact URI:** The first step involves accessing the output artifact generated by the StatisticsGen component. Each TFX component generates output artifacts that are stored, along with relevant metadata, in the metadata store. Using a MetadataStore object, you can query for the specific StatisticsGen execution. Each execution corresponds to an artifact URI pointing to the statistics data.

2. **Load the Statistics Proto:** Once you have the URI, you need to load the serialized statistics proto files. The output artifact can contain one or more serialized files corresponding to separate dataset splits, so each file needs processing. The proto messages are decoded into a Python object using the appropriate proto library.

3. **Parse the Statistics:** The loaded proto message contains the actual statistical values for each feature. Accessing and interpreting them requires awareness of the structure of the *DatasetFeatureStatisticsList* message. This involves looping through features and their statistics.

To illustrate these steps, consider three practical code examples:

**Example 1: Accessing Basic Statistics**

This code shows how to extract common statistics like `min`, `max`, `mean`, and `std_dev` for numeric features. In a custom TFX component I developed for data quality monitoring, this was essential for creating baseline data profiles.

```python
import tensorflow as tf
from tfx.orchestration import metadata
from tfx.proto import example_gen_pb2
from tensorflow_metadata.proto.v1 import statistics_pb2

def get_numeric_stats(metadata_connection_string):
    """Access and print basic statistics for numeric features."""
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_connection_string
    )
    store = metadata.MetadataStore(connection_config)
    
    # Query for the StatisticsGen executions
    stats_gen_artifact_types = store.get_artifact_types(
        names=["Stats"])  # Artifact type name for StatisticsGen output is "Stats"
    if not stats_gen_artifact_types:
        print("No StatisticsGen executions found.")
        return

    stats_gen_executions = store.get_executions_by_type(
        stats_gen_artifact_types[0].id
    )
    if not stats_gen_executions:
       print("No executions of this type")
       return
    
    # Get latest execution
    latest_execution = max(stats_gen_executions, key=lambda exec: exec.id)
    
    # Get the output artifacts of the latest execution
    output_artifacts = store.get_artifacts_by_execution_id(latest_execution.id)
    stats_artifacts = [artifact for artifact in output_artifacts if artifact.type_id == stats_gen_artifact_types[0].id]
    
    if not stats_artifacts:
        print("No statistics artifacts found for this execution.")
        return

    for artifact in stats_artifacts:
        for uri_part in tf.io.gfile.glob(artifact.uri + "/*"):
            dataset_stats_list = statistics_pb2.DatasetFeatureStatisticsList()
            with tf.io.gfile.GFile(uri_part, 'rb') as f:
                dataset_stats_list.ParseFromString(f.read())

            for dataset_stats in dataset_stats_list.datasets:
                for feature_stats in dataset_stats.features:
                     if feature_stats.HasField("num_stats"):
                            num_stats = feature_stats.num_stats
                            print(f"Feature: {feature_stats.name}")
                            print(f"  Min: {num_stats.min}")
                            print(f"  Max: {num_stats.max}")
                            print(f"  Mean: {num_stats.mean}")
                            print(f"  Std Dev: {num_stats.std_dev}")
```

This code iterates through each execution artifact, loads the protobuf files from the file system location denoted by the URI, and then iterates through the feature statistics, printing basic information for any numerical feature. I've found it crucial to handle cases with multiple file fragments as StatisticsGen can write the data into multiple part files.

**Example 2: Accessing String Statistics**

String feature statistics can differ, providing information like the count of each unique value, using histograms. This example demonstrates how to retrieve string statistics. I used this when exploring categorical feature distributions and identifying possible data imbalances during model development.

```python
import tensorflow as tf
from tfx.orchestration import metadata
from tfx.proto import example_gen_pb2
from tensorflow_metadata.proto.v1 import statistics_pb2

def get_string_stats(metadata_connection_string):
    """Access and print statistics for string features."""
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_connection_string
    )
    store = metadata.MetadataStore(connection_config)
    
    # Query for the StatisticsGen executions
    stats_gen_artifact_types = store.get_artifact_types(
        names=["Stats"])  # Artifact type name for StatisticsGen output is "Stats"
    if not stats_gen_artifact_types:
        print("No StatisticsGen executions found.")
        return

    stats_gen_executions = store.get_executions_by_type(
        stats_gen_artifact_types[0].id
    )
    if not stats_gen_executions:
       print("No executions of this type")
       return
    
    # Get latest execution
    latest_execution = max(stats_gen_executions, key=lambda exec: exec.id)
    
    # Get the output artifacts of the latest execution
    output_artifacts = store.get_artifacts_by_execution_id(latest_execution.id)
    stats_artifacts = [artifact for artifact in output_artifacts if artifact.type_id == stats_gen_artifact_types[0].id]
    
    if not stats_artifacts:
        print("No statistics artifacts found for this execution.")
        return


    for artifact in stats_artifacts:
        for uri_part in tf.io.gfile.glob(artifact.uri + "/*"):
            dataset_stats_list = statistics_pb2.DatasetFeatureStatisticsList()
            with tf.io.gfile.GFile(uri_part, 'rb') as f:
               dataset_stats_list.ParseFromString(f.read())

            for dataset_stats in dataset_stats_list.datasets:
                for feature_stats in dataset_stats.features:
                    if feature_stats.HasField("string_stats"):
                            string_stats = feature_stats.string_stats
                            print(f"Feature: {feature_stats.name}")
                            print("  Unique values:")
                            for bin in string_stats.rank_histogram.buckets:
                                print(f"    {bin.label}: {bin.sample_count}")
```
This code provides the distinct value counts, useful for debugging categorical feature cardinality and distribution. String statistics are primarily accessed via the `rank_histogram` field of the `string_stats` proto message.

**Example 3: Accessing Schema Information**

The Schema proto also holds details regarding the expected data types for each feature. Accessing this information is valuable when understanding the assumed data types within a TFX pipeline.

```python
import tensorflow as tf
from tfx.orchestration import metadata
from tfx.proto import example_gen_pb2
from tensorflow_metadata.proto.v1 import schema_pb2

def get_schema_info(metadata_connection_string):
    """Access and print schema information."""
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_connection_string
    )
    store = metadata.MetadataStore(connection_config)

    # Query for the SchemaGen executions
    schema_gen_artifact_types = store.get_artifact_types(
        names=["Schema"])
    if not schema_gen_artifact_types:
        print("No SchemaGen executions found.")
        return

    schema_gen_executions = store.get_executions_by_type(
        schema_gen_artifact_types[0].id
    )
    if not schema_gen_executions:
       print("No executions of this type")
       return
    
    # Get latest execution
    latest_execution = max(schema_gen_executions, key=lambda exec: exec.id)

    # Get the output artifacts of the latest execution
    output_artifacts = store.get_artifacts_by_execution_id(latest_execution.id)
    schema_artifacts = [artifact for artifact in output_artifacts if artifact.type_id == schema_gen_artifact_types[0].id]
    
    if not schema_artifacts:
        print("No schema artifacts found for this execution.")
        return

    for artifact in schema_artifacts:
        for uri_part in tf.io.gfile.glob(artifact.uri + "/*"):
            schema = schema_pb2.Schema()
            with tf.io.gfile.GFile(uri_part, 'rb') as f:
                schema.ParseFromString(f.read())

            for feature in schema.feature:
                print(f"Feature: {feature.name}, Type: {feature.type}")
```

The code above shows how to access the Schema artifact generated by a SchemaGen component (which typically feeds the StatisticsGen component). I've found this particularly useful in ensuring type consistency across the data preprocessing and model building stages of my pipelines.

The TFX library's internal `_make_proto` functions of the `tfx.components.statistics_gen.executor.Executor` class offer a way to programmatically create these statistic protobufs. Although this function is intended for internal component execution, I use it to debug and generate sample protobuf structures to cross-reference my decoding efforts and ensure accuracy. However, this requires a deeper understanding of the internal TFX workings.

**Resource Recommendations**

To deepen understanding and use of StatisticsGen data, I recommend consulting the following resources. These do not include specific URLs, to comply with the instructions.

*   **TensorFlow Metadata Library (tfmd):** The official documentation provides details about the proto structures used in TFX, particularly the `tensorflow_metadata.proto.v1` module.
*   **TFX Component API:** Exploring the TFX library code, particularly the source code for the `StatisticsGen` component and the `MetadataStore` class, provides insights into its internal workings.
*   **TensorFlow API Documentation:** Reviewing the general TensorFlow documentation related to protocol buffers can be helpful in understanding how to parse and manipulate proto messages.

Accessing and utilizing StatisticsGen statistics directly, while not part of the standard TFX pipeline workflow, is attainable by navigating the TFX metadata store, understanding the relevant proto message structures, and crafting custom processing logic. This allows detailed analysis and targeted interventions within TFX pipelines when required. This advanced skill enables more control over data analysis and model tuning.

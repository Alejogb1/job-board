---
title: "How can I print data fed to a Keras model.fit() using a Petastorm dataset?"
date: "2025-01-30"
id: "how-can-i-print-data-fed-to-a"
---
Debugging data pipelines feeding into a Keras model during training, especially when using a Petastorm dataset, requires a specific approach due to the asynchronous nature of data loading. Directly printing the output within the `model.fit()` call is generally ineffective. The key fact is that the data provided to `model.fit()` is an iterator generated by Petastorm, not directly accessible within the training loop itself. Instead, we must inspect the data at the source, which is the iterator yielded by Petastorm's `make_batch_reader`.

My experience building distributed training pipelines for large image datasets has repeatedly highlighted this challenge. Typically, a seemingly straightforward `model.fit()` call hides the complexities of the data feeding process underneath. The problem is that Petastorm, designed for efficient data access from parquet files, uses a reader class which internally handles batching and preprocessing, meaning the data only becomes available as batches within the reader and the reader is what's being passed to `fit` not the actual dataset.

To print the data batches, we need to access the iterator provided by Petastorm and extract a few batches manually prior to initiating the training process. This allows inspection of the data before it ever reaches the model. Additionally, after confirming that the data looks correct, we can establish a logger during training to capture the model inputs along with associated output.

**Explanation:**

The Petastorm library's `make_batch_reader` method returns an iterable that yields batches of data based on the defined schema and options. This iterator is then passed directly to `model.fit()`. The model itself only sees the batched data, not the raw dataset or the internal steps of how the iterator produces data. Directly printing within the keras training loop will print the output of the model itself, not the input data from the dataset. The data prefetching by Petastorm to enhance training speed is a strength but makes debugging more nuanced.

To observe data, we must create our own loop on the output of `make_batch_reader` and print data accordingly. We can achieve this with a straightforward iteration and access to data inside that iteration, which allows inspection of a few sample batches before passing the iterator to the `fit()` method. Furthermore, while not strictly a direct printing from the data fed to `model.fit()`, using a custom callback during training allows observation of data as it is being passed into the model if debugging is needed during a full training process.

Here is how we can do that using code examples:

**Code Example 1: Direct Inspection of Petastorm Iterator:**

This code demonstrates how to grab data from the Petastorm reader and print the first few batches.

```python
from petastorm import make_batch_reader
from petastorm.tf_utils import make_petastorm_dataset
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

# Assume a basic dummy parquet file has been created according to the petastorm documentation.
# Create a very basic parquet file, for illustration purposes.
rows = 500
data_location = "file:///tmp/my_test_parquet"

# Dummy data for parquet file.
np_array = np.random.rand(rows, 5)
metadata = [ {"x": i} for i in range(rows)]
import pandas as pd
df = pd.DataFrame({'feature': np_array.tolist(), "meta": metadata})
import pyarrow as pa
import pyarrow.parquet as pq
table = pa.Table.from_pandas(df)
pq.write_table(table, data_location)

# Sample schema definition
schema_fields = ['feature', 'meta']


with make_batch_reader(data_location, schema_fields=schema_fields) as reader:
    iterator = iter(reader)
    for i in range(3):  # Inspect the first 3 batches
        try:
          batch = next(iterator)
          print(f"Batch {i+1} Features Shape:", np.array(batch.feature).shape)
          print(f"Batch {i+1} Metadata:", batch.meta)
        except StopIteration:
            print("End of dataset reached.")
            break

# Rest of the training process can proceed.
# Below code is provided for a minimal example and would need to be replaced for an actual use case.
# This is here to demonstrate where the reader would get passed to fit.

model = Sequential([Dense(10, activation="relu", input_shape=(5,)), Dense(1)])
dataset = make_petastorm_dataset(reader)
model.compile(optimizer='adam', loss='mse')
model.fit(dataset, epochs=2)

```

**Commentary:** This code snippet utilizes the `make_batch_reader` to create a reader and then fetches the first three batches directly.  We then print the shape of the numpy arrays contained in the features to confirm the correct batching. We can confirm that batch size, data type and more as needed by accessing the elements of the batch directly. This allows us to verify that the data is in the expected format before training begins.

**Code Example 2:  Logging Data During Training using a custom callback**

This example demonstrates how to create a simple Keras callback to log the input data during training. The key here is to use the `on_train_batch_begin` callback that receives the batch as it is about to be processed by the model.

```python
from petastorm import make_batch_reader
from petastorm.tf_utils import make_petastorm_dataset
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf
import numpy as np
from tensorflow.keras.callbacks import Callback

# Assume a basic dummy parquet file has been created according to the petastorm documentation.
# Create a very basic parquet file, for illustration purposes.
rows = 500
data_location = "file:///tmp/my_test_parquet"

# Dummy data for parquet file.
np_array = np.random.rand(rows, 5)
metadata = [ {"x": i} for i in range(rows)]
import pandas as pd
df = pd.DataFrame({'feature': np_array.tolist(), "meta": metadata})
import pyarrow as pa
import pyarrow.parquet as pq
table = pa.Table.from_pandas(df)
pq.write_table(table, data_location)


class DataLogger(Callback):
  def on_train_batch_begin(self, batch, logs=None):
        if logs is None:
            logs = {}
        if batch % 10 == 0:
          inputs = self.model.inputs
          data = self.model.inputs
          print("Batch Input Shape:", np.array(data[0]).shape)


schema_fields = ['feature', 'meta']

with make_batch_reader(data_location, schema_fields=schema_fields) as reader:
    dataset = make_petastorm_dataset(reader)

model = Sequential([Dense(10, activation="relu", input_shape=(5,)), Dense(1)])
model.compile(optimizer='adam', loss='mse')
model.fit(dataset, epochs=2, callbacks=[DataLogger()])
```

**Commentary:** This example uses a Keras callback to intercept batches before they enter the model. Inside `on_train_batch_begin` we fetch the inputs passed to the model and print their shapes. This callback lets us verify that the data passed to the model during each batch is as expected. Note that we fetch the model inputs using the model.inputs property.  This allows observation of the data during training itself instead of before.

**Code Example 3:  Logging Data During Training using a custom training loop**

This example demonstrates how to implement a custom training loop and log data during the training process. This provides ultimate flexibility to debug data issues but requires more code overhead.

```python
from petastorm import make_batch_reader
from petastorm.tf_utils import make_petastorm_dataset
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf
import numpy as np

# Assume a basic dummy parquet file has been created according to the petastorm documentation.
# Create a very basic parquet file, for illustration purposes.
rows = 500
data_location = "file:///tmp/my_test_parquet"

# Dummy data for parquet file.
np_array = np.random.rand(rows, 5)
metadata = [ {"x": i} for i in range(rows)]
import pandas as pd
df = pd.DataFrame({'feature': np_array.tolist(), "meta": metadata})
import pyarrow as pa
import pyarrow.parquet as pq
table = pa.Table.from_pandas(df)
pq.write_table(table, data_location)

schema_fields = ['feature', 'meta']


model = Sequential([Dense(10, activation="relu", input_shape=(5,)), Dense(1)])
optimizer = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.MeanSquaredError()

with make_batch_reader(data_location, schema_fields=schema_fields) as reader:
    dataset = make_petastorm_dataset(reader)

    for epoch in range(2):
        for batch_idx, batch in enumerate(dataset):
          if batch_idx % 10 == 0:
            print("Batch Input Shape:", np.array(batch[0]).shape)
          with tf.GradientTape() as tape:
              output = model(batch[0])
              loss = loss_fn(batch[0], output)
          gradients = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

**Commentary:** This code implements a custom training loop, where the user is in complete control over how data is fed into the model. We iterate over the iterator manually using a for loop, and we can print the data before each batch is fed into the model. This provides a way to understand data before model processing for each batch and allows for more complete debugging and control of the training process.

**Resource Recommendations:**

For deeper insights into Petastorm:

1.  The official Petastorm documentation offers a comprehensive overview of its features, including data loading and schema definition.
2.  Review the examples available in the Petastorm repository on GitHub. These examples often showcase common use cases and best practices for working with different data formats and transformations.
3.  Study the Tensorflow Keras documentation on custom training loops and custom callbacks. These resources will be invaluable in creating specialized logging for your use case.

Debugging data pipelines is an iterative process and these methods provide a starting point to understand how to extract the data batches you are feeding to Keras models when using a Petastorm reader. Remember to adapt the logging and debugging methods to your specific data formats and use cases.

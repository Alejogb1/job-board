---
title: "How can I extract output tokens from a model's last hidden state?"
date: "2025-01-30"
id: "how-can-i-extract-output-tokens-from-a"
---
Extracting output tokens from a model's last hidden state necessitates a nuanced understanding of the model's architecture and the decoding process.  Crucially, the last hidden state doesn't directly contain the output tokens themselves; rather, it encodes the contextual information upon which the output token probabilities are predicated.  This understanding informed my approach during the development of a multilingual sentiment analysis system, where accurately retrieving nuanced sentiment required precise decoding from the final hidden layer.

My experience highlights that the method for extraction hinges on the type of model used.  Transformer-based models, recurrent neural networks (RNNs), and other architectures employ different mechanisms to generate output.  A consistent principle, however, is the utilization of a softmax function to convert the raw output of the last hidden state into a probability distribution over the vocabulary.  The token with the highest probability is then selected as the predicted output token.  This process is iteratively repeated during inference, with the previously generated token influencing the next prediction through the model's inherent mechanisms (e.g., autoregressive generation).

**1.  Explanation:**

The extraction process typically involves three primary steps:

a. **Obtaining the last hidden state:** This step involves running the input sequence through the model and accessing the final hidden representation generated by the model's encoder or decoder.  The specific method for accessing this state is model-dependent and typically involves accessing an attribute or method provided by the model's framework (e.g., `model.hidden_states` in some frameworks).

b. **Applying a linear layer (optional):**  Many models employ a linear projection layer after the last hidden state to map the high-dimensional hidden representation to the dimension of the vocabulary. This linear transformation adjusts the representation to better suit the task of predicting the probability distribution over tokens.  The weights of this layer are learned during training.

c. **Applying the softmax function:** The output of the linear layer (or the last hidden state directly, if no linear layer is used) is fed into the softmax function.  This function converts the raw scores into a probability distribution, where each element represents the probability of a specific token being the next output token.  The token corresponding to the maximum probability is then selected.

The entire process forms a decoding step, mapping the model's internal representation to a discrete set of output tokens.  Note that for sequence generation tasks, this process is repeated sequentially, feeding the predicted token back into the model to generate the next token until an end-of-sequence token is produced.


**2. Code Examples with Commentary:**

The following examples illustrate the extraction process for different hypothetical model architectures, focusing on the conceptual steps rather than specific framework implementations.  Each example assumes a simplified scenario for clarity.

**Example 1:  Simplified RNN Model**

```python
import numpy as np

# Hypothetical RNN output (last hidden state)
last_hidden_state = np.array([0.2, 0.5, 0.1, 0.2])

# Vocabulary (simplified for demonstration)
vocabulary = ["a", "b", "c", "d"]

# Apply softmax
probabilities = np.exp(last_hidden_state) / np.sum(np.exp(last_hidden_state))

# Get the index of the most likely token
predicted_index = np.argmax(probabilities)

# Extract the predicted token
predicted_token = vocabulary[predicted_index]

print(f"Probabilities: {probabilities}")
print(f"Predicted token: {predicted_token}")
```

This example demonstrates a simplistic scenario.  A real-world RNN would have a much larger hidden state and vocabulary.  The crucial steps are the application of the softmax function and identification of the highest probability index.

**Example 2:  Simplified Transformer Model with Linear Projection**

```python
import numpy as np

# Hypothetical last hidden state from Transformer encoder
last_hidden_state = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.2, 0.1, 0.2]])

# Hypothetical linear projection weights
W = np.array([[0.2, 0.1, 0.3, 0.4], [0.1, 0.4, 0.2, 0.3], [0.3, 0.2, 0.4, 0.1], [0.4, 0.3, 0.1, 0.2]])

# Apply linear projection
projected_output = np.dot(last_hidden_state, W)

# Apply softmax (separately for each sequence element)
probabilities = np.exp(projected_output) / np.sum(np.exp(projected_output), axis=1, keepdims=True)

# Get the index of the most likely token for each sequence element
predicted_indices = np.argmax(probabilities, axis=1)

# Assuming a vocabulary (replace with your actual vocabulary)
vocabulary = ["a", "b", "c", "d"]

#Extract predicted tokens
predicted_tokens = [vocabulary[i] for i in predicted_indices]

print(f"Probabilities:\n{probabilities}")
print(f"Predicted tokens: {predicted_tokens}")

```

This example introduces a linear projection layer, common in transformer models, before the softmax function.  The linear transformation projects the hidden state to a space more suitable for predicting token probabilities.  Note the batch processing (multiple sequences).

**Example 3:  Beam Search Decoding (Conceptual)**

```python
# Hypothetical probability matrix from a decoder (multiple possible next tokens)
probability_matrix = np.array([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7]])

# Beam width
beam_width = 2

# Initialize beam
beam = [(0, "", [])] # (score, sequence, token_indices)

# Beam search iterations (simplified for demonstration)
for i in range(2): # Generate 2 tokens
    new_beam = []
    for score, sequence, indices in beam:
        for j in range(probability_matrix.shape[1]):
            new_score = score + np.log(probability_matrix[len(indices),j])
            new_sequence = sequence + vocabulary[j]
            new_indices = indices + [j]
            new_beam.append((new_score, new_sequence, new_indices))
    # Sort by score, select top beam_width
    new_beam.sort(reverse=True)
    beam = new_beam[:beam_width]

# Get the best sequence
best_sequence = beam[0][1]

print(f"Best sequence: {best_sequence}")
```

This example showcases beam search, a decoding algorithm that considers multiple potential token sequences instead of greedily selecting only the most likely one at each step.  The best sequence, obtained after several iterations, provides a more robust solution than the single-token prediction methods.  The example demonstrates the essential structure; actual implementations involve more detailed handling of probabilities and sequence management.


**3. Resource Recommendations:**

For a deeper understanding, I recommend consulting standard textbooks on natural language processing, deep learning, and sequence modeling.  Furthermore, review the documentation for relevant deep learning frameworks; each framework provides unique utilities and functionalities for interacting with model internals.  Finally, explore research papers on decoder architectures and decoding algorithms for advanced techniques beyond greedy decoding and beam search, such as sampling methods.

---
title: "Why does the DNN model's time series prediction shape differ from the trained labels in a windowed dataset?"
date: "2025-01-30"
id: "why-does-the-dnn-models-time-series-prediction"
---
The mismatch between a deep neural network (DNN) model's time series prediction shape and the trained labels, particularly within a windowed dataset context, frequently arises from a fundamental difference in how the model interprets the input and how the windowing process creates training targets.  Essentially, the model learns to predict *relative* changes or *next steps* based on sequences, while the windowed labels represent absolute values at specific points.  I’ve encountered this across various projects, from industrial sensor data to financial market modeling.  This discrepancy impacts not just prediction shape, but the evaluation metrics as well if not carefully handled.

A DNN trained on windowed time series data isn’t inherently equipped to extrapolate beyond the immediate context presented by the input window. The windowing method, often used to create manageable training samples from a continuous time series, introduces a subtle but crucial distinction between what the model *sees* and what it's *trained to predict*. When we create windows, we take a fixed-length segment of the series as input (e.g., 10 data points representing the past) and a corresponding target, which is commonly a single point or a short sequence of points occurring after the input window.  The model, therefore, isn't learning to reproduce a shape; instead, it is learning to transform the features within a fixed input window into an output target related to the end (or close to it) of this window.

Let's consider a simplified scenario. If you are working with a time series representing temperature fluctuations across days and employing a window size of 5, the DNN's input might be the temperatures of the last 5 days, and its output might be the temperature for the 6th day, or potentially the average temperature for the next 2-3 days. The labels in this dataset, constructed by the windowing mechanism, are discrete points or short sequences that follow each specific input window.  The DNN attempts to learn the function that maps the input window's features (e.g., trends, seasonality) to this specific target. If you then feed the trained model a time series not used for training, it’s going to output points generated by the same function, but these points will not naturally conform to the general shape of labels used for training. The generated sequence will have its own characteristics based on the input, but lack direct correspondence in global shape characteristics to windowed labels.

One of the biggest problems arises when the output shape in the training data is different from the input shape in training. If you have an input sequence of 10 values and you train to predict only the last value in sequence, you have a different shape for the model’s output and training label. When you then feed it a test sequence of length 10, and ask the model to predict a sequence of 10 values, the model won’t have been trained on that scenario. This often results in the model generating a single output value that gets replicated or a sequence of values that doesn't align with the original training data in shape or scale.

To demonstrate this, consider these code examples.

**Code Example 1: Simple RNN with single-point prediction**

```python
import numpy as np
import tensorflow as tf

# Sample Time Series Data
time_series = np.sin(np.linspace(0, 10 * np.pi, 1000))
window_size = 20
stride = 1

# Windowing function
def create_windows(data, window, stride):
  windows = []
  labels = []
  for i in range(0, len(data) - window, stride):
    windows.append(data[i:i + window])
    labels.append(data[i + window])
  return np.array(windows), np.array(labels)

windows, labels = create_windows(time_series, window_size, stride)
windows = np.reshape(windows, (windows.shape[0], windows.shape[1], 1))
labels = np.reshape(labels, (labels.shape[0], 1))


# RNN Model
model = tf.keras.models.Sequential([
    tf.keras.layers.SimpleRNN(50, activation='relu', input_shape=(window_size, 1)),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')

model.fit(windows, labels, epochs=10, verbose=0)

# Prediction on full dataset using only the first window as input
full_input = np.reshape(time_series[:window_size], (1, window_size, 1))
predictions = []

for i in range(len(time_series) - window_size):
    pred = model.predict(full_input, verbose=0)
    predictions.append(pred[0, 0])
    full_input = np.append(full_input[:, 1:, :], np.reshape(pred, (1,1,1)), axis=1)

import matplotlib.pyplot as plt
plt.plot(time_series[window_size:], label="True Series (shifted)")
plt.plot(predictions, label="Predictions")
plt.legend()
plt.title("Simple RNN single point Prediction")
plt.show()

```

In this example, the RNN is trained to predict the *next* value given an input window.  The training labels are single values at the end of a window. When we predict on the full dataset, we can feed in the first window, and then iteratively append each new prediction to the input sequence, which creates a long series from single-point predictions. The output shape here differs drastically from original sine wave shape. It will continue the trend, but not reproduce the original data's shape over a long time scale.

**Code Example 2: LSTM with sequence-to-sequence prediction.**

```python
import numpy as np
import tensorflow as tf

# Sample Time Series Data
time_series = np.sin(np.linspace(0, 10 * np.pi, 1000))
window_size = 20
forecast_horizon = 5
stride = 1

# Windowing function for sequence prediction
def create_windows_seq(data, window, forecast, stride):
    windows = []
    labels = []
    for i in range(0, len(data) - window - forecast + 1, stride):
        windows.append(data[i:i + window])
        labels.append(data[i + window: i + window + forecast])
    return np.array(windows), np.array(labels)

windows, labels = create_windows_seq(time_series, window_size, forecast_horizon, stride)
windows = np.reshape(windows, (windows.shape[0], windows.shape[1], 1))
labels = np.reshape(labels, (labels.shape[0], labels.shape[1], 1))

# LSTM Model
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(50, activation='relu', input_shape=(window_size, 1), return_sequences=True),
    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))
])

model.compile(optimizer='adam', loss='mse')
model.fit(windows, labels, epochs=10, verbose=0)

# Prediction on full dataset using only the first window as input
full_input = np.reshape(time_series[:window_size], (1, window_size, 1))
predictions = []

for i in range(len(time_series) - window_size - forecast_horizon + 1):
    pred = model.predict(full_input, verbose=0)
    predictions.append(pred[0, :, 0])
    full_input = np.concatenate((full_input[:, forecast_horizon:, :], np.reshape(pred, (1, forecast_horizon,1))), axis = 1)


import matplotlib.pyplot as plt
plt.plot(time_series[window_size + forecast_horizon-1:], label="True Series (shifted)")
plt.plot(np.concatenate(predictions), label="Predictions")
plt.legend()
plt.title("LSTM sequence-to-sequence Prediction")
plt.show()

```

Here, I use a recurrent layer and also output the full predicted sequence. We need to use `TimeDistributed` to make sure that every time step in the sequence goes through the `Dense` layer. Again, even when predicting the full sequence, the output doesn't resemble the training series' shape, especially if you predict iteratively.  The output follows a short-term trend given by each input window but doesn't reflect the long-term trend of the overall sine wave.

**Code Example 3: Convolutional model with single-point prediction**

```python
import numpy as np
import tensorflow as tf

# Sample Time Series Data
time_series = np.sin(np.linspace(0, 10 * np.pi, 1000))
window_size = 20
stride = 1

# Windowing function
def create_windows(data, window, stride):
  windows = []
  labels = []
  for i in range(0, len(data) - window, stride):
    windows.append(data[i:i + window])
    labels.append(data[i + window])
  return np.array(windows), np.array(labels)

windows, labels = create_windows(time_series, window_size, stride)
windows = np.reshape(windows, (windows.shape[0], windows.shape[1], 1))
labels = np.reshape(labels, (labels.shape[0], 1))


# Convolutional Model
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(window_size, 1)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')

model.fit(windows, labels, epochs=10, verbose=0)

# Prediction on full dataset using only the first window as input
full_input = np.reshape(time_series[:window_size], (1, window_size, 1))
predictions = []

for i in range(len(time_series) - window_size):
    pred = model.predict(full_input, verbose=0)
    predictions.append(pred[0, 0])
    full_input = np.append(full_input[:, 1:, :], np.reshape(pred, (1,1,1)), axis=1)

import matplotlib.pyplot as plt
plt.plot(time_series[window_size:], label="True Series (shifted)")
plt.plot(predictions, label="Predictions")
plt.legend()
plt.title("Simple Conv1D single point Prediction")
plt.show()

```

In this convolutional network example, the model learns local patterns in the windows but again does not predict a shape resembling the full series.  The output is a series of single points which are concatenated together.  The Convolutional network acts as a feature extractor for each window, but again does not predict the overall shape.

To summarize, the DNN models are learning a local function based on each window of data.  These models do not learn the global characteristics or shapes of the time series.

To improve this, one would likely need to explore techniques like:

*   **Sequence-to-Sequence models:** These models are specifically designed to output a sequence based on a sequence, rather than a single point or a short subsequence. This often involves an encoder-decoder architecture.
*   **Attention Mechanisms:** Incorporating attention layers allows the model to focus on specific parts of the input sequence that are relevant to the output.
*   **Transformer Networks:**  The Transformer architecture, known for its performance in NLP, can also be applied to time series, capturing long-range dependencies.
*   **Careful Input/Output Shape Matching:** Ensure that the model predicts a sequence of length comparable to the input, if you want the model to learn a similar output shape to the input.
*   **Using other datasets:**  Using a dataset that includes full examples of the target shape may help in some contexts.
* **Feature Engineering:** Carefully selecting features, such as adding seasonality information to the input window, may result in better shape predictions.

Further learning can be gained from resources explaining recurrent neural networks, particularly LSTMs and GRUs, also from information on Convolutional Networks and their applications to sequential data.  Research the concepts behind sequence-to-sequence modelling, and how attention can enhance them. Understanding these concepts has helped me significantly in dealing with real world time-series projects, and it will help you tackle these types of problems more effectively.

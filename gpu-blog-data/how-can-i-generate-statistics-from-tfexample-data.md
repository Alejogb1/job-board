---
title: "How can I generate statistics from tf.Example data generated by a Beam pipeline using GenerateStatistics?"
date: "2025-01-30"
id: "how-can-i-generate-statistics-from-tfexample-data"
---
`tf.Example` data, the fundamental serialization format for TensorFlow, frequently originates from large-scale data processing pipelines, commonly built with Apache Beam. Efficiently generating statistics from this data, rather than manually parsing and calculating them, is essential for model development and validation. The `GenerateStatistics` transform within TensorFlow Data Validation (TFDV) directly addresses this, simplifying the aggregation of vital statistical information.

**Explanation:**

The `GenerateStatistics` transform in TFDV operates on a Beam PCollection of `tf.Example` records. It analyzes the schema of the data and, based on this schema, calculates a wide range of statistics for each feature. These statistics encompass measures like mean, standard deviation, min, max, number of missing values, unique value counts, and histograms for numerical features. For categorical or string features, it calculates similar information such as vocabulary size, frequent values, and histograms over string values (represented as bytes). The transform outputs a `tf.Example` containing these generated statistics. This output `tf.Example` is typically ingested further into TFDV workflows for visualization, schema inference, and data validation.

The core benefit of using `GenerateStatistics` comes from its tight integration with TensorFlow and Beam, allowing it to efficiently distribute the statistical computations across a cluster, thus enabling rapid processing of very large datasets. Instead of requiring developers to implement custom aggregation logic, this transform encapsulates these procedures behind a simple interface. The automatic handling of various data types, including numeric, string, and binary data, further eases the development process. The calculated statistics are then used in downstream tasks like schema generation, which involves inferring the expected type, domain, and range of the features present in the data, and data validation, which identifies anomalous data points or features that deviate from the expected distributions, essential for maintaining training set quality and addressing data drift in machine learning workflows. I've personally leveraged these outputs extensively to diagnose issues during model training and deployment.

**Code Examples:**

Here are three illustrative code examples demonstrating how to use `GenerateStatistics`, coupled with commentary on each case:

*Example 1: Basic usage with default options.*

```python
import apache_beam as beam
import tensorflow as tf
from tensorflow_data_validation.beam import generate_statistics
from tensorflow_data_validation.utils import display_util
from google.protobuf import text_format

# Mock tf.Example creation function
def _create_example(x, y, z):
    feature = {
        'x': tf.train.Feature(float_list=tf.train.FloatList(value=[x])),
        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),
        'z': tf.train.Feature(bytes_list=tf.train.BytesList(value=[z.encode('utf-8')])),
    }
    return tf.train.Example(features=tf.train.Features(feature=feature))

# Mock PCollection of tf.Example records
def create_mock_data():
    return [
        _create_example(1.0, 1, 'A'),
        _create_example(2.0, 2, 'B'),
        _create_example(3.0, 1, 'A'),
        _create_example(4.0, 3, 'C'),
    ]

with beam.Pipeline() as pipeline:
    input_data = pipeline | 'CreateData' >> beam.Create(create_mock_data())
    stats = input_data | 'GenerateStats' >> generate_statistics.GenerateStatistics()
    
    # Output the stats (text-formatted) for inspection
    def process_stats(stats_example):
      print(text_format.MessageToString(stats_example, as_utf8=True))
    
    stats | 'PrintStats' >> beam.Map(process_stats)

```

This example demonstrates the simplest invocation of `GenerateStatistics`. It constructs a small, in-memory Beam pipeline, creates mock `tf.Example` data, and passes this PCollection to `GenerateStatistics`. The output, a `tf.Example` containing the generated statistics, is then printed. Observe that, by default, the transform correctly identifies data types (float, int, and bytes) and calculates descriptive statistics for each. No schema needs to be supplied at this point, TFDV infers it during execution.

*Example 2: Providing a custom schema.*

```python
import apache_beam as beam
import tensorflow as tf
from tensorflow_data_validation.beam import generate_statistics
from tensorflow_data_validation import types
from tensorflow_metadata.proto.v0 import schema_pb2
from google.protobuf import text_format

# Mock tf.Example creation function (same as example 1)
def _create_example(x, y, z):
    feature = {
        'x': tf.train.Feature(float_list=tf.train.FloatList(value=[x])),
        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),
        'z': tf.train.Feature(bytes_list=tf.train.BytesList(value=[z.encode('utf-8')])),
    }
    return tf.train.Example(features=tf.train.Features(feature=feature))

# Mock PCollection of tf.Example records (same as example 1)
def create_mock_data():
    return [
        _create_example(1.0, 1, 'A'),
        _create_example(2.0, 2, 'B'),
        _create_example(3.0, 1, 'A'),
        _create_example(4.0, 3, 'C'),
    ]

# Define a custom schema
schema_text = """
feature {
  name: "x"
  type: FLOAT
  presence {
    min_fraction: 1.0
  }
}
feature {
    name: "y"
    type: INT
}
feature {
    name: "z"
    type: BYTES
}
"""
schema = text_format.Parse(schema_text, schema_pb2.Schema())

with beam.Pipeline() as pipeline:
    input_data = pipeline | 'CreateData' >> beam.Create(create_mock_data())
    stats = input_data | 'GenerateStats' >> generate_statistics.GenerateStatistics(
        schema=schema,
    )
    
    # Output the stats (text-formatted) for inspection
    def process_stats(stats_example):
      print(text_format.MessageToString(stats_example, as_utf8=True))
    
    stats | 'PrintStats' >> beam.Map(process_stats)
```

Here, the `GenerateStatistics` transform is initialized with a pre-defined schema. This allows more control over the statistical calculations (e.g., forcing type interpretation). Even though in this example the schema doesn't differ from TFDV's inferences based on data, providing a schema gives me, in practice, greater clarity and avoids potential data inconsistencies. It serves as a single source of truth, enhancing the robustness of large pipelines.

*Example 3: Using `stats_options` for more control.*

```python
import apache_beam as beam
import tensorflow as tf
from tensorflow_data_validation.beam import generate_statistics
from tensorflow_data_validation import statistics_options
from google.protobuf import text_format

# Mock tf.Example creation function (same as previous examples)
def _create_example(x, y, z):
    feature = {
        'x': tf.train.Feature(float_list=tf.train.FloatList(value=[x])),
        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),
        'z': tf.train.Feature(bytes_list=tf.train.BytesList(value=[z.encode('utf-8')])),
    }
    return tf.train.Example(features=tf.train.Features(feature=feature))

# Mock PCollection of tf.Example records (same as previous examples)
def create_mock_data():
    return [
        _create_example(1.0, 1, 'A'),
        _create_example(2.0, 2, 'B'),
        _create_example(3.0, 1, 'A'),
        _create_example(4.0, 3, 'C'),
    ]


# Define custom stats_options. We limit num_top_values to 2 to shorten the statistics output
stats_options = statistics_options.StatsOptions(
        num_top_values = 2
    )


with beam.Pipeline() as pipeline:
    input_data = pipeline | 'CreateData' >> beam.Create(create_mock_data())
    stats = input_data | 'GenerateStats' >> generate_statistics.GenerateStatistics(
        stats_options = stats_options,
    )
    
    # Output the stats (text-formatted) for inspection
    def process_stats(stats_example):
      print(text_format.MessageToString(stats_example, as_utf8=True))
    
    stats | 'PrintStats' >> beam.Map(process_stats)
```

In this final example, I illustrate the use of `StatsOptions`. The `StatsOptions` object allows granular control over aspects of the calculation like number of bins for histograms, desired sample sizes, and the number of top values to keep for categorical data. By passing this object, we tune the statistical computation to specific requirements. I have used this to cut down on processing time and memory usage when I only required a subset of all possible stats. Here we demonstrate that only the top 2 values of categorical features are computed.

**Resource Recommendations:**

For a deeper understanding and best practices, these resources will prove beneficial:

1.  **TensorFlow Data Validation (TFDV) documentation:** This provides a comprehensive overview of TFDV concepts, including schema inference, data validation, and `GenerateStatistics` internals. Focusing on the tutorials, one can learn to use TFDV in real-world projects.
2. **Apache Beam documentation:** To understand how Beam pipelines are constructed and how data flows, the Beam programming guide is a fundamental resource. Key areas include core concepts such as PCollections, transforms, and pipelines.
3.  **TensorFlow Metadata (TFMD) documentation:** Because schemas are a critical part of TFDV's use, understanding `Schema` protos and TFMD can be valuable. This includes topics such as feature specifications, type conversions, and custom domains.
4.  **TensorFlow documentation:** While this isnâ€™t specific to TFDV, a firm understanding of TensorFlow fundamentals, especially `tf.Example`, is essential. The serialization and deserialization logic for `tf.Example` will help in comprehending how TFDV operates.

These resources, when taken together, give you the necessary expertise to utilize `GenerateStatistics` effectively. Through practical implementation and thoughtful review of available documentation, users can obtain in-depth insights into how these components interoperate and how to leverage them in complex data processing workflows.

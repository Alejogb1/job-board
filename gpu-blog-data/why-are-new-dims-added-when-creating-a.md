---
title: "Why are new dims added when creating a padding mask?"
date: "2025-01-30"
id: "why-are-new-dims-added-when-creating-a"
---
Padding masks, encountered frequently in sequence processing tasks, such as Natural Language Processing (NLP), necessitate the addition of new dimensions primarily to facilitate broadcasting during matrix operations. Specifically, these extra dimensions ensure compatibility when a sequence of varying length is to be masked during attention calculations or other similar tensor manipulations. Understanding the inherent nature of these masks and the operational mechanics of broadcasting provides clarity on their dimensionality.

In my experience developing sequence-to-sequence models for machine translation, I often encountered situations where padding was necessary to ensure consistent batch sizes. Sequences within a batch, invariably differing in length, require padding up to the length of the longest sequence in the batch to form a rectangular tensor. This padding introduces 'dummy' tokens, typically represented by zeros, which must be ignored during subsequent computations. A padding mask, a binary tensor, is thus created where '1' indicates an actual token, and '0' denotes a padded token.

The core reason for adding new dimensions arises when applying this mask to other tensors, typically the query, key, and value tensors in a self-attention mechanism. These query, key, and value tensors are typically of the shape [batch_size, sequence_length, embedding_dimension], and the mask needs to be applied to every element along the embedding dimension, effectively across the 'last' two dimensions of those tensors. Directly applying a mask of the shape [batch_size, sequence_length] would not work due to shape mismatches. This leads to the use of broadcasting.

Broadcasting in array computation enables operations between tensors with different shapes, as long as certain compatibility rules are met. Specifically, smaller dimensions are "stretched" to match larger ones. In the context of applying a mask, we want to have the mask applied over the "embedding dimension" of query, key, and value tensors. To accomplish this, we need to reshape the mask such that it will "broadcast" across the embedding dimension, while leaving the other dimensions undisturbed.

Specifically, I frequently used the `tf.newaxis` operation (or its NumPy counterpart) when working with TensorFlow to add the dimension needed. We effectively reshape the mask from something like `[batch_size, sequence_length]` into `[batch_size, sequence_length, 1]`. This added dimension does not alter the information content of the mask but provides the structure necessary to apply the mask to the attention scores, which are usually of shape `[batch_size, sequence_length, sequence_length]`. This reshaping then allows the mask to broadcast across the last two dimensions of the attention score tensor effectively 'ignoring' the padded tokens.

Letâ€™s analyze three code examples demonstrating this concept, with practical context and annotations:

**Example 1: Creating and Applying a Padding Mask in TensorFlow**

```python
import tensorflow as tf

# Example batch of sequences with varying lengths.
sequences = tf.constant([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0], [6, 7, 8, 9, 0]])
padding_value = 0

# Generate the mask, '1' indicates valid token, '0' is padding
mask = tf.cast(sequences != padding_value, tf.float32)
print("Original Mask Shape:", mask.shape) # Output: Original Mask Shape: (3, 5)

# Expand the mask shape to allow broadcasting with the attention scores
expanded_mask = mask[:, :, tf.newaxis]
print("Expanded Mask Shape:", expanded_mask.shape) # Output: Expanded Mask Shape: (3, 5, 1)

# Example tensor representing attention scores.
attention_scores = tf.random.normal(shape=(3, 5, 5))

# Apply the mask. The mask is broadcast along the last dimension of attention_scores
masked_scores = attention_scores * expanded_mask
print("Masked Scores Shape:", masked_scores.shape) # Output: Masked Scores Shape: (3, 5, 5)
print("Masked scores: \n", masked_scores)
```

In this example, the initial mask is generated by comparing each element of the sequence with the padding value, resulting in a boolean mask, which is then cast to float32. `tf.newaxis` introduces a new dimension, enabling broadcasting with the attention scores. We then perform an element-wise multiplication, effectively zeroing out the attention scores corresponding to the padded tokens.

**Example 2: Applying the Mask with NumPy**

```python
import numpy as np

sequences = np.array([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0], [6, 7, 8, 9, 0]])
padding_value = 0

mask = (sequences != padding_value).astype(float)
print("Original Mask Shape:", mask.shape) # Output: Original Mask Shape: (3, 5)

expanded_mask = mask[:, :, np.newaxis]
print("Expanded Mask Shape:", expanded_mask.shape) # Output: Expanded Mask Shape: (3, 5, 1)

attention_scores = np.random.rand(3, 5, 5)

masked_scores = attention_scores * expanded_mask

print("Masked scores shape: ", masked_scores.shape) # Output: Masked scores shape: (3, 5, 5)
print("Masked Scores: \n", masked_scores)

```
This example mirrors the TensorFlow implementation but uses NumPy arrays and the corresponding `np.newaxis`. The functionality and purpose are identical, showcasing the concept across different array manipulation libraries. Again, we observe the mask's shape transforming with the addition of a new dimension, allowing for broadcasting in the element-wise multiplication.

**Example 3: Masking in Attention Computation**

```python
import tensorflow as tf
import numpy as np

batch_size = 3
seq_len = 5
embedding_dim = 128

sequences = tf.constant([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0], [6, 7, 8, 9, 0]])
padding_value = 0

mask = tf.cast(sequences != padding_value, tf.float32)
expanded_mask = mask[:, :, tf.newaxis]

# Example Query Key Value tensors
query = tf.random.normal(shape = (batch_size, seq_len, embedding_dim))
key = tf.random.normal(shape = (batch_size, seq_len, embedding_dim))
value = tf.random.normal(shape = (batch_size, seq_len, embedding_dim))

# Attention scores calculation.
attention_scores = tf.matmul(query, key, transpose_b = True) / tf.math.sqrt(tf.cast(embedding_dim, tf.float32))

masked_attention_scores = attention_scores + (expanded_mask * -1e9) # Adding a large negative number to mask.
print("Masked Attention Scores Shape:", masked_attention_scores.shape) # Output: Masked Attention Scores Shape: (3, 5, 5)

#Applying softmax over the last dimension.
attention_weights = tf.nn.softmax(masked_attention_scores, axis = -1)

output = tf.matmul(attention_weights, value)
print("Output Shape:", output.shape) # Output: Output Shape: (3, 5, 128)

```

This example illustrates mask integration within a complete, albeit simplified, attention calculation. It generates random query, key, and value tensors, computes the unmasked attention scores, and applies the padding mask not by direct multiplication (as done before), but by adding a large negative number to the scores corresponding to the padded tokens. This method, a common technique in attention mechanisms, effectively zeroes out the softmax probabilities of the masked tokens.

For further investigation, I would recommend focusing on studying documentation related to broadcasting in libraries like TensorFlow, PyTorch, and NumPy. Specific sections on tensor manipulation and shape operations, as well as tutorials on sequence modeling that cover attention mechanisms, will be helpful. Moreover, the original paper describing Transformer architectures will provide further background to the reasons behind masking in this context.

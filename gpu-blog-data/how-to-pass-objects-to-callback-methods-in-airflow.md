---
title: "How to pass objects to callback methods in Airflow?"
date: "2025-01-26"
id: "how-to-pass-objects-to-callback-methods-in-airflow"
---

Within the context of Apache Airflow, passing objects to callback methods requires careful consideration due to the framework's inherent serialization and execution model. Airflow primarily relies on pickling for data transfer between its components, particularly for task instances and their associated arguments. This impacts how objects, especially those containing complex state or dependencies, can be safely and effectively passed to functions invoked by callbacks like `on_success_callback` or `on_failure_callback`. My experience debugging Airflow DAGs has shown that improper handling of object passing in these callbacks is a frequent source of runtime errors, often manifesting as pickling failures or unexpected behavior.

The core problem stems from the fact that Airflow workers typically execute tasks in separate processes or even on separate machines. When you define a callback with an object parameter, that object's state and methods need to be effectively captured and made accessible in the worker's environment when the callback is triggered. Simple python objects like integers or strings usually pose no problem, as they serialize predictably, however, more complex objects containing custom logic, connections, or file handles can introduce complications.

The primary mechanism for dealing with this challenge is avoiding direct passage of objects altogether and instead utilizing metadata, context variables, and, when necessary, employing a shared storage mechanism accessible to all worker instances. Airflow task instances carry significant metadata about the current task and DAG run, information easily accessible within callback functions via the `context` argument. Context variables include the task's execution date, run ID, and access to the XCom mechanism, allowing you to access output from previous tasks. This reduces the need to directly pass objects since associated information can be extracted and re-initialized inside the callback. Furthermore, XCom, Airflow's mechanism for cross-task communication, facilitates the transfer of information between tasks, enabling callbacks to retrieve necessary data through these channels instead of attempting to directly serialize and pass object references.

Here are three examples demonstrating correct approaches:

**Example 1: Leveraging Task Context to Access Metadata:**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def my_task(**kwargs):
    return {'task_name': 'my_task_name'}  # Example metadata

def success_callback(context):
    ti = context['ti']
    task_name = ti.xcom_pull(task_ids='my_task', key='return_value').get('task_name')
    print(f"Task {task_name} succeeded on {context['execution_date']}")

with DAG(
    dag_id='callback_metadata_example',
    start_date=datetime(2023, 1, 1),
    schedule=None,
    catchup=False
) as dag:
    task = PythonOperator(
        task_id='my_task',
        python_callable=my_task,
        on_success_callback=success_callback
    )

```

In this example, the `my_task` returns a dictionary containing a `task_name`. Instead of attempting to pass this object to `success_callback`, we store it using XCom. Within the `success_callback`, I retrieve the return value from the task instance (`ti`) using `xcom_pull` and use context variables like `execution_date` for reporting the successful execution. No objects are directly passed between task and callback avoiding pickling errors. The focus is on leveraging the available Airflow context and communication tools.

**Example 2: Re-constructing an Object Using Serialized Data:**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import json

class DataProcessor:
    def __init__(self, config):
        self.config = config

    def process(self, data):
       print (f"Processing {data} with config {self.config}")

def my_task(**kwargs):
    config_data = {'param1': 10, 'param2': 'value'}
    data_processor = DataProcessor(config_data)
    return  json.dumps(config_data)

def success_callback(context):
    ti = context['ti']
    config_json = ti.xcom_pull(task_ids='my_task', key='return_value')
    config = json.loads(config_json)
    processor = DataProcessor(config)
    processor.process("my_input_data")

with DAG(
    dag_id='callback_object_reconstruct',
    start_date=datetime(2023, 1, 1),
    schedule=None,
    catchup=False
) as dag:
    task = PythonOperator(
        task_id='my_task',
        python_callable=my_task,
        on_success_callback=success_callback
    )
```

Here, the goal is to use a `DataProcessor` instance in the callback. Directly passing the `DataProcessor` instance would almost certainly cause serialization issues. Instead, I extract the `DataProcessor` configuration as JSON within the `my_task` and send it via XCom. In the `success_callback`, the serialized data is retrieved and used to *reconstruct* a new `DataProcessor` instance. This approach allows the callback to perform necessary actions without relying on the direct passage of a complex object. Serialization is handled via simple json strings making the data transportable across Airflow workers.

**Example 3: Utilizing a Shared Storage Mechanism (Simplified Example)**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import pickle

class SharedState:
    def __init__(self):
        self.data = None

    def save_data(self, data, file_path):
        with open(file_path, 'wb') as file:
             pickle.dump(data,file)

    def load_data(self, file_path):
        with open(file_path, 'rb') as file:
             self.data = pickle.load(file)
        return self.data


def my_task(**kwargs):
   state = SharedState()
   data_to_save = {"key":"value"}
   file_path = "/tmp/shared_state.pkl"
   state.save_data(data_to_save,file_path)
   return file_path

def success_callback(context):
    ti = context['ti']
    file_path = ti.xcom_pull(task_ids='my_task', key='return_value')
    state = SharedState()
    loaded_data = state.load_data(file_path)
    print (f"Data loaded from file:{loaded_data}")

with DAG(
    dag_id='callback_shared_storage',
    start_date=datetime(2023, 1, 1),
    schedule=None,
    catchup=False
) as dag:
    task = PythonOperator(
        task_id='my_task',
        python_callable=my_task,
         on_success_callback=success_callback
    )
```

This example simulates using a shared storage mechanism â€“ in this case, a simple file system in `/tmp/`. In a realistic scenario, this could be a shared object storage like an S3 bucket or a database. Here, the `my_task` persists data via `SharedState` to the shared filesystem using `pickle` and stores the file path via XCom. The callback retrieves the file path and loads the data into a `SharedState` object which can be used in the callback itself.  While simple, the core idea of serializing object data into a persistent location and then reconstructing an object instance in the callback using that data remains consistent when moving to an actual shared storage.

In summary, the challenge of passing objects to callback methods in Airflow stems from the distributed nature of the execution environment. Attempting to pass objects directly through method arguments will often result in serialization failures or data corruption. Instead, Airflow provides a robust context, cross-task communication via XCom, and, for more complex scenarios, the ability to leverage shared storage mechanisms that provide a consistent way of transporting and recreating state between task instances and callback execution. It is essential to embrace these tools and design your callback logic accordingly, ensuring that necessary information is transferred via serializable formats and reconstructed as needed, instead of attempting to pass objects directly.  I advise developers using Airflow to extensively use these techniques to circumvent common issues that arise when creating complex DAGs.

For continued learning I recommend reviewing the official Apache Airflow documentation, paying specific attention to sections on context variables, XCom, and task lifecycle. Additionally, consulting advanced Airflow books that contain deeper insights into task instance behavior and strategies for building robust pipelines is always a good practice. Finally, following the best practices of using pure functions where possible, and limiting dependencies between tasks greatly reduces issues with object passing between functions.

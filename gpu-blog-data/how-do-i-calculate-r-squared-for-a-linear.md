---
title: "How do I calculate R-squared for a linear regression model (smf.ols) on training and testing datasets?"
date: "2025-01-30"
id: "how-do-i-calculate-r-squared-for-a-linear"
---
The computation of R-squared, a common metric for assessing linear regression model fit, requires distinct approaches depending on whether you're evaluating performance on the training set used for model fitting or on a separate, held-out testing dataset. In my experience developing predictive models for financial analysis, understanding this distinction is crucial to avoid misleading conclusions about model generalizability.

**Core Concept: R-squared and Its Computation**

R-squared, often called the coefficient of determination, quantifies the proportion of variance in the dependent variable that is predictable from the independent variables. The formula involves comparing the Sum of Squared Residuals (SSR), the variance not explained by the model, with the Total Sum of Squares (TSS), the total variance in the dependent variable. More formally:

R² = 1 - (SSR / TSS)

Where:

*   **SSR (Sum of Squared Residuals)**: Σ (yᵢ - ŷᵢ)², where yᵢ is the actual value and ŷᵢ is the predicted value for the i-th data point. This represents the variability unexplained by the regression model.

*   **TSS (Total Sum of Squares)**: Σ (yᵢ - ȳ)², where ȳ is the mean of the actual values of the dependent variable. This is the total variability of the dependent variable around its mean.

When evaluating R-squared on the training set, the ŷᵢ values are the fitted values generated by the model using the training data itself. However, when assessing performance on the test set, the ŷᵢ values are obtained by applying the trained model to new, unseen data. The TSS for both calculations differs; it’s relative to the dataset being analyzed, either training or testing.

**Training Set R-squared**

The training R-squared is calculated using the residuals from the training set, after fitting the linear regression model. It provides insight into how well the model fits the data used for training. A high training R-squared might seem promising but isn't an indication of good generalization capability, i.e., that the model will perform well on new, unseen data. Overfitting, where the model learns the intricacies of the training data too closely including noise, can lead to very high training R-squared while exhibiting poor performance on testing data.

**Testing Set R-squared**

The testing R-squared is calculated using residuals generated from applying the trained model to the test dataset. This metric is critical for assessing the generalization power of the model. A robust model should demonstrate reasonably high R-squared values on both the training and testing data. A significantly lower R-squared on the testing set than on the training set suggests overfitting or that the assumptions of the model are not met by the testing data.

**Code Examples**

I use `statsmodels.formula.api` (hereafter `smf`) for regression. I demonstrate how to compute R-squared using both training and testing datasets.

**Example 1: Basic R-squared on Training Data**

```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split

# Sample Data
np.random.seed(42)
X = np.random.rand(100, 2) * 10
y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(100) * 2 # Simulate linear relationship with noise
data = pd.DataFrame({'y': y, 'X1': X[:, 0], 'X2': X[:, 1]})

# Split into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)

# Fit the model on the training data
model = smf.ols('y ~ X1 + X2', data=train_data).fit()

# Extract R-squared from the model's summary
training_r_squared = model.rsquared
print(f"Training R-squared: {training_r_squared:.3f}")
```

**Commentary:** This code snippet first generates sample data exhibiting a linear relationship between dependent and independent variables. It splits this data into training and testing subsets, then fits a linear regression model (`smf.ols`) using the training subset. The `model.rsquared` attribute readily provides the R-squared value for the training dataset. Note the usage of `fit()` to actually train the model before accessing the R-squared. This is crucial as the R-squared won't exist before this fit happens.

**Example 2: R-squared on Test Data using Model Predictions**

```python
# Generate predictions on the test data
test_predictions = model.predict(test_data)

# Calculate residuals on the test set
test_residuals = test_data['y'] - test_predictions

# Calculate SSR for the test data
test_ssr = np.sum(test_residuals**2)

# Calculate TSS for the test data
test_tss = np.sum((test_data['y'] - test_data['y'].mean())**2)

# Calculate R-squared for the test data
testing_r_squared = 1 - (test_ssr / test_tss)
print(f"Testing R-squared: {testing_r_squared:.3f}")
```

**Commentary:** In this example, the model trained in the previous example is used to generate predictions on the test dataset using `model.predict()`. Residuals are then computed as the difference between actual test values and predicted test values. With these residuals, we proceed to calculate the sum of squared residuals (`test_ssr`) and the total sum of squares (`test_tss`) for the *test* dataset. The R-squared is then computed using the formula described before. Notice that the mean used to calculate TSS, `test_data['y'].mean()`, is specific to the test dataset’s actual y values. This ensures the testing R-squared measures the model's performance on this specific set of unseen data.

**Example 3: Comparing Training and Testing R-squared**

```python
# Both calculations combined in one output

print(f"Training R-squared (using model.rsquared): {training_r_squared:.3f}")
print(f"Testing R-squared (calculated manually): {testing_r_squared:.3f}")

# Check difference
r_squared_diff = training_r_squared - testing_r_squared
print(f"Difference between R-squared values: {r_squared_diff:.3f}")
```

**Commentary:** This example consolidates the results from the preceding examples, directly showing training R-squared from the model’s `rsquared` attribute, and the testing R-squared, calculated manually. Furthermore, it explicitly computes the difference between these two metrics. Such a comparison is critical. Large positive differences can indicate overfitting. A model that performs significantly better on the training data than on the testing data is likely not generalizable and therefore less useful in real-world applications. A negative difference is unusual, and could indicate sampling issues.

**Resource Recommendations**

Several good introductory and advanced statistics resources can assist with gaining a deeper understanding of regression analysis and R-squared, without linking to online material.

*   **Introductory Statistics Textbooks:** Any standard introductory statistics textbook will cover the basics of linear regression and R-squared. Look for resources that emphasize understanding the underlying statistical principles, not just mathematical calculations.
*   **Regression Analysis Specific Books:** Books focused solely on regression analysis are also very valuable. These resources often include more detailed explanations of model assumptions, model validation techniques, and practical considerations. Look for publications that cover Ordinary Least Squares regression.
*   **Scientific Python Documentation:** Documentation for modules like `statsmodels` and `scikit-learn` provide insights into model evaluation in Python. These resources often include practical examples and implementation details. The statistical background of the function and methods are typically only briefly covered however.
*   **Statistical Computing Books:** Books focused on statistical computing with Python or R may also prove useful. These offer a hands-on approach to learning these methods in practice, highlighting the application of different metrics.

In summary, calculating and interpreting R-squared for both training and test data is paramount for evaluating a linear regression model's effectiveness. It's not simply a matter of running the code but understanding the concepts of model fit, generalization capability, and overfitting. The manual approach I presented for calculating testing R-squared helps clarify the underlying calculation and makes it easier to avoid common pitfalls in model evaluation.

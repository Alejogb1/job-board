---
title: "Can sub-word embeddings from BERT be used to create word embeddings faster?"
date: "2025-01-30"
id: "can-sub-word-embeddings-from-bert-be-used-to"
---
The computational cost of training traditional word embeddings, such as Word2Vec or GloVe, scales linearly with vocabulary size. This presents a bottleneck, especially when dealing with large text corpora containing a vast and ever-evolving vocabulary. Sub-word embeddings, specifically those generated by models like BERT, offer an alternative avenue for obtaining word representations, potentially expediting the process. While they don't *directly* create word embeddings in the conventional sense, they provide a rich set of sub-word units that can be leveraged to construct these representations. My experience training and deploying NLP models has shown that the speed gains stem from the pre-computed nature of BERT's sub-word embeddings and their ability to handle out-of-vocabulary (OOV) words.

Here's how it works and why it can be faster. BERT, unlike models trained directly on words, operates on sequences of tokenized inputs, often using Byte-Pair Encoding (BPE) or WordPiece algorithms. These algorithms break down words into smaller sub-word units. For instance, the word "unbelievable" might be tokenized as "un", "##believ", "##able". BERT then produces vector representations for these sub-words. These representations, while not word embeddings themselves, contain contextual information due to BERT's transformer architecture. Crucially, these sub-word vectors are pre-computed during the training phase of BERT itself, and can be accessed directly.

To generate a word vector using these pre-computed sub-word embeddings, one typically aggregates the embeddings of its constituent sub-words. This aggregation might involve simple averaging, summation, or more complex techniques like using attention mechanisms. Importantly, this aggregation is substantially faster than training a new embedding model from scratch, as it doesn't require learning new parameters, only performing linear algebra operations on already trained vectors. This is the primary source of speed improvement. Further, BERT's ability to handle OOV words via decomposition into known sub-words addresses a key limitation of traditional word embedding methods. If "unbelievable" is not in the training vocabulary of a traditional word embedding model, it would often be replaced with an unknown token. With BERT, we can still create an embedding for it by combining its sub-word embeddings.

Letâ€™s look at some code examples to illustrate this process:

**Example 1: Averaging Sub-word Embeddings**

```python
import torch
from transformers import BertTokenizer, BertModel

def get_word_embedding_averaged(word, tokenizer, model):
    """Averages sub-word embeddings to obtain a word embedding."""
    tokens = tokenizer.tokenize(word)
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = torch.tensor([input_ids]) #Batch size 1
    with torch.no_grad():
      outputs = model(input_ids)
    token_embeddings = outputs.last_hidden_state
    word_embedding = torch.mean(token_embeddings, dim=1).squeeze(0)
    return word_embedding

if __name__ == "__main__":
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    word = "unbelievable"
    embedding = get_word_embedding_averaged(word, tokenizer, model)

    print(f"Shape of '{word}' embedding: {embedding.shape}")
    # Example: Shape of 'unbelievable' embedding: torch.Size([768])
    word2 = "cat"
    embedding2 = get_word_embedding_averaged(word2,tokenizer, model)
    print(f"Shape of '{word2}' embedding: {embedding2.shape}")
    # Example: Shape of 'cat' embedding: torch.Size([768])
```

This example demonstrates the most straightforward method. We first tokenize the input word into its sub-word components using BERT's tokenizer. Then, we pass the tokenized input to the BERT model to get the contextualized sub-word embeddings. Finally, we average these embeddings to get a single word embedding. The `torch.no_grad()` context prevents backpropagation during this process. This approach is fast and relatively effective for many applications. The resulting embedding's shape is consistent for different words, as it aligns with BERT's hidden dimension. Both the word 'unbelievable' and 'cat' which are tokenized into multiple subwords and single subword respectively result in an embedding of the same shape, 768 in this case for `bert-base-uncased`.

**Example 2: Using CLS Token Embedding**

```python
import torch
from transformers import BertTokenizer, BertModel

def get_word_embedding_cls(word, tokenizer, model):
  """Uses the CLS token embedding as the word embedding."""
  encoded_input = tokenizer(word, return_tensors='pt')
  with torch.no_grad():
      output = model(**encoded_input)
  cls_embedding = output.last_hidden_state[:, 0, :]
  return cls_embedding.squeeze(0)

if __name__ == "__main__":
  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  model = BertModel.from_pretrained('bert-base-uncased')

  word = "unbelievable"
  embedding = get_word_embedding_cls(word, tokenizer, model)

  print(f"Shape of '{word}' embedding (CLS): {embedding.shape}")
  # Example: Shape of 'unbelievable' embedding (CLS): torch.Size([768])

  word2 = "cat"
  embedding2 = get_word_embedding_cls(word2, tokenizer, model)
  print(f"Shape of '{word2}' embedding (CLS): {embedding2.shape}")
   # Example: Shape of 'cat' embedding (CLS): torch.Size([768])
```

Another approach leverages the embedding associated with the special `[CLS]` token that BERT prepends to the input sequence. This token is meant to capture sentence-level information, but in the context of single-word inputs, it can serve as a reasonable word embedding. This method is typically faster because it does not require averaging or summing and only uses the pre-computed `[CLS]` embedding. As with example 1, the embedding shape for the word 'unbelievable' and 'cat' remains consistent at `768`.

**Example 3: Weighted Averaging based on Attention scores**

```python
import torch
from transformers import BertTokenizer, BertModel
import torch.nn.functional as F

def get_word_embedding_weighted(word, tokenizer, model):
    """Calculates weighted average using attention scores."""
    tokens = tokenizer.tokenize(word)
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    input_ids = torch.tensor([input_ids])
    with torch.no_grad():
        outputs = model(input_ids, output_attentions=True)

    token_embeddings = outputs.last_hidden_state
    attention_weights = outputs.attentions[-1]  # use the last layer attention

    # Average the attention heads
    attention_weights = torch.mean(attention_weights, dim=1)

    # Sum the attention weights across tokens
    attention_weights = torch.sum(attention_weights, dim=1)

    # Normalize the attention weights to sum to 1
    attention_weights = F.softmax(attention_weights, dim = 1)

    weighted_embedding = torch.sum(token_embeddings * attention_weights.unsqueeze(-1), dim = 1).squeeze(0)

    return weighted_embedding


if __name__ == "__main__":
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    word = "unbelievable"
    embedding = get_word_embedding_weighted(word, tokenizer, model)
    print(f"Shape of '{word}' embedding (weighted): {embedding.shape}")
    # Example: Shape of 'unbelievable' embedding (weighted): torch.Size([768])

    word2 = "cat"
    embedding2 = get_word_embedding_weighted(word2, tokenizer, model)
    print(f"Shape of '{word2}' embedding (weighted): {embedding2.shape}")
   # Example: Shape of 'cat' embedding (weighted): torch.Size([768])
```

This final example illustrates a more sophisticated technique by incorporating attention weights derived from the model's last attention layer. This approach dynamically weights the sub-word embeddings based on their importance within the context of the word. The implementation calculates the attention scores, averages across attention heads, sums it up across the tokens, normalizes using softmax, and computes the weighted sum of the embeddings. This can result in more contextually nuanced word embeddings compared to basic averaging, at the cost of increased computation. As with previous examples the final embedding shape is `768` regardless of tokenization.

In summary, sub-word embeddings from BERT can be used to create word embeddings faster than traditional methods. The speed advantage comes from leveraging pre-computed sub-word vectors and avoiding full-scale model training for each application. The trade-off is that these derived embeddings are not necessarily trained specifically for the task at hand. They are contextualized, meaning they change depending on their input context, a behavior that might not be ideal for some tasks where static word representations are preferred. Choosing the right aggregation method, and understanding the context of your particular application is key to extracting the most value. While simple averaging is fast, more complex methods like weighted averaging with attention might provide better performance but come with greater computational costs during vector construction.

For further investigation, I recommend exploring books on Natural Language Processing, focusing on transformer architectures and sub-word tokenization, also research papers focusing on contextualized word embeddings and their various aggregation techniques, and finally, exploring documentation and tutorials provided by popular NLP libraries which cover practical aspects of applying these models.

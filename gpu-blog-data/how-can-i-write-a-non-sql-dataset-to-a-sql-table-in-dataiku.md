---
title: "How can I write a non-SQL dataset to a SQL table in Dataiku?"
date: "2025-01-26"
id: "how-can-i-write-a-non-sql-dataset-to-a-sql-table-in-dataiku"
---

Having spent years wrestling with data integration challenges across various platforms, I've frequently encountered situations where data residing outside traditional SQL databases needs to be ingested into SQL systems within Dataiku. While Dataiku offers built-in connectors for common file formats and databases, directly writing non-SQL datasets – such as those generated by Python scripts or API calls – to a SQL table requires a specific approach. The core challenge lies in bridging the gap between Dataiku's managed dataset abstraction and the transactional nature of SQL databases. The solution revolves around programmatically leveraging Dataiku's Python API and establishing a controlled data write process.

The primary method involves using a Python recipe in Dataiku to first transform the non-SQL dataset into a Pandas DataFrame, then write this DataFrame to the target SQL table using a database connection established through Dataiku. This approach sidesteps the limitations of relying solely on Dataiku’s recipe creation from the GUI for non-traditional data sources. The process encompasses several stages: data retrieval or generation; transformation into a structured Pandas DataFrame; acquiring a database connection; writing the DataFrame; and implementing exception handling. Careful attention to data type mapping, schema management, and transaction control is critical to ensure data integrity and performance. The following sections detail this process with illustrative examples.

**Example 1: Simple List to SQL**

Let’s begin with a basic scenario: We have a Python list representing customer IDs, which we want to write to a SQL table named 'customer_ids'. We assume the table already exists with a single integer column named 'customer_id'.

```python
import dataiku
import pandas as pd
from sqlalchemy import create_engine

# 1. Generate the non-SQL data (in this case, a Python list)
customer_list = [101, 102, 103, 104, 105]

# 2. Transform into a Pandas DataFrame
df = pd.DataFrame(customer_list, columns=['customer_id'])

# 3. Get the database connection from Dataiku
client = dataiku.api_client()
conn_name = 'my_sql_connection' # Replace with your connection name
connection = client.get_connection(conn_name)
connection_params = connection.get_settings()

# Extract the connection string components
database_type = connection_params.get('type')
database_host = connection_params.get('host')
database_port = connection_params.get('port')
database_name = connection_params.get('database')
database_user = connection_params.get('user')
database_password = connection_params.get('password')

# Construct the SQLAlchemy connection string based on type
if database_type == 'PostgreSQL':
    connection_string = f'postgresql+psycopg2://{database_user}:{database_password}@{database_host}:{database_port}/{database_name}'
elif database_type == 'MySQL':
    connection_string = f'mysql+pymysql://{database_user}:{database_password}@{database_host}:{database_port}/{database_name}'
elif database_type == 'SQL Server':
    connection_string = f'mssql+pyodbc://{database_user}:{database_password}@{database_host}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server'
else:
    raise ValueError(f"Unsupported database type: {database_type}")

# Create a SQLAlchemy engine
engine = create_engine(connection_string)


# 4. Write the DataFrame to the SQL table
try:
    df.to_sql('customer_ids', engine, if_exists='append', index=False)
    print("Data successfully written to 'customer_ids' table.")
except Exception as e:
    print(f"Error writing to SQL table: {e}")
finally:
    engine.dispose()
```

**Commentary on Example 1:** This example demonstrates the core process. We extract database credentials via Dataiku's API and use them to construct a SQLAlchemy engine for database interaction. The `to_sql` method efficiently handles the write operation; `if_exists='append'` adds data without deleting existing rows, and `index=False` avoids writing the Pandas DataFrame index to the database. The `try-except-finally` block ensures we handle potential errors gracefully and properly release resources using `engine.dispose()`. Note that the specific SQLAlchemy connector (`psycopg2`, `pymysql`, `pyodbc`) and connection string structure will vary depending on your particular SQL database vendor and its configurations. It's critical to tailor the SQLAlchemy engine creation to the particular database type. Also, it is assumed that the SQL table schema is compatible with the DataFrame, otherwise schema creation logic or SQL DDL statements need to be added before the insertion logic.

**Example 2: API Data to SQL**

In a more complex scenario, let's imagine fetching data from a REST API, which returns JSON. The data contains information about product details that needs to be stored in a SQL table named 'products' which has columns like 'product_id', 'product_name', and 'price'.

```python
import dataiku
import pandas as pd
import requests
from sqlalchemy import create_engine

# 1. Fetch data from a REST API
try:
    response = requests.get("https://api.example.com/products")
    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
    data = response.json()
except requests.exceptions.RequestException as e:
    print(f"Error fetching data from API: {e}")
    exit()  # or handle differently

# 2. Transform JSON data to Pandas DataFrame
try:
    df = pd.DataFrame(data)  # Assumes the API returns a list of JSON objects
    # Assuming API returns data with appropriate keys, rename them if needed
    df = df.rename(columns={'id': 'product_id', 'name': 'product_name'})
except Exception as e:
    print(f"Error creating DataFrame: {e}")
    exit()

# 3. Get the database connection from Dataiku (same as Example 1)
client = dataiku.api_client()
conn_name = 'my_sql_connection'
connection = client.get_connection(conn_name)
connection_params = connection.get_settings()

database_type = connection_params.get('type')
database_host = connection_params.get('host')
database_port = connection_params.get('port')
database_name = connection_params.get('database')
database_user = connection_params.get('user')
database_password = connection_params.get('password')

if database_type == 'PostgreSQL':
    connection_string = f'postgresql+psycopg2://{database_user}:{database_password}@{database_host}:{database_port}/{database_name}'
elif database_type == 'MySQL':
    connection_string = f'mysql+pymysql://{database_user}:{database_password}@{database_host}:{database_port}/{database_name}'
elif database_type == 'SQL Server':
    connection_string = f'mssql+pyodbc://{database_user}:{database_password}@{database_host}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server'
else:
    raise ValueError(f"Unsupported database type: {database_type}")


engine = create_engine(connection_string)

# 4. Write to SQL table
try:
    df.to_sql('products', engine, if_exists='append', index=False, dtype={'product_id':'INTEGER','price':'FLOAT'})
    print("Data written to 'products' table.")
except Exception as e:
    print(f"Error writing to SQL table: {e}")
finally:
    engine.dispose()
```

**Commentary on Example 2:** This example introduces API integration. We first fetch the JSON data and convert it into a Pandas DataFrame. It demonstrates an essential step of column renaming to match the SQL schema. Additionally, the `dtype` argument in `df.to_sql` demonstrates how to explicitly specify data types for columns in the database to prevent type mismatch issues; otherwise, automatic type mapping by Pandas might lead to errors or less optimal database performance. A further step of data type validation would be helpful here. The `raise_for_status()` function ensures that API calls are validated for successful HTTP status codes. Error handling during the API call and DataFrame creation is key since these operations have a high chance of failing.

**Example 3: Data Aggregation and Conditional Inserts**

Let’s assume a scenario involving a dataset of website visits where we want to aggregate visits per user and store the aggregated data to a table ‘aggregated_visits’ but only insert new aggregated user data (not overwrite existing ones). We'll need to use Dataiku's managed dataset to simulate our source.

```python
import dataiku
import pandas as pd
from sqlalchemy import create_engine, text

# 1. Read input dataset from Dataiku (simulated with a generated DataFrame)
# In reality, get the input dataset as follows
# input_dataset = dataiku.Dataset("my_input_dataset")
# df = input_dataset.get_dataframe()

# Here's a simulated DataFrame
data = {'user_id': [1, 2, 1, 3, 2], 'visit_time': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02','2024-01-03']}
df = pd.DataFrame(data)

# 2. Aggregate visits per user
aggregated_df = df.groupby('user_id').size().reset_index(name='visit_count')

# 3. Get the database connection from Dataiku (same as Example 1 and 2)
client = dataiku.api_client()
conn_name = 'my_sql_connection'
connection = client.get_connection(conn_name)
connection_params = connection.get_settings()

database_type = connection_params.get('type')
database_host = connection_params.get('host')
database_port = connection_params.get('port')
database_name = connection_params.get('database')
database_user = connection_params.get('user')
database_password = connection_params.get('password')

if database_type == 'PostgreSQL':
    connection_string = f'postgresql+psycopg2://{database_user}:{database_password}@{database_host}:{database_port}/{database_name}'
elif database_type == 'MySQL':
    connection_string = f'mysql+pymysql://{database_user}:{database_password}@{database_host}:{database_port}/{database_name}'
elif database_type == 'SQL Server':
    connection_string = f'mssql+pyodbc://{database_user}:{database_password}@{database_host}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server'
else:
    raise ValueError(f"Unsupported database type: {database_type}")


engine = create_engine(connection_string)

# 4. Conditional insert logic using raw SQL execution
try:
    with engine.connect() as con:
        for index, row in aggregated_df.iterrows():
            user_id = row['user_id']
            visit_count = row['visit_count']
            sql_query = text(f"INSERT INTO aggregated_visits (user_id, visit_count) "
                            f"SELECT :user_id, :visit_count "
                            f"WHERE NOT EXISTS (SELECT 1 FROM aggregated_visits WHERE user_id = :user_id)")
            con.execute(sql_query, {'user_id': user_id, 'visit_count': visit_count})
        print("Aggregated visits written to 'aggregated_visits' table with new records only.")
except Exception as e:
    print(f"Error writing to SQL table: {e}")
finally:
    engine.dispose()

```
**Commentary on Example 3:** This example uses Dataiku's managed dataset concept by reading a simulated DataFrame. In a real-world scenario, this data would come from a dataset managed by Dataiku. It showcases how to leverage Pandas' groupby functionality to perform aggregations. Crucially, instead of directly using `to_sql`, we execute a raw SQL query with `con.execute()`. This allows implementing conditional inserts using `INSERT ... WHERE NOT EXISTS`, ensuring we only add new user aggregates to the database table, which is particularly helpful in data update scenarios. Binding variables to the query parameters using the dictionary notation provides a more secure and clear approach than embedding the variables directly in the query string; that way it is less prone to SQL injection attacks.

**Resource Recommendations**

To further understand the technologies employed in this response, I recommend focusing on the following resources:

*   **Dataiku's official documentation:** The Dataiku documentation is indispensable for learning the nuances of the Python API, dataset management, and connection handling. This provides specific instruction on how to retrieve the various Dataiku configuration elements.
*   **Pandas Documentation:** A comprehensive resource on data manipulation, including operations on DataFrames and series. The sections on `DataFrame` creation, indexing, data aggregation, and `to_sql` are particularly relevant.
*   **SQLAlchemy Documentation:** Provides detailed information on how to interact with SQL databases using Python, focusing on connection establishment, transaction control, query execution, and schema management. The core features are explained with sufficient details.
*   **Database-specific driver documentation:** Documents such as the documentation of the Python database drivers `psycopg2`, `pymysql`, `pyodbc` should also be reviewed, particularly to understand how to set up and use a specific database instance through a Python API.

These resources should provide the necessary background knowledge to fully understand and implement the presented approach. The combination of Dataiku's managed datasets with the flexibility of the Python API and robust database interaction with SQLAlchemy offers a scalable and maintainable solution for writing various types of data into SQL databases.

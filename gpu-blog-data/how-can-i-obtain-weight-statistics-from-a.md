---
title: "How can I obtain weight statistics from a TensorFlow model?"
date: "2025-01-30"
id: "how-can-i-obtain-weight-statistics-from-a"
---
Quantifying the distribution of weights in a TensorFlow model is crucial for various aspects of model analysis, from identifying potential overfitting to understanding layer importance. During my time developing large-scale image recognition systems, I frequently needed to extract and analyze these statistics, which often involved custom tooling given the diverse network architectures and debugging requirements. The standard TensorFlow APIs provide the necessary building blocks, but obtaining comprehensive insights requires a structured approach.

The primary method for extracting model weights involves accessing the `trainable_variables` or `variables` attributes of a `tf.keras.Model` or a `tf.Module` object. These attributes return lists of `tf.Variable` objects, each representing a weight or bias in the model. These `tf.Variable` objects, in turn, can be directly accessed via their numpy array representation, allowing for standard statistical computation. It is important to differentiate between `trainable_variables` and `variables`. The former encompasses only parameters optimized during training, whereas the latter includes all variables within the model, potentially including non-trainable parameters. For most statistical analysis on the learning process, `trainable_variables` is the appropriate choice.

Once a list of variables is obtained, individual or aggregate statistics can be calculated using NumPy or TensorFlow's mathematical operations. Common metrics include mean, standard deviation, min, max, histograms, and specific quantiles, computed across all weights in a layer, across all weights in the entire model, or on a per-variable basis. For layer-wise analysis, it is essential to understand that TensorFlow organizes weights based on layer. This means that when accessing model weights, you obtain variables in the order the layers are defined in the model rather than their interconnectedness in the computational graph. Therefore, to properly align the results with corresponding layers, you must process the variable list in conjunction with the model's `layers` attribute.

The following code examples illustrate how to achieve this, starting with layer-specific statistics.

```python
import tensorflow as tf
import numpy as np

# Assume a simple model is defined
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(20,)),
    tf.keras.layers.Dense(5, activation='sigmoid')
])

# Obtain the trainable variables
trainable_vars = model.trainable_variables

# Iterate through layers, matching variables
layer_index = 0
for layer in model.layers:
    if hasattr(layer, 'kernel') and hasattr(layer, 'bias'): # Check for layers with weights and biases
        kernel = trainable_vars[layer_index]
        bias = trainable_vars[layer_index+1] # Bias is always after the kernel
        kernel_array = kernel.numpy()
        bias_array = bias.numpy()
        
        print(f"--- Layer {layer_index // 2 + 1} ({layer.name}) ---") # Integer division so that each pair is accounted for only once

        print("Kernel statistics:")
        print(f"  Mean: {np.mean(kernel_array):.4f}")
        print(f"  Std Dev: {np.std(kernel_array):.4f}")
        print(f"  Min: {np.min(kernel_array):.4f}")
        print(f"  Max: {np.max(kernel_array):.4f}")

        print("Bias statistics:")
        print(f"  Mean: {np.mean(bias_array):.4f}")
        print(f"  Std Dev: {np.std(bias_array):.4f}")
        print(f"  Min: {np.min(bias_array):.4f}")
        print(f"  Max: {np.max(bias_array):.4f}")
        
        layer_index += 2  # Increment to the next set of weights, a kernel and a bias
    elif hasattr(layer, 'kernel'): # Some layers may only have a kernel
        kernel = trainable_vars[layer_index]
        kernel_array = kernel.numpy()

        print(f"--- Layer {layer_index // 2 + 1} ({layer.name}) ---")

        print("Kernel statistics:")
        print(f"  Mean: {np.mean(kernel_array):.4f}")
        print(f"  Std Dev: {np.std(kernel_array):.4f}")
        print(f"  Min: {np.min(kernel_array):.4f}")
        print(f"  Max: {np.max(kernel_array):.4f}")
        
        layer_index += 1 #Increment to next variable, only kernel exists
    else:
        continue
```

This script demonstrates the extraction of per-layer weight statistics, iterating through the layers of a Keras sequential model. Inside the loop, it obtains the corresponding weight variables using the `layer_index`, which increments by two for each dense layer, accounting for both the kernel matrix and bias vector. The script is also designed to handle layers such as convolutional layers, which only have a kernel weight and not a bias. The numpy representation of the kernel and bias is then accessed with `.numpy()`, allowing numpy's statistical functions to operate on the values. A similar process can be used for other types of layers. Output is provided on a per-layer basis, labeled with its sequential position and layer name, displaying basic statistics.

Next, consider extracting a more comprehensive histogram of model weights, particularly useful for identifying weight clustering or sparsity. The code demonstrates how a single weight statistic, a histogram, can be extracted at a per-variable level.

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt # Add matplotlib for visualization

model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(20,)),
    tf.keras.layers.Dense(5, activation='sigmoid')
])

trainable_vars = model.trainable_variables

for i, var in enumerate(trainable_vars):
    var_array = var.numpy()
    hist, bins = np.histogram(var_array.flatten(), bins=50) # Flatten before calculating histogram
    
    plt.figure()
    plt.hist(var_array.flatten(), bins=50) # Generate histogram plot for variable
    plt.title(f"Histogram of Variable {i} ({var.name})")
    plt.xlabel("Weight Value")
    plt.ylabel("Frequency")
    plt.show()
```

This example iterates through the model's trainable variables and produces a histogram plot of each variable using `matplotlib`. This visualization allows more direct evaluation of the distribution of weights. Before histogramming, the variable array is flattened, aggregating all weights in the tensor for statistical analysis. The bin count of the histogram is configurable by the user.

Finally, letâ€™s examine how to calculate statistics of *all* model weights as a single distribution. This could be a useful check to see how the overall weight distribution develops during training.

```python
import tensorflow as tf
import numpy as np

model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(20,)),
    tf.keras.layers.Dense(5, activation='sigmoid')
])

trainable_vars = model.trainable_variables
all_weights = np.concatenate([var.numpy().flatten() for var in trainable_vars])

print("--- Overall Model Weights ---")
print(f"  Mean: {np.mean(all_weights):.4f}")
print(f"  Std Dev: {np.std(all_weights):.4f}")
print(f"  Min: {np.min(all_weights):.4f}")
print(f"  Max: {np.max(all_weights):.4f}")
```

This code iterates over all trainable variables, flattens the weights of each to a single array, concatenates these flattened arrays, and then computes model-wide statistics on the resulting flattened array. It provides an overall view of the model weight distribution. The use of `np.concatenate` avoids having to maintain explicit counters, making for a cleaner operation.

To further enhance the understanding and application of model weight analysis, I recommend studying the following resources. Specifically, the TensorFlow documentation for `tf.keras.Model` and `tf.Variable` offers a detailed specification of the API. For statistical computations, the NumPy documentation is indispensable. Furthermore, consider reading the literature on regularization and model pruning techniques, as these often directly correlate with weight statistics. Texts on deep learning best practices will also frequently cover how weight statistics can be used to diagnose model training issues. Investigating these resources will provide a strong foundation for analyzing and understanding model behavior through weight distributions.

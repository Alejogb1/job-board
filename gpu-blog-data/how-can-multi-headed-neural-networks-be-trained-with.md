---
title: "How can multi-headed neural networks be trained with labels provided only for specific heads at a time?"
date: "2025-01-30"
id: "how-can-multi-headed-neural-networks-be-trained-with"
---
Multi-headed neural networks, often employed in complex tasks requiring diverse outputs, present a unique training challenge when ground truth labels are not available for all heads simultaneously. This scenario, common in applications like multi-task learning with sparse supervision or hierarchical prediction, necessitates a specialized approach to backpropagation. I've encountered this firsthand in developing a model for medical image analysis, where different heads were responsible for detecting distinct pathologies; often, only a subset of pathologies were known for a given image. The key lies in strategically masking the loss contribution from heads lacking labels, ensuring that gradients only propagate back through relevant parameters.

The foundation of training lies in the backpropagation algorithm, where the error from the output layer is propagated backward through the network to update weights and biases. In multi-headed networks with shared feature extractors, this presents a problem if we naively compute loss across all heads, regardless of label availability. Heads lacking labels would still contribute to the gradient updates through the shared layers, potentially leading to detrimental effects. These gradients, based on arbitrary outputs, can steer the shared feature extractor in unintended directions, degrading overall performance.

Consider, for example, a network with three heads. Head 1 might be tasked with predicting object presence, Head 2 with bounding box coordinates, and Head 3 with object classification. If we only possess object presence labels for a given training example, the ideal training should only update weights based on the loss generated by Head 1.

Implementing this approach fundamentally involves masking the loss function. Specifically, we only calculate and apply loss for heads where corresponding ground truth labels are available. The standard practice involves using a binary mask that corresponds to the availability of labels. If a label is present for a head, its corresponding mask element will be 1; otherwise, it will be 0. This mask is then multiplied by the loss for that specific head, effectively zeroing out the loss if no label exists.

Here are three code examples illustrating the concept, using a simplified PyTorch-like syntax for clarity:

**Example 1: Basic Masking with Per-Head Losses**

```python
import torch
import torch.nn as nn

class MultiHeadModel(nn.Module):
    def __init__(self, num_heads, input_size, hidden_size, output_size):
        super(MultiHeadModel, self).__init__()
        self.shared_features = nn.Linear(input_size, hidden_size)
        self.heads = nn.ModuleList([nn.Linear(hidden_size, output_size) for _ in range(num_heads)])
        self.loss_fn = nn.MSELoss(reduction='none')  # Changed to 'none'

    def forward(self, x):
        shared = torch.relu(self.shared_features(x))
        outputs = [head(shared) for head in self.heads]
        return outputs

    def compute_masked_loss(self, outputs, labels, masks):
        total_loss = 0
        for i, (output, label, mask) in enumerate(zip(outputs, labels, masks)):
            loss = self.loss_fn(output, label)
            masked_loss = loss * mask
            total_loss += masked_loss.mean() #Take mean per head
        return total_loss / len(outputs) #average of mean per head losses

# Parameters
num_heads = 3
input_size = 10
hidden_size = 20
output_size = 5

# Model instantiation
model = MultiHeadModel(num_heads, input_size, hidden_size, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Dummy input data, labels and masks
inputs = torch.randn(32, input_size)
labels = [torch.randn(32, output_size), torch.randn(32, output_size), torch.randn(32, output_size)] # Labels for all heads
masks = [torch.ones(32, 1), torch.zeros(32, 1), torch.ones(32, 1)] # Mask: head 1 & 3 present, 2 not present

# Forward pass
outputs = model(inputs)

# Compute masked loss
loss = model.compute_masked_loss(outputs, labels, masks)

# Backpropagation
optimizer.zero_grad()
loss.backward()
optimizer.step()

print("Loss:", loss.item())
```

This initial example illustrates the fundamental approach. `nn.MSELoss(reduction='none')` prevents an averaging of the loss across samples initially allowing us to apply masks per sample. Within the `compute_masked_loss` method we create a mask and apply it to the loss for each head, subsequently computing the mean across samples for each head and averaging across heads to have a single loss for backpropagation. Importantly, note how `masks` specifies that labels are only available for the first and third heads, thus the gradients from the second head's output are effectively disregarded during backpropagation.

**Example 2: Handling Variable Mask Shapes**

```python
import torch
import torch.nn as nn

class MultiHeadModel(nn.Module):
    def __init__(self, num_heads, input_size, hidden_size, output_size):
         super(MultiHeadModel, self).__init__()
         self.shared_features = nn.Linear(input_size, hidden_size)
         self.heads = nn.ModuleList([nn.Linear(hidden_size, output_size) for _ in range(num_heads)])
         self.loss_fn = nn.MSELoss(reduction='none')

    def forward(self, x):
        shared = torch.relu(self.shared_features(x))
        outputs = [head(shared) for head in self.heads]
        return outputs

    def compute_masked_loss(self, outputs, labels, masks):
        total_loss = 0
        for i, (output, label, mask) in enumerate(zip(outputs, labels, masks)):
            loss = self.loss_fn(output, label)
            mask = mask.expand_as(loss) # Expand mask
            masked_loss = loss * mask
            total_loss += masked_loss.mean() # Take mean per head
        return total_loss / len(outputs)

# Parameters
num_heads = 3
input_size = 10
hidden_size = 20
output_size = 5

# Model instantiation
model = MultiHeadModel(num_heads, input_size, hidden_size, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Dummy input data, labels and masks
inputs = torch.randn(32, input_size)
labels = [torch.randn(32, output_size), torch.randn(32, output_size), torch.randn(32, output_size)] # Labels for all heads
masks = [torch.ones(32,1), torch.zeros(32,1), torch.tensor([[1, 0, 1, 1, 1],
                                                      [1, 1, 0, 1, 0],
                                                      [0, 0, 0, 1, 1],
                                                      [1, 1, 1, 1, 1],
                                                      [0, 1, 1, 0, 1],
                                                      [1, 0, 1, 1, 0],
                                                      [1, 1, 0, 0, 1],
                                                      [0, 0, 1, 1, 0],
                                                      [0, 1, 0, 1, 0],
                                                      [0, 1, 0, 1, 1],
                                                      [0, 0, 1, 0, 1],
                                                      [1, 0, 0, 1, 0],
                                                      [0, 1, 0, 0, 1],
                                                      [0, 1, 1, 1, 0],
                                                      [1, 0, 1, 0, 0],
                                                      [0, 1, 1, 0, 1],
                                                      [1, 0, 1, 1, 0],
                                                      [0, 0, 0, 0, 1],
                                                      [0, 0, 1, 0, 0],
                                                      [0, 1, 1, 0, 0],
                                                      [0, 0, 1, 0, 1],
                                                      [1, 0, 1, 1, 1],
                                                      [1, 0, 1, 0, 1],
                                                      [1, 1, 1, 1, 0],
                                                      [1, 1, 0, 0, 0],
                                                      [1, 0, 0, 1, 0],
                                                      [0, 0, 0, 1, 1],
                                                      [0, 1, 1, 0, 0],
                                                      [1, 0, 1, 1, 1],
                                                      [1, 0, 1, 1, 0],
                                                      [0, 1, 0, 1, 0],
                                                      [0, 0, 0, 1, 1]], dtype=torch.float32)]
# Forward pass
outputs = model(inputs)

# Compute masked loss
loss = model.compute_masked_loss(outputs, labels, masks)

# Backpropagation
optimizer.zero_grad()
loss.backward()
optimizer.step()

print("Loss:", loss.item())
```

This expands on the previous example by allowing the mask to have the same shape as the loss. This flexibility is useful when we might only have a label for a subset of the outputs for a specific head and sample. For instance, this mask can be applied when each output of head three corresponds to a different output for the given task (e.g., multi-label classification). The `.expand_as(loss)` method is used to automatically create a mask with compatible dimensions, which is then multiplied element-wise by the loss.

**Example 3: Using Different Loss Functions Per Head**

```python
import torch
import torch.nn as nn

class MultiHeadModel(nn.Module):
    def __init__(self, num_heads, input_size, hidden_size, output_sizes):
        super(MultiHeadModel, self).__init__()
        self.shared_features = nn.Linear(input_size, hidden_size)
        self.heads = nn.ModuleList([nn.Linear(hidden_size, size) for size in output_sizes])
        self.loss_fns = [nn.MSELoss(reduction='none'), nn.L1Loss(reduction='none'), nn.BCEWithLogitsLoss(reduction='none')]

    def forward(self, x):
        shared = torch.relu(self.shared_features(x))
        outputs = [head(shared) for head in self.heads]
        return outputs

    def compute_masked_loss(self, outputs, labels, masks):
         total_loss = 0
         for i, (output, label, mask) in enumerate(zip(outputs, labels, masks)):
              loss = self.loss_fns[i](output, label)
              mask = mask.expand_as(loss)
              masked_loss = loss * mask
              total_loss += masked_loss.mean() #Take mean per head
         return total_loss / len(outputs)

# Parameters
num_heads = 3
input_size = 10
hidden_size = 20
output_sizes = [5, 3, 1]

# Model instantiation
model = MultiHeadModel(num_heads, input_size, hidden_size, output_sizes)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Dummy input data, labels and masks
inputs = torch.randn(32, input_size)
labels = [torch.randn(32, output_sizes[0]), torch.randn(32, output_sizes[1]), torch.randn(32, output_sizes[2])]
masks = [torch.ones(32,1), torch.zeros(32,1), torch.ones(32,1)]

# Forward pass
outputs = model(inputs)

# Compute masked loss
loss = model.compute_masked_loss(outputs, labels, masks)

# Backpropagation
optimizer.zero_grad()
loss.backward()
optimizer.step()

print("Loss:", loss.item())
```

This example further extends the concept to demonstrate how different loss functions can be utilized for different heads. This is often necessary when heads output values that are of different types (e.g., a regression head, a classification head). Here, we have MSE for the first, L1 for the second, and binary cross entropy for the third, emphasizing the flexibility of this training strategy. This scenario is similar to one I used in image analysis where each head was responsible for distinct types of prediction.

For further exploration, I recommend delving into resources discussing multi-task learning, particularly works focusing on scenarios involving incomplete or heterogeneous label sets. Additionally, research papers and documentation on masked loss functions will prove invaluable.  Consult materials covering gradient masking techniques, as these are at the core of the described methods. Examining the source code of established deep learning libraries (e.g., PyTorch, TensorFlow) will clarify implementation details. Finally, exploring research regarding attention mechanisms and their applications in multi-task settings can inform design choices. Understanding the mathematical foundations of backpropagation and gradient descent will provide necessary context.
